---
layout: post
title: 機械学習案件に携わってきてどんな理論的トピックを扱ってきたか 
categories: ['雑談']
---

### TL;DR
- 機械学習を仕事で使う人は実際のところどんな理論的内容を扱っているのか
- コードは GitHub にあるし研究は論文になるが、実業務で必要な理論的トピックに関してはなかなか目にする機会がない
- 自分がこれまでどんなことをしてきたのかを簡単にまとめてみた
<br>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

以前 [機械学習をやる上で線形代数のどのような知識が必要になるのか](https://yoheikikuta.github.io/linear_algebra/) という記事を書いた。
これに関して知人と話している時に「ある程度知っている人がこれだけやれば十分みたいな話をするのは不適切じゃないか」という感じの話をした。
ある対象に関してそれなりに知っている人が考える「これで十分」というのは、暗黙の前提や関連事項の理解があったりして、表面に出てくるのはほんの一部だったりするものである。

これは結構広く成り立つことだろう。
まあそれ自体はそれでいいし、表面に出てくるものを深く知ろうと思ったら自然とそれに必要な土台の部分まで進んでいくものだと思う。

ただそう考えた時にこの手の議論はそんなに意味のある結論を導くものじゃないなぁという気もする。
色んな人々の共通部分を抜き出そうとして、結局誰にとってもまあそうかなという程度の結論になる（か、背景が共有されないまま自分はこう思うみたいな話がなされる）。

そういうのじゃなくて、もうちょっと自分が読んだとして面白いと思えるような話がいいよな。
それが何かなと考えたら、「どんな仕事をしているかという概略とそこで実際にどんな理論的トピックを扱っているか」とか聞きたいなと思い至った。
エンジニアリングの話は GitHub とかで公開しているのを見れることが多いし、研究なら論文を読めばいい。
そうじゃなくて業務で機械学習を使った時に扱ってきた理論的な話ってどんなものなのか共有する（こういうのはそんなに見ない気がする）という話である。

せっかくなので社会人5年目に突入したこのタイミングで自分の話を少しまとめてみよう。


### 前提の話
- 自分のバックグラウンド
  - 素粒子理論で博士号取得。
  - 社会人になった時点での機械学習に関する知識はほぼなし。
- 社会人になって実際の業務で関わった案件とそれを遂行するのにどんな理論的なトピックを扱ったかを書く
  - ほぼ0から始めたのでせっかくなのである程度時系列順に書いてみる。
  - 必ずしも業務で使ったものでなく勉強したものも書いた。自分の興味を書くという意味でまあいいっしょ。
  - 抜け漏れは多分たくさんある（まあこれはしゃあない）。
  - エンジニアリングの話（これもほぼ0から始めたので色々勉強したが、主題じゃない）等は割愛。


### 実際にやってきたことを振り返ってみる
記憶がおぼろげでもしかしたら年次が間違ってるものもあるかもしれないが、各年毎に書いてみる。

ちなみに自分は仕事でデータ分析とか機械学習をメインでやってきている。
他の内容、例えば二ヶ月間コンサルの研修に行くとか、もいろいろやってきてはいるんだが、まあ基本的にはずっと機械学習っぽいことをしてきている人間である。

#### 1年目
最初の頃は多変量解析を中心にやっていた。
業務で使ったというより OJT 的な研修とかを含めて勉強したという感じ。
重回帰やPLS回帰、そして問題になりうる多重共線性などを理解したが、数学的には特異値分解が分かっていれば問題なく、物理でもよく扱っていたので特に苦労した部分はない。

ここで目的関数を最適化するとか正則化項をどうするとかの統計的機械学習の基礎もやった気がする。
深い学習理論とかには足を踏み入れてないので、微分使いますくらいの話。
ノルム云々という話は物理をやっていれば慣れているものなので、数学的に困ったことはほぼない。

てこ比とか懐かしい。練習問題で手計算で計算して以来、一度も使ってないです。
検定の勉強もちょっとやったけど、基本的な帰無仮説や対立仮説とかt検定など大学でやったことあるものくらいだった（余談だが物理やってたときもコルモゴロフ・スミルノフ検定とかは見たことがあった）。

SPSSなどの商用ソフトをちょっと触った。
ただしどれも本当に触ったことあるというレベルで、今後もガッツリ使うことはないんじゃないかと思う。

業務ではトピックモデルとかベイジアンネットを使った分析を結構やっていた。
PLSAを使うことが多かったのでAICとかBICを理解するためにそのへんの学習理論に手を出した（カルバックライブラーからの導出とか最尤推定の理解とか）。
ベイジアンネットを使う時はベイズ的な確率の取扱を復習したけど、これは大学でやった内容を超えるものは出会ってない。
また、これらの意味を考える上で比較対象としてのLDA（とグラフィカルモデルの基礎）とか構造方程式モデリングとかをちょっと勉強したりもした。

研究開発的な業務としてディープラーニングにも着手した。
この頃は今ほど流行っていたわけでもなく情報もそんなになかったので、人工知能学会誌の連載（のちに [深層学習](https://www.kindaikagaku.co.jp/information/kd0487.htm) として出版）を読んで勉強したのを覚えている。
「ボルツマン」マシンとか言われると親しみを覚えれるのは物理をやってた人あるあるだろう（エネルギー関数を決める指導原理が特になくて機械学習は大変だなぁとか思うわけですよ）。
ここでギブスサンプリングとかが出てくるのでメトロポリスヘイスティングを勉強したり、MCMCをちょっと舐めるみたいなことをした。
統計物理とかなり近しい話であるので、そこそこ昔のホップフィールドネットワークとかを調べてみたりもしていた。

業務で機械学習を扱うのだから基本的な内容は押さえておこうと思って読んだのが、[データ解析のための統計モデリング入門](https://www.iwanami.co.jp/book/b257893.html)と[はじめてのパターン認識](http://www.morikita.co.jp/books/book/2235)だった。
この辺は会社の同僚と業務時間後に勉強会とかで読んでいた。
どちらの本もよく書かれていて学び始めには結構良かった。

見直してみると1年目のときに勉強した内容は対外発表したりほとんどしてない。
そういう文化というか習慣はこの時の自分からは遠いものだったなぁ。

#### 2年目
クライアント先に滞在して先方からお願いされる分析をするという業務もやるようになった。
例えば時系列の分析でARIMAとかを扱った。
時系列は[経済・ファイナンスデータの計量時系列分析](http://www.asakura.co.jp/G_12.php?isbn=ISBN978-4-254-12792-8)で勉強した。
この本は読んでて結構面白くて、単位根課程ではよく使う漸近理論が使えず（平均へ回帰しない）、ブラウン運動に基づく漸近分布を使ったりするので興味深い。
ただし、個人的には時系列は理論はやってて楽しいけど、予測は難しすぎるのであんまり業務では使いたくない。
その他周りの人がやっていたので、状態空間モデルとかもっと基本的なカルマンフィルタとかを勉強してその界隈の言葉で会話はできるようになった。

ディープラーニングでは画像分析をやるようになって、分類とか物体検出の基本的なところを学んだ。
実際に使ったのもやはり典型的な分類とか物体検出だが、モデルを活用してちゃんとサービスをリリースするというところまではできてない。
内容はあんまり系統的に勉強した記憶はなく、例えばAlexNetとかVGGみたいな有名所のモデルの論文を読んで都度理解するみたいな感じだった。
物体検出は当時勢いがあったFaster-RCNNを読み込んでいたと思う。
Region of Proposalだとか矩形回帰だとかを理解したぞというところまでいくのに何回も読み直した記憶がある。
また、画像を扱うようになっていわゆる一昔前の手法も理解のために勉強した。
HOGとかSIFTとかに代表される特徴量とかですね。
この辺はちょっとした分析とかディープラーニングとの比較だとかでちょこちょこ使っていた。
MATLABでやってた。

レコメンドの案件をやるようになってからはランク学習を勉強した。
pointwiseやpairwiseなどの基本的な取り扱い（listwiseは使ったことない）や損失関数や精度指標をちゃんと理解するみたいなところから始めた。
理解のためにSVMを使った論文はいくつか読んだが、実際に自分が使うモデルとしてはGBDTを選んだ。
ここでboostingを真面目に勉強して、RandomForestやAdaBoostやGradient Boostingなどを理解した。
汎関数微分とかが出てくるわけだが、こいつは物理ではよく出てくるもので久しぶりに出てきてくれてありがとうと思えるくらいのものだった。
ここでやったレコメンドは最終的にはちゃんとサービスインして、かつ内容を論文してPAKDD(long)にacceptされた。

2年目からは少しずつ論文を書いたり論文読み会で発表したりということをするようになった。
もともと自分は発表したりするのが結構好きだし、色んな意味で対外発表は自分に良い結果をもたらすようになった。

#### 3年目
引き続きという感じだが、転職活動を始めたということもあってか目立って新しいことに着手した記憶がない。
なぜかと思って考えていたら本を書く（そしてそのために勉強会を開催する）ということもやっていたからだった。
このときのことを考えると うっ、頭が... まあこのくらいにしておこう。

12月から転職をしてそこからはディープラーニングを使った画像分析をやることが主業務になった。
しばらくはデプロイした画像分類モデルの性能を上げるために、会社のデータを使って会社のサービスに使うモデルを作るために様々なCNNを勉強したり試したりしたという時期だった。
それまではつまみ食い的にモデルを勉強していたが、このタイミングでCNNの主たる発展を論文を読みながらちゃんと理解した。
少なくとも理論物理で博士を取った人間にとってはCNNのモデルの論文で数学的に難しいところはあまりない。
なのでこれらは地道に読んでいって自分の中に蓄積させていくという感じだった。
ここで色々試して結果が良かったものを取り入れながらサービスを改善しつつ継続できているというのはなかなか素晴らしいことだと思う。

goodfellowらが著者のDeep Learningを読んだのもこの年だったかな。
他社の人と closed な勉強会を開催して読んだのだった。
optimizer 辺り（よくあるSGD, momentum, RMSProp, Adam,...）は自分が発表担当であったこともあり、かなり理解を深めた。
(L-)BFGSとかこれを機会にちゃんと自分で手を動かして理解できたのは良かった（仕事で必要になる場面はほとんどないが）。

業務とはダイレクトに結びついていないが、知人を集めて closed な PRML 勉強会を開催するようにして、これはとても良かった。
ベイジアンの理解はかなり進んで、サンプリング周りとか変分推定とかはだいぶ理解が進んだ。
Bishop氏が理論物理のPh.Dであることもあってか、物理の人には読みやすい本なのでは（ただしPeskin-Schroederが好きな人であれば）。

#### 4年目
今思い返してみると社内でハッカソンを主導したり対外発表したり採用活動（学会スポンサー含む）したり副業をしたりで忙しい年だった気がする。
論文を書いたり学会で座長をしたり派遣プログラムでNIPSに行ったりとアカデミックな活動もそこそこやった。

業務としては機械学習をサービスに載せるには割とまだギャップがあるということを思い知ることが多かった。
opensetの問題とかカテゴリ間の類似度の問題とかに直面して、関連しそうな論文を探して読んだりした。
前者では極値理論とか出てくるので極値分布とかをちょっと調べたが、一昔前に色々やられてる内容まで立ち戻って勉強したりはしてない（ので自分で使いこなせるとかいうレベルにはない）。
後者ではそんなに調べられている前例がなかったので、自分たちで色々やってそれを論文にまで持っていった。
これが良い学会に通れば文句なしなのだが、そうは甘くなかった。
実際の業務でやった内容を論文にして良い学会に通すということができればかなり満足度が高いんだがなぁ...
今回自分たちがやったものはまだそのレベルまでいってないということでした。

数学っぽいのではGAN周りの理解を深めるために関数解析とかを少しやったりしたが、これはまだまだ表面的な感じ。
幾何・代数・解析としたときに解析（もうちょっと特定すると測度論とか実解析）が一番手薄だなという物理の人は多そう（個人の感想です）。

この年にやった内容のかなりの部分は発表資料があって全部公開してるし、もう疲れたのでこんなもんで終わらせておこう。

## まとめ
業務で機械学習案件に従事する人がどんな理論的なトピックを扱ってきたのかを知るのは面白そう。
とりあえず自分のを簡単にまとめてみた。途中から飽きて明らかに手を抜いているが、それはそういうものだ。

---
---
<br>

