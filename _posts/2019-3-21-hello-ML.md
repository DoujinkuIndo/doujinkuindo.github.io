---
layout: post
title: Getting started with Machine Learning 
---

#### What is Machine Learning? 
It is a great topic to debate while drinking a beer. But apart from that, it is exacly what it sounds. Making a machine able to answer some questions/solve some problems.


#### What problems can be solved?

Things like classification, identification, projection, anomaly detection, noise reduction.

#### Classification 

One of the common application of ML is classification. Taken from Wikipedia:

>In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.

So I'll try to produce a kind of hello world in terms of ML classification algorithm.

#### Tools

Data Science currently evolves around a few programming languages (Python, R, Scala, Matlab) and ready made API from Amazon, Google, Microsoft and alike.

For this tutorial I will try to implement a classification algorithm using Python and the commonly used library [scikit](https://scikit-learn.org/stable/index.html). The installation requires other packages as well, so I found [Anaconda](https://www.anaconda.com/distribution/) as a ready to use platform.

~~As an editor I will use [Jupyter Notebook](https://jupyter.org/) whichis embedded in Anaconda and runs in your browser.~~  
Decided to use [Spyder](https://docs.spyder-ide.org/) instead, also bundled in Anaconda.

![Spyder screenshot](https://vladcozma.github.io/blog/images/hello-ml/hello-ml-02.jpg "Spyder")  




#### Problem

Given a set of messages and a set of categories, train the machine to output categories to new messages:

 | Message            | Category    |  
 | ------------------ | :---------: |  
 | I want a new car   |  shopping   |  

 | Message            | Category    |  
 | ------------------ | :---------: |  
 | I want speed       |  ?          |  
 

This will be a *supervised* classification, since I will first feed a data set with classifications already done from which it will learn to do new ones. Therefore I used two datasets. ```training-dataset.csv``` and ```validation-dataset.csv```. After training the model with the training dataset I would expect it will produce de same labels found in the validation dataset. that is because 
> Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. { https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation}.

In case you only have one dataset, you can manually split it in two, or use 
```python
from sklearn.model_selection  import train_test_split
# this method split one dataset into train set and validation set
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33) 
``` 
	
And you will have two datasets, train and test. Playing with the parameters will give you different accuracy levels.


##### Step 1 - loading data

I found a library, [Pandas](http://pandas.pydata.org/), which provides data analysis structures and tools

```python
 import pandas as pd
 trainset = pd.read_csv("training-dataset.csv",sep=";")
 print('Training dataset loaded. Description below:\n')
 trainset.info()
 print('\n')
```

```
Training dataset loaded. Description below:

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 889 entries, 0 to 888
Data columns (total 3 columns):
Message                 889 non-null object
Additional Condition    889 non-null object
Label                   889 non-null object
dtypes: object(3)
memory usage: 20.9+ KB
```
I loaded the training dataset in a pandas dataset type (basically a matrix). Same goes for validation set.

##### Step 2 - creating an estimator

Wikipedia [says](https://en.wikipedia.org/wiki/Estimator) an estimator is a rule to calculate an output based on an input. 

In scikit-learn, an estimator for classification is a Python object that implements the methods fit(X, y) and predict(T).

The accuracy of your estimation highly depends on the chosen estimator. And the choice of estimator depends on the type and size of the problem. scikit-learn has some estimators embedded, as seen in the comparation found on [scikit](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) and looks like  

![Classification methods](https://vladcozma.github.io/blog/images/hello-ml/hello-ml-03.jpg "Classification")  

  
I will define a train method which will use the classifier's fit method to train, and will output an accuracy score. 

```
# train method
def train(classifier, message, label):

    classifier.fit(trainset.Message, trainset.Label)
    print("Accuracy: %s" % classifier.score(validationset.Message, validationset.Label))
    return classifier
```

To choose the right estimator, there's a cheet sheet available as well.  

![Choosing the estimator](https://vladcozma.github.io/blog/images/hello-ml/hello-ml-04.jpg "Estimators")  
  
So first I decide I'll use the Multinomial Naive Bayes

```python
from sklearn.naive_bayes import MultinomialNB
```  

Then the text must be split into a feature vector using
```python
from sklearn.feature_extraction.text import TfidfVectorizer
```
`TfidVectorizer` is a way of words extraction which also gives an importance rating to words discovered in the texts. See [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) for detailed explanation.

Both of the classes are transformers, which means we should execute them individually before we run the estimator to get the end result. scikit has built in a pipeline that does just that `class sklearn.pipeline.Pipeline(steps, memory=None)` which can be used as  
```python
pipeline = Pipeline([("transform1",transformer_1),
	("transform2",transformer_2),
	("estimator",estimator)])
```

My pipeline will, therefore, be:  
```python
trial = Pipeline([
    ('vectorizer', TfidfVectorizer()),
    ('classifier', MultinomialNB()),
])
```	 
and this pipeline will be the classifier used by the `train` method. So the whole code looks like 
```python
# train method
def train(classifier, message, label):
    classifier.fit(trainset.Message, trainset.Label)
    print("Accuracy: %s" % classifier.score(validationset.Message, validationset.Label))
    return classifier	

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline 

# a pipeline build
trial = Pipeline([
    ('vectorizer', TfidfVectorizer()),
    ('classifier', MultinomialNB()),
])
    
# training the model
classifier = train(trial, trainset.Message, trainset.Label)

# writing predictions
output = pd.DataFrame(classifier.predict(validationset.Message))
output = pd.concat([validationset.Message, output],axis=1)

with open('validation-results.csv', 'w+') as f:        
    output.to_csv(f, header=True, sep=';')    
```

Further steps can be taken in andjusting the pipeline for a better accuracy, with some suggestions [here](https://nlpforhackers.io/text-classification/).   
Code is available [here](https://github.com/VladCozma/ml-classification)
  
  

Referrences:  

+ (https://scikit-learn.org)

+ (https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a)

+ (https://nlpforhackers.io/text-classification/)
	
+ (https://www.dataquest.io/blog/pandas-cheat-sheet)


