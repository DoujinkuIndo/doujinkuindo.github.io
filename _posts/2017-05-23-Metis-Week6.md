---
layout: post
title: Project McNulty
---

*"Data that is loved tends to survive."*

*Kurt Bollacker, Data Scientist*


McNulty was the most interesting project so far! In preparation, during the last three weeks, we covered a lot of interesting topics. Supervised Machine Learning/Classification, setting up PostgreSQL on AWS, building front end application using flask, and creating awesome visualizations using D3js. Apart from familiarizing these data science toolkits, the primary objective was to get a hands-on experience in developing an end to end machine learning product prototype.

My project was on predicting Healthcare Claims outcome. Typically, a claims file from a hospital includes type of procedure performed, facility it was performed, total cost of the procedure, portion of the cost billed to insurance company, and the portion cost billed to the patient. I have acquired the dataset from the IPPS repository of data.gov. My original objective was to predict whether a new claim will get approved or not, given the wide cost variance across cities in USA, for a DRG procedure. However, given the very limited additional features available – zip code, hospital address, and number of similar procedures performed at the hospital – I wasn’t quite sure how my model performs.

I choose four features: DRG Classification, Total Estimated Payments for the Procedure, Patient Charges, State, Zip Code, and Number of procedures performed. Predictor was claims outcome (Accepted/Rejected). In the preliminary phase I have chosen Logistic Regression, KNeighborsClassifier, GaussianNB, DecisionTreeClassifier, and RandomForestClassifier. Out of these models LogisticRegression turned out to be the best and the most efficient one - Prediction accuracy of 0.849, Prediction Precision of 0.876, and Prediction Recall of 0.926. Considering the fact that I want to embed my model in a flask application I decided to go with Logistic Regression, and tune it further.

Feature engineering on my LogisticRegression became very challenging. With DRG Classification, State, Zip Code, and Number of procedures performed features scaled to 250+, but model performance improvement was less than 5% of the previous results. My preliminary assessment was that the additional features I have incorporated with feature engineering wasn’t helping the model at all. Or, it could be that the dataset itself is not the best one.

One thing I have noticed is health insurance companies uses many supplementary data points such as age of the person, previous medical history, hospital facility rank, and plan details of the patient to make a final decision on the claim, which makes perfect sense. However, such details are not available in the public dataset. So, there is not enough evidence or predictive power the model could bring with the given data set.

To summarize, healthcare transparency is a major topic these days. My data analysis revealed that many of the insurance companies charges wide ranging fee for a given procedure, within a given state, and within cities. So, a simple and intuitive smart-phone application with could help the patient to identify the right healthcare provider, hospital facility, and get to know how much it would cost for a given procedure would be awesome, I believe.

So, I will continue working on this as a side project. I will incorporate more lateral datasets, and my hope is to develop a Knowledge-based Recommendation System, and I want to share my ideas in one of the investigations I have signed up for.

So, please stay tuned…


Best!

Sathish
