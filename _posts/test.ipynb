{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many light verbs are too many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, let's grab a function to find the number of light verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's out text\n",
    "text = \"Call me Ishmael. Some years ago- never mind how long precisely- having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people's hats off- then, I account it high time to get to sea as soon as I can.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call', 'me', 'Ishmael', '.', 'Some', 'years', 'ago-', 'never', 'mind', 'how', 'long', 'precisely-', 'having', 'little', 'or', 'no', 'money', 'in', 'my', 'purse', ',', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore', ',', 'I', 'thought', 'I', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world', '.', 'It', 'is', 'a', 'way', 'I', 'have', 'of', 'driving', 'off', 'the', 'spleen', 'and', 'regulating', 'the', 'circulation', '.', 'Whenever', 'I', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth', ';', 'whenever', 'it', 'is', 'a', 'damp', ',', 'drizzly', 'November', 'in', 'my', 'soul', ';', 'whenever', 'I', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouses', ',', 'and', 'bringing', 'up', 'the', 'rear', 'of', 'every', 'funeral', 'I', 'meet', ';', 'and', 'especially', 'whenever', 'my', 'hypos', 'get', 'such', 'an', 'upper', 'hand', 'of', 'me', ',', 'that', 'it', 'requires', 'a', 'strong', 'moral', 'principle', 'to', 'prevent', 'me', 'from', 'deliberately', 'stepping', 'into', 'the', 'street', ',', 'and', 'methodically', 'knocking', 'people', \"'s\", 'hats', 'off-', 'then', ',', 'I', 'account', 'it', 'high', 'time', 'to', 'get', 'to', 'sea', 'as', 'soon', 'as', 'I', 'can', '.']\n"
     ]
    }
   ],
   "source": [
    "# Now let's find the tokens\n",
    "def tokinze_text(raw_text):\n",
    "    tokens = nltk.word_tokenize(raw_text)\n",
    "    return tokens\n",
    "\n",
    "tokens = tokinze_text(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['call', 'me', 'ishmael', 'some', 'years', 'ago-', 'never', 'mind', 'how', 'long', 'precisely-', 'having', 'little', 'or', 'no', 'money', 'in', 'my', 'purse', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore', 'i', 'thought', 'i', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world', 'it', 'is', 'a', 'way', 'i', 'have', 'of', 'driving', 'off', 'the', 'spleen', 'and', 'regulating', 'the', 'circulation', 'whenever', 'i', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth', 'whenever', 'it', 'is', 'a', 'damp', 'drizzly', 'november', 'in', 'my', 'soul', 'whenever', 'i', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouses', 'and', 'bringing', 'up', 'the', 'rear', 'of', 'every', 'funeral', 'i', 'meet', 'and', 'especially', 'whenever', 'my', 'hypos', 'get', 'such', 'an', 'upper', 'hand', 'of', 'me', 'that', 'it', 'requires', 'a', 'strong', 'moral', 'principle', 'to', 'prevent', 'me', 'from', 'deliberately', 'stepping', 'into', 'the', 'street', 'and', 'methodically', 'knocking', 'people', 'hats', 'off-', 'then', 'i', 'account', 'it', 'high', 'time', 'to', 'get', 'to', 'sea', 'as', 'soon', 'as', 'i', 'can']\n",
      "[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158]\n"
     ]
    }
   ],
   "source": [
    "# OK, that grabbed everything. Let's just extract the words\n",
    "words = []\n",
    "word2token_map = []\n",
    "# While we're going through everything, let's also extract the punctuation\n",
    "puncs = []\n",
    "punc2token_map = []\n",
    "for idx, token in enumerate(tokens):\n",
    "    # Use \"isalnum\" to check if character is alphanumberic (decimal or letter, aka not punctuation)\n",
    "    if token[0].isalnum() or (token in [\"'m\", \"'re\", \"'ve\", \"'d\", \"'ll\"]):\n",
    "        words.append(token.lower())\n",
    "        word2token_map.append(idx)\n",
    "    else:\n",
    "        puncs.append(token)\n",
    "        punc2token_map.append(idx)\n",
    "            \n",
    "print(words)\n",
    "print(word2token_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make a single function to determine the parts of speech\n",
    "# We'll use this to copy and paste from other notebooks\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "#from collections import Counter # Is this used?\n",
    "\n",
    "# First we break the text into tokens\n",
    "def tokinze_text(raw_text):\n",
    "    tokens = nltk.word_tokenize(raw_text)\n",
    "    return tokens\n",
    "tokens = tokinze_text(text)\n",
    "\n",
    "def mytagger(tokens):\n",
    "    '''This function inputs tokens'''\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    return tags\n",
    "\n",
    "tagged = mytagger(tokens)\n",
    "\n",
    "# Note that IN can be either a preposition or a conjunction, for now we're going to list it with the prepositions\n",
    "common_noun_pos = ['NN', 'NNS']\n",
    "common_nouns = []\n",
    "verb_pos = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "verbs=[]\n",
    "adjective_pos = ['JJ', 'JJR', 'JJS']\n",
    "adjectives = []\n",
    "pronoun_pos = ['PRP', 'PRP$', 'WP', 'WP$']\n",
    "pronouns = []\n",
    "adverb_pos = ['RB', 'RBR', 'RBS', 'WRB']\n",
    "adverbs = []\n",
    "proper_noun_pos = ['NNP', 'NNPS']\n",
    "proper_nouns = []\n",
    "conjunction_pos = ['CC']\n",
    "conjunctions = []\n",
    "preposition_pos = ['IN', 'TO']\n",
    "prepositions = []\n",
    "interjection_pos = ['UH']\n",
    "interjections = []\n",
    "modal_pos = ['MD'] # But these are also verbs, so let's make sure they show up as such\n",
    "modals = []\n",
    "tagged_other_pos = ['CD', 'DT', 'EX', 'FW', 'LS', 'PDT', 'POS', 'RP', 'SYM', 'WDT']\n",
    "tagged_others = []\n",
    "other = []\n",
    "\n",
    "for idx, token in enumerate(tagged):\n",
    "    if token[1] in common_noun_pos:\n",
    "        common_nouns.append(token)\n",
    "    elif token[1] in verb_pos:\n",
    "        verbs.append(token)\n",
    "    elif token[1] in adjective_pos:\n",
    "        adjectives.append(token)\n",
    "    elif token[1] in pronoun_pos:\n",
    "        pronouns.append(token)\n",
    "    elif token[1] in adverb_pos:\n",
    "        adverbs.append(token)\n",
    "    elif token[1] in proper_noun_pos:\n",
    "        proper_nouns.append(token)\n",
    "    elif token[1] in conjunction_pos:\n",
    "        conjunctions.append(token)\n",
    "    elif token[1] in preposition_pos:\n",
    "        prepositions.append(token)\n",
    "    elif token[1] in interjection_pos:\n",
    "        interjections.append(token)\n",
    "    elif token[1] in modal_pos:\n",
    "        modals.append(token)\n",
    "    elif token[1] in tagged_other_pos:\n",
    "        tagged_others.append(token)\n",
    "    else:\n",
    "        other.append(token)\n",
    "    \n",
    "\n",
    "parts_of_speech = [common_nouns, verbs, adjectives, pronouns, adverbs, proper_nouns, conjunctions, prepositions, interjections, modals]\n",
    "   \n",
    "# Apped modals to verbs\n",
    "# Create nouns that is both proper nouns and common nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'am', \"'m\", 'is', 'are', \"'re\", 'wa', 'were', 'been', 'have', 'ha', 'had', \"'ve\", 'do', 'doe', 'did', 'done', 'go', 'goe', 'went', 'gone', 'give', 'gave', 'given', 'put', 'take', 'took', 'taken', 'feel', 'felt', 'begin', 'began', 'begun', 'get', 'got', 'make', 'put', '']\n"
     ]
    }
   ],
   "source": [
    "# Let's grab our list of all the light verbs\n",
    "file = 'corpora/light_verbs.txt'\n",
    "with open(file, 'r') as f:\n",
    "    light_verbs = f.read().splitlines()\n",
    "print(light_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# But you should be comparing stems, so find the stems for all these first\n",
    "# Before we do any stemming, let's make a list of all the irregular word stems\n",
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'StyleStudio/corpora/irregular-stems')) as f:\n",
    "    dict_irregular_stems_lines = f.read().splitlines()\n",
    "    dict_irregular_stems_draft = [line.split(',') for line in dict_irregular_stems_lines]\n",
    "    dict_irregular_stems = {}\n",
    "    for stem_old, stem_new in dict_irregular_stems_draft:\n",
    "        dict_irregular_stems[stem_old] = stem_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's create a stemmer\n",
    "stemmer = nltk.PorterStemmer()\n",
    "def stem_better(word):\n",
    "    stem = stemmer.stem(word.lower())\n",
    "    if stem in dict_irregular_stems:\n",
    "        stem = dict_irregular_stems[stem]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OK, now let's put all the stems inside the data dictionary\n",
    "data = {}\n",
    "stems = [stem_better(word) for word in words]\n",
    "data['stems'] = [None] * len(tokens)\n",
    "for idx, stem in enumerate(stems):\n",
    "    data['stems'][word2token_map[idx]] = stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# OK, now we've got our list of light verb stems, and our stems of all our words (data['stems']), \\n# let's see what light verbs we have\\n\\nlt_vrb = []\\nfor x in range(len(verbs)):\\n    if verbs[x] in light_verbs:\\n        lt_vrb.append(verbs[x])\\nprint(lt_vrb)\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# OK, now we've got our list of light verb stems, and our stems of all our words (data['stems']), \n",
    "# let's see what light verbs we have\n",
    "\n",
    "lt_vrb = []\n",
    "for x in range(len(verbs)):\n",
    "    if verbs[x] in light_verbs:\n",
    "        lt_vrb.append(verbs[x])\n",
    "print(lt_vrb)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#light_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, None, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, None, False, False, False, False, False, False, False, False, None, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, None, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, None, False, False, False, False, False, False, False, False, False, None, False, False, True, False, False, None, False, False, False, False, False, None, False, False, False, False, False, False, False, False, False, None, False, False, False, False, False, False, False, False, False, False, None, False, False, False, False, False, True, False, False, False, False, False, False, None, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, None, False, False, False, False, None, False, False, False, None, False, False, False, False, False, False, True, False, False, False, False, False, False, False, None]\n"
     ]
    }
   ],
   "source": [
    "# Now let's enumerate through the word tokens and find the light verbs\n",
    "light = [None] * len(tokens)\n",
    "verb_pos = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "for idx_word, word in enumerate(words):\n",
    "    idx = word2token_map[idx_word]\n",
    "    #print(tagged[idx][1])\n",
    "    light[idx] = (tagged[idx][1] in verb_pos) and (data['stems'][idx] in light_verbs)\n",
    "print(light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['having', 'is', 'have', 'is', 'get', 'get']\n",
      "[12, 48, 52, 75, 111, 151]\n"
     ]
    }
   ],
   "source": [
    "# Now let's print all the light verbs\n",
    "from itertools import compress\n",
    "light_verbs_in_text = list(compress(tokens, light))\n",
    "print(light_verbs_in_text)\n",
    "# Now let's look where they are in the text\n",
    "light_verbs_posi = [i for i, x in enumerate(light) if x]\n",
    "print(light_verbs_posi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next: find and print the entire sentence\n",
    "# Then: Compare to other famous authors, make a cool graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's grab this to look at sentences\n",
    "punct_error_re = re.compile('^([\"\\]\\)\\}]+)(?:[ \\n]|$)')\n",
    "ellipsis_re = re.compile('\\.\\.\\.[\"\\(\\)\\[\\]\\{\\} ] [A-Z]')\n",
    "newline_re = re.compile('\\n[\"\\(\\[\\{ ]*[A-Z]')\n",
    "empty_sent_re = re.compile('^[\\n ]*$')\n",
    "def sentencize(text):\n",
    "    # tokenize text into sentences\n",
    "    text_eg_ie = text.replace('e.g.', 'e.---g.').replace('i.e.', 'i.---e.')\n",
    "    sents_draft = nltk.sent_tokenize(text_eg_ie)\n",
    "    for idx, sent in enumerate(sents_draft[:]):\n",
    "        sents_draft[idx] = sents_draft[idx].replace('e.---g.', 'e.g.').replace('i.---e.', 'i.e.')\n",
    "        print(idx)\n",
    "        if idx > 0:\n",
    "            punct_error = punct_error_re.findall(sent)\n",
    "            if punct_error:\n",
    "                sents_draft[idx-1] += punct_error[0]\n",
    "                sents_draft[idx] = sents_draft[idx][len(punct_error[0])+1:]\n",
    "                print(idx)\n",
    "\n",
    "    # separate sentences at ellipsis characters correctly\n",
    "    sents_draft_2 = []\n",
    "    for sent in sents_draft:\n",
    "        idx = 0\n",
    "        for ellipsis_case in ellipsis_re.finditer(sent):\n",
    "            sents_draft_2.append(sent[idx:(ellipsis_case.start() + 3)])\n",
    "            idx = ellipsis_case.start() + 3\n",
    "            print(idx)\n",
    "        sents_draft_2.append(sent[idx:])\n",
    "\n",
    "    # separate sentences at newline characters correctly\n",
    "    sents = []\n",
    "    for sent in sents_draft_2:\n",
    "        idx = 0\n",
    "        for newline_case in newline_re.finditer(sent):\n",
    "            print(\"Now Im here\") # is this even being used?\n",
    "            sents.append(sent[idx:(newline_case.start() + 1)])\n",
    "            idx = newline_case.start() + 1\n",
    "            print(idx)\n",
    "        sents.append(sent[idx:])\n",
    "\n",
    "    # delete empty sentences\n",
    "    sents = [sent for sent in sents if not empty_sent_re.match(sent)]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "sents = sentencize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call me Ishmael.',\n",
       " 'Some years ago- never mind how long precisely- having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.',\n",
       " 'It is a way I have of driving off the spleen and regulating the circulation.',\n",
       " \"Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people's hats off- then, I account it high time to get to sea as soon as I can.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What I'm trying to do now is find and print the sentence that goes with the word I've suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OK, so next we'll assign every word a sentence number\n",
    "# tokenize sentences into words and punctuation marks\n",
    "sents_tokens = [nltk.word_tokenize(sent) for sent in sents]\n",
    "tokens = [token for sent in sents_tokens for token in sent]\n",
    "data['values'] = tokens\n",
    "# Now let's give every word a sentence number. All the words in the first sentence have a value\n",
    "#  of 1, second sentence 2, etc.\n",
    "data['sentence_numbers'] = [(idx+1) for idx, sent in enumerate(sents_tokens) for token in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['having', 'is', 'have', 'is', 'get', 'get']\n",
      "[12, 48, 52, 75, 111, 151]\n",
      "[1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "#OK, now we've got our light words:\n",
    "print(light_verbs_in_text)\n",
    "# We've also got our position of each word\n",
    "print(light_verbs_posi)\n",
    "# And, finally, we've got our sentence numbers:\n",
    "print(data['sentence_numbers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OK, there might be some extra stuff above, but let's get back on topic\n",
    "# We're going to find the ratio of light verbs to total words for this book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the beginning of Great Expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's grab some of Great Expectations, maybe the first 10000 characters?\n",
    "import re\n",
    "from urllib import request\n",
    "\n",
    "# Now let's grab some text from Great Expectations\n",
    "url = 'http://www.gutenberg.org/files/1400/1400-0.txt'\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some text we'll start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = raw[886:] # Jumping to where it actually starts\n",
    "# Let's clean out all the annoying line markings\n",
    "text = text.replace('\\r', '')\n",
    "text = text.replace('\\n', ' ')\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see what percentage of these words are light verbs\n",
    "# First we have to tokenize\n",
    "tokens = tokinze_text(text)\n",
    "# Then we have to extract the words. We'll ignore punctuation for now\n",
    "words = []\n",
    "word2token_map = []\n",
    "for idx, token in enumerate(tokens):\n",
    "    # Use \"isalnum\" to check if character is alphanumberic (decimal or letter, aka not punctuation)\n",
    "    if token[0].isalnum() or (token in [\"'m\", \"'re\", \"'ve\", \"'d\", \"'ll\"]):\n",
    "        words.append(token.lower())\n",
    "        word2token_map.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we have to tag all the words\n",
    "tagged = mytagger(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll make a function out of the last step\n",
    "def find_light_verbs(tokens, words):\n",
    "    # Let's enumerate through the word tokens and find the light verbs\n",
    "    light = [None] * len(tokens)\n",
    "    verb_pos = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    for idx_word, word in enumerate(words):\n",
    "        idx = word2token_map[idx_word]\n",
    "        #print(tagged[idx][1])\n",
    "        light[idx] = (tagged[idx][1] in verb_pos)# and (data['stems'][idx] in light_verbs)\n",
    "    return light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "light_verbs = find_light_verbs(tokens, words) # using light and light interchangeably; bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36584 light verbs in this text\n",
      "That is 15.89% of words in this text\n"
     ]
    }
   ],
   "source": [
    "# Let's delete all the None values\n",
    "cleaned = [x for x in light_verbs if x is not None]\n",
    "print(\"There are {} light verbs in this text\".format(sum(cleaned)))\n",
    "print(\"That is {:.2%} of words in this text\".format(sum(cleaned)/len(light_verbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's do the same for Moby Dick\n",
    "url = 'http://www.gutenberg.org/files/2701/2701-0.txt'\n",
    "beginning = 28876\n",
    "def light_verbs_in_book(url, beginning):\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    # Here is some text we'll start with\n",
    "    text = raw[beginning:] # Jump to the beginning\n",
    "    # Let's clean out all the annoying line markings\n",
    "    text = text.replace('\\r', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "\n",
    "    # First we have to tokenize\n",
    "    tokens = tokinze_text(text)\n",
    "    # Then we have to extract the words. We'll ignore punctuation for now\n",
    "    words = []\n",
    "    word2token_map = []\n",
    "    for idx, token in enumerate(tokens):\n",
    "        # Use \"isalnum\" to check if character is alphanumberic (decimal or letter, aka not punctuation)\n",
    "        if token[0].isalnum() or (token in [\"'m\", \"'re\", \"'ve\", \"'d\", \"'ll\"]):\n",
    "            words.append(token.lower())\n",
    "            word2token_map.append(idx)\n",
    "            \n",
    "    # Now we have to tag all the words\n",
    "    tagged = mytagger(tokens)  \n",
    "    \n",
    "    light_verbs = find_light_verbs(tokens, words)\n",
    "    \n",
    "    cleaned = [x for x in light_verbs if x is not None]\n",
    "    print(\"There are {} light verbs in this text\".format(sum(cleaned)))\n",
    "    print(\"That is {:.2%} of words in this text\".format(sum(cleaned)/len(light_verbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#light_verbs_in_book(url, beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33245 light verbs in this text\n",
      "That is 13.11% of words in this text\n"
     ]
    }
   ],
   "source": [
    "# Now let's do the same for Moby Dick\n",
    "url = 'http://www.gutenberg.org/files/2701/2701-0.txt'\n",
    "beginning = 28876\n",
    "\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "# Here is some text we'll start with\n",
    "text = raw[beginning:] # Jump to the beginning\n",
    "# Let's clean out all the annoying line markings\n",
    "text = text.replace('\\r', '')\n",
    "text = text.replace('\\n', ' ')\n",
    "\n",
    "\n",
    "# First we have to tokenize\n",
    "tokens = tokinze_text(text)\n",
    "# Then we have to extract the words. We'll ignore punctuation for now\n",
    "words = []\n",
    "word2token_map = []\n",
    "for idx, token in enumerate(tokens):\n",
    "    # Use \"isalnum\" to check if character is alphanumberic (decimal or letter, aka not punctuation)\n",
    "    if token[0].isalnum() or (token in [\"'m\", \"'re\", \"'ve\", \"'d\", \"'ll\"]):\n",
    "        words.append(token.lower())\n",
    "        word2token_map.append(idx)\n",
    "\n",
    "# Now we have to tag all the words\n",
    "tagged = mytagger(tokens)  \n",
    "\n",
    "light_verbs = find_light_verbs(tokens, words)\n",
    "\n",
    "cleaned = [x for x in light_verbs if x is not None]\n",
    "print(\"There are {} light verbs in this text\".format(sum(cleaned)))\n",
    "print(\"That is {:.2%} of words in this text\".format(sum(cleaned)/len(light_verbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Should this be out off all words or just verbs? I think verbs\n",
    "# OK, so let's look at how many are words. We should also do a part of speech break down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's do the part of speech break down now\n",
    "# We're doing this on the Part of Speech notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From the Part of Speech notebook:\n",
    "# OK, but we haven't made a way to visual the results yet. Let's do that now with a pie chart\n",
    "import matplotlib.pyplot as plt\n",
    "def pos_plotter(parts_of_speech):\n",
    "    '''This function inputs a specific list of lists that is shown in the Part of Speech notebook'''\n",
    "    all_labels = ['common_nouns', 'verbs', 'adjectives', 'pronouns', 'adverbs', 'proper_nouns', 'conjunctions', 'prepositions', 'interjections', 'modals']\n",
    "    pos_dict = dict(zip(all_labels, parts_of_speech))\n",
    "    labels=[]\n",
    "    data=[]\n",
    "    for pos, lst in pos_dict.items():\n",
    "        if lst:\n",
    "            data.append(len(lst))\n",
    "            labels.append(pos)\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(data, labels=labels)\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8XHW9+P/X+8wkmUnSpnvTpktoKTRt05aCZS2FIrJU\nEBcE2RfBigpev3rtdbmMCj+j4AVRFFyQIF5BUNRLEe+V2patlqXQLWFturJ0TbPP9v79cU7KNM0+\nZ+bM8nk+Hnl0cuYs70nS93zmfT6LqCqGYRiG9yyvAzAMwzBsJiEbhmFkCJOQDcMwMoRJyIZhGBnC\nJGTDMIwMYRKyYRhGhjAJ2TAMI0OYhGwYhpEhTEI2DMPIECYhG0kRkWavYzCMXOH3OgAjO4mIAOJ1\nHIaRS0wLOc+JSI2IfCHh+5CIfFVEviYiL4jIOhH5jvNcpYi8JiIPABuAic72O0Rko4g8JSKjnW03\nisgm5/iHvHhthpFtTEI2HgY+nfD9p4FdwDRgPjAXOFZETnWenwb8TFVnquoWoAR4UVVnAiuBm539\nlgLHqOpsYEnqX4ZhZD+TkPOcqq4FxojIeBGZA+wDqoGPAGuBl4Hp2IkYYIuqrk44RRw7qQM8CJzi\nPF4H/E5ELgOiqX0VhpEbTA3ZAHgE+BRQjp1cJwPfV9V7E3cSkUqgpY9zdc7nuhg4FTgP+KaIVKuq\nScyG0QvTQjbATsIXYyflR4C/A9eISCmAiFSIyJgejrWc4wAuAZ4REQuYqKr/BL4OlAGlKYzfMHKC\naSEbqOpGERkC7FDVd4B3RKQKeN7uTEEzcBkQ6+bwFmC+iHwLeB+4CPABD4pIGXZPjLtUdX8aXoph\nZDUxK4YYhmFkBlOyMAzDyBAmIRuGYWQIU0M2skuozMK+STjc+Vewu97FuvwbB5qA3YQau6t9G0bG\nMTVkI3OEykqAI7H7PHd+TcROvsOBEcBQBvbJLg7sxb7hmPi1E3gN2AS8SajRdMkzPGcSspF+dit3\nBvZIwA8BVdjJd7xHEYWBN4A67AS9EVhNqHGrR/EYecokZCP1QmXFwInYo/gWAMeTHf2StwCrsIeE\nryLU+IbH8Rg5ziRkIzVCZZOA84GPAQuBAm8DcsU7wNPA34C/EGrc53E8Ro4xCdlwT6hsHh8k4bke\nR5NqEeCfwB+Bxwg17vI4HiMHmIRsJCdUNgX4LHApMMnjaLwSwy5t/BF4hFDj+x7HY2Qpk5CNgQuV\nFQEfB64DTsdMVJ8oAvwVuBf4B6FG8x/M6DeTkI3+C5VVYSfhK4CRHkeT8VbHqx67OPztZ4FfN9Qs\nNnN5GH0yCdnoW6jsBODbwLleh5JNzun4/lt1Onkq9gRMDwK3N9QsftPjsIwMZhKy0bNQ2enAt4BF\nXoeSbfbqkFfmddzb9cZmDPgt8L2GmsVvexCWkeFMQjYOFyo7B/gmcLLXoWSrmyNXrq6NnXVCD09H\n+SAxb05jWEaGMwnZ+ECo7ETgTuwRdMYgxVTeO6rjgZExfH3NFRMFarET85Y0hGZkOJOQDQiVjQd+\ngN11zfSYSNITsfkrb4h8eeEADmnH/vnXNNQsbk9RWEYWMAk5n9nd174CfIPsGMqc8VSJntxx166d\njBo3iMPfBm5qqFn8uNtxGdnBJOR8FSo7H/gvYKrXoeSSLfExqxeG7+ypdtxfjwM3mvpy/jEJOd+E\nykYC9/DBwqSGi5aEv7z2yfj8Y1w4VTvw/wHfb6hZbKYGzRMmIeeTUNm5wK+Bcq9DyUUd6n/76I4H\nprh82jXAZQ01i81Mc3nALOGUD0JlJYTK7gWWYZJxyvw+tmh7Ck47H1hbuXTZ9Sk4t5FhTAs514XK\nTgIewNSKU0qV1tkdv4w0UVKWwsv8BfhsQ83i3Sm8huEh00LOZaGyb2DPQmaScYqt1yNeSnEyBnta\n0/WVS5edleLrGB4xLeRcZK9Ndz/mxl3afKIjVP+yHjU9TZeLA//RULP4h2m6npEmJiHnmlDZEcCf\ngdleh5IvDmhww+yOX8/y4NIPAteZwSS5w5QsckmobBHwAiYZp9XPo+c3enTpy4CVlUuXDWYQipGB\nTAs5V4TKbgJuB/qaP8FwUVxlz/SO+0vDFBR5GMZO4IKGmsUveBiD4QLTQs4FobLbsScFMsk4zVbG\nZ2/wOBkDjAdWVS5ddr7HcRhJMi3kbBYqs7BH3V3ndSj5SJX46eEf7WjQcRO9jsURBS5vqFn8kNeB\nGINjWsjZKlTmx7mp43Uo+epdhr+UQckY7E9Iv6tcuuxarwMxBsck5Gxkz9L2R+AzXoeSz26LXOR1\nCN2xgF9WLl12k9eBGANnShbZxu5j/BfgDK9DyWcR9W07qqO2QrEyuVHzrYaaxbd6HYTRf5n8x2R0\nFSoLYM9HYZKxxx6LnfJ2hidjgFsqly77ptdBGP1nWsjZwq4Z/wk4z+tQ8p0qHcd2/Lx5L2UjvY6l\nn65rqFn8K6+DMPqW6e/wBlBdWy3tIr/EJOOM8LpOeDGLkjHAPZVLl13gdRBG30xCzg4/OGvi+NnN\nIk1eB2LAd6NXpHoSIbf5gN9XLl22wOtAjN6ZhJzhqmurbwC+ttfnm7doUsX2XT5rl9cx5bMWLap7\nNj7Li3krkhUA/lq5dFm114EYPTMJOYNV11afB9zV+X2bZVV9ZGJF6+YC/1YPw8prv46ds8frGJIw\nDHiycumyCq8DMbpnbuplqOra6rnAs0Bx1+dEddcD77y3Z25HOF3TPRqAKo0zOn5T0EbRYb+TLLMa\nWNhQszjsdSDGoUwLOQNV11aXAn+gm2QMoCKjLx83tuIfxcG16Y0sv62Oz3g1B5IxwAnAHYM9WESu\nEJF1IvKqiPxWRCpFZLmz7SkRmeTsd7+I/FxEVovI2yJymojcJyJ1InJ/wvmaReQ2EdkoIv8Qkfki\nssI55nxnn4CI/EZE1ovIWhE53dl+lYj8SUSeFJE3RKTXOaKda93qxL5aRMY623t7DZ9KPN759zQn\nxkdFpF5Efici4jxXIyKbnHPdPpCfrUnImeleYFqve4gM+bcxo2b+bmjp8+kJKb+pojdHr8ykYdLJ\nuqFy6bLLB3qQiMwEvgUsUtU5wE3AT4BaVZ0N/I6EMhswHDgR+Dfgr9hvBDOBahGZ6+xTAixX1ZlA\nE3ALcCbwceC7zj5fAFRVq7FHqNaKSMB5bi5wEVANXCQivf2eSoDVTuyr+GDqgd5eQ0+OAb4MzACm\nACeLyEgn7pnOuW7px3kOMgk5w1TXVl8DXNKvnUUKa0YMP/62EcNWpTYqYw9DX3ldJx7hdRwuu7dy\n6bI5AzxmEfCIqu4GUNW92An3v53nfwuckrD//6hdF10PvKeq61U1DmwEKp19wsCTzuP1wEpVjTiP\nO/c5BXvuFlS1HtgCHOU895SqNqpqO7AJmNxL/GHgcefxSwnn7+019GSNqm53Xs8rzrkagXbg1yLy\nCaC1H+c5yCTkDFJdWz0D+526/0SsB8qGnnrTmFErUhKUAcAd0U/lYr01CPyxcumyYSm8Rofzbzzh\ncef3ndPFRvSDm1kH93MSXX+mlE08b6yPYxKv1de+YM+gZwGIiAUU9nZdVY1irxT+KPBRPnij6ReT\nkDNEdW11kF7qxn1ZXlJ82kXjxz4ds/8wDBfF1Hrn97FFx3kdR4pMBX49gP2XAxc6H80RkRHAc8DF\nzvOXAk+7GqHtaefciMhRwCTgNRfP39NraACOdR6fDxT0dhIRKQXKVPUJ7DLNgD6BmIScOX6CXVsb\ntE1FRQvOnTD+xQ7BrLHmomXx41+PY/m8jiOFPlG5dNll/dlRVTcCtwIrReRV4L+ALwFXi8g64HLs\nurLbfgZYIrIeeBi4SlU7+jhmIHp6Db8EFjqv9USgpY/zDAEed87zDPCVgQRhur1lgOra6kuwbyS4\nYmgs9uoT23dWlsU120aUZRxVIid2/HTvu4wY63UsKbYfqG6oWbzd60DymWkhe6y6troc+Lmb5zzg\n8805Y2LF++/6fO+6ed581KDlL+ZBMgZ70MgvvQ4i35mE7L3bgaFun7TDsqadPXF89PWCgs1unzuf\n3Bq9JBf6HffX2ZVLl13hdRBuEJF/icgrXb4yfti4KVl4qLq2+lRgZSqvIap7fvXu++/Ob+9Iqj6d\nj9q14K3pHbVTvY4jzfYCMxpqFr/ndSD5yLSQPVJdW+0Hfprq66jIyGvLx1QuKyl+MdXXyjUPxj68\nw+sYPDAC+JHXQeQrk5C980XskUWpJ1KydPTIOb8uG/psWq6XA1RpvjP6ybl975mTLqlcumy+10Hk\nI5OQPeDcyPtOWi8qUnDn8LKTvjNyeEpLJLniFT1ybTPFrtf2s4SQxFwXxuCZhOyN20jBjbw+icij\nQ4cs/NzY0SsVzM2DXoQiV+RDz4renFS5dFlGLqudy8xNvTSrrq1egD2piaemhcPP/GHHuyf4+zc0\nNa80asn6OR2/zPg78mmwBZjeULPYDDRKE9NCTr87vQ4A4I3CwlPOmjh+bZvIgCY/yQd3Rz9mlsqy\nTcYe/mukiWkhp1F1bfU5wBNex5GoJB7f+LdtO8cNj8dHeB1LJoir7Dq6o7Ysgr+w773zQhMwpaFm\n8W6vA8kHpoWcXv/hdQBdtVjWzA9Pqti/3e/Lxy5eh1keP2aTScaHGALc6HUQ+cK0kNOkurb6FFIz\nC5YrLNV3fr/z3ZYZ4ciRXsfiFVXip4bvfGebjjFrzh1qHzCpoWZxs9eB5DrTQk6fjGsdJ4qLjLt4\nfPmop4OBdV7H4pWdjHzRJONuDQc+53UQ+cAk5DSorq2eA5zrdRx9UZFhN4wdPe1PpSVrvI7FCz+I\nXJzLU2wm6yuVS5eZUk6KZVxCdhYt/KnzeImIDHiyExEZJiI3JHw/XkQedTPOAVrq4bUHRiR486gR\nx/50WFnGlldSIaK+LX+NnzTP6zgy2HjgSq+DyHUZl5ATqeo9qvrAIA4dBhxMyKq6U1U/1cv+KVNd\nWz0VuNCLaw+aiO/e4WULlo4emTej+h6JLWwAe9Vgo0f/Xrl0mfkUkUJpT8gi8mcReclZ8vt6Z9vV\nIvK6iKwBTk7YNyQiX3UeT3WW+n5JRJ4WkenO9rEi8pizrPerInISUANMdabcu81Z4nuDs/9qZ+Xc\nzmusEJHjRKTEWaJ8jbPM+Mec52c6215xlvXufTXow/07kJV/xMtKSxZeVT4m50f1qdL2w+hFs72O\nIwscCZztdRC5zIsW8jWqeixwHHCjiFRgz+twMvZKrzN6OO4XwJecY7+KvaQL2Mt1r3SW9Z6HvZrt\nUuAtVZ2rql/rcp6HgU8DiMg4YJyqvgh8E3sp8vnA6cBtIlICLAF+rKpznZj7vaJCdW11GfZyMFnr\npWBg4XkTxj0ftlfrzUmbdPJL+xky3Os4ssQ1XgeQy7xIyDc661OtBiZiJ6wVqrpLVcPYCfMQzsKB\nJwGPiMgrwL3AOOfpRTgrbqhqTFUb+7j+H4DO8sWnsVeHBfgIsNQ5/woggL2Q4vPAN0Tk68BkVW0b\nwGu9GHtl36y2paDgpDMnVWxoFsnJEWzfjVxuBsX033mVS5eN8TqIXJXWhCwipwEfBk50WrRrgfp+\nHGoB+50Wb+dX1WBiUNUdwB4RmQ1cxAdvAAJ8MuH8k1S1TlX/G3u12TbgCRFZNIDLXTWYGDPRXp9v\n3qJJFdt3+6xdXsfipmYNbPqXzujpU5lxuAKy/FNfJkt3C7kM2KeqrU4N+ATsFuRCERkpIgV0cwNM\nVQ8Am0XkQgCxdS6v/RTweWe7T0TKsId7Dukljoexa7tlqtrZ7/bvwJdE7Bs7InKM8+8U4G1VvQv4\nC9CvWmN1bfXRzuvLGW2WVXXmxIrWBr9/q9exuOUX0cV7vY4hC5myRYqkOyE/CfhFpA77xttq4B0g\nhF0aeBao63JM5w2lS4FrnXLHRuBjzvabgNOd5cFfAmao6h7gWRHZICK3dRPHo9jlhD8kbPse9rv/\nOhHZ6HwPdlljg1PKmAX0t9dHTnYRiopMPn/CuOCrRYWveR1LsuLKvl/EPnqs13FkoRmVS5flVGMj\nU2T00GkR+Qnwsqr+xutYBqq6tvpt4Aiv40gZ1QN3vr/7rTNa247xOpTBejo2a+XlkW8s9DqOLPXL\nhprF13sdRK7J2H7IIvI94Hjgr17HMlDVtdUfIpeTMYDI0C+PGTXjd0NLn/c6lMFQRUPRKyu9jiOL\nfaxy6bKMzR/ZKmN/oKr6bVWd75Qfsk1+rLQgUlQzYvjxt48Y5vmE+wO1i7KX39KKyV7HkcXGYDeY\nDBdlbELOVtW11UK2jcxLhohVWzb01C+PGbXC61AG4kfRC2Nex5ADzvc6gFxjErL7qrH7L+eVp0qK\nT/vM+LFPxyDjE11UrR2PxE4zN/OSd57XAeQak5Ddd7rXAXhlQ1HRgsUTxr/QIWT0Gmz/Ez/xzThW\nVg5nzzAzK5cum+J1ELnEJGT3neZ1AF7aUeA/4YyJFa8dsKSvEZOeUCX8/cglZiCIe0zZwkUmIbvI\nqR+f6nUcXmv0+eYsmljx/ns+33tex9LVWzruxfcZPtrrOHLIYq8DyCUmIbtrNmDmRQA6LGvaWRPH\nR94sKNjsdSyJbo1eVup1DDnmBDMlp3tMQnZX3taPuxMTmfCJivKhLwSKNnkdC0CbFr7xz/gxZppN\nd5Vi38g2XGASsrtO8zqATKMiI68pHzP5iZLiF72O5YHYR971OoYcdaLXAeQKk5BdUl1bbWHqx90T\nKfn66JFz7isb8qxXIajSdFf041k7zDvDneR1ALnCJGT3zMZendfojkjBHcOHnXTLyOGeLAv1kh61\ntoWgqR+nhmkhu8QkZPeYP8q+iMjDQ4csXDJ2dNqXhbo5cuX4dF4vz0w1k9a7wyRk90z3OoBs8Wxx\ncOGnxpc/G4VoOq63T0tf3ahHHJmOa+UxM6+FC0xCds9RXgeQTV4vKjzlrInj17aJtKb6Wj+Jfjzl\n1zBMg8QNJiG7xyTkAXrf7//QokkVb++3rH2pukZM5f0HYmcel6rzGweZv38XmITsgura6kLATOU4\nCM2WNeuMiRV7d/h9O1Nx/v+LH1sXxV+QinMbhzAJ2QUmIbtjKmBGKw1S2JKp504YL5sKC95087yq\nxL4XudwkivQwP2cXmITsDvPHmKS4yLiLx5ePeiYYWNf33v2zTUe/sIPR49w6n9Gr8sqly4Z6HUS2\nMwnZHUd7HUAuUJFhnx87etpjpSVr3Djf96OXFLlxHqPfTMMkSSYhu8P8IbpFJPifo0Yce/ewsqeT\nOU2H+jf/LT5/rlthGf1iuhYmySRkd5g/RDeJ+O4ZXrbgG6NGDnpU30Ox07eBiJthGX0yg0OSZBKy\nO8z8uinwP0NKFl5dPmbAo/pUaf1R9MI5qYrL6NFIrwPIdiYhu2OI1wHkqheDgYXnV4x7Pgzh/h6z\nQY94+QClZamMy+iWSchJMgnZHWbSmhRqKCw46cxJFRtaRJr7s//NkStHpTomo1smISfJJGR3mISc\nYnt9vnmLJlVs2+2zdvW2X5MGN76sR5lhvN4wq+UkySTkJFXXVhcBZiRYGrRaVtWZEytat/j923ra\n557oefvTGZNxCNNCTpJJyMkz9eM0iopMPn/CuKJXiwpf6/pcXGXvL2OLzbwV3jEJOUkmISfPlCvS\nLC4y5rJxY8ctLw6+krh9Vbx6fZgCMxjEO+ZnnySTkJNnWsheEBl605hRVQ8NKV0NoEr8O9ErjvA6\nrDxn5nNJkt/rAHKAScheESm6deTw+Tv8/lWX7pXizTrelCu8ZRJykkxCTp75mOYyUY0HOmgu7qC1\nuIO20nbaS9q1o7SN8JA2oqXtGittI17SjpZ0IMUdxfK7Yz75+qdGrd5XFGiKBILN8WDwgBUMNPuD\nhS0BvxUziSINYmq1wmKvw8hqJiEnr98DFnKVP6bhYActxR20lNjJs720nXDJBwk07iRQKemAYIf6\nAmF8gQgFhVEK/TGK/DGClhIUpUSgGBjqfPVp16jZa6eHjxvR0v7qkfGOYUfEGofRDHR2Wvb7OxqL\nilp2B4JNjcFgU1sw0BQJBJqlsKi1sKCgvdjni5aJxEeJmPsBSdrrxUVFpAE4TlV3J7NPJjAJOXlt\nXgcwUIURbXWSZ1txB22lbdpR2k64tI1IabtGS9uIl7ZBSQda3K5WMIwVCFNQFMVfEKWoIEbAFycg\ncUoESgQKsb88WXV70/QrCgva9wz9SGRO85MFr8SRQ++NRKNFZdFoUVlLS+/dZH2+SHNRUcuuQKCp\nMRhsag0EmyLBQBOFRa3+goL2Yr8/MtRJ3GYUYPfiXgeQ7UxCTl57Kk8uqvFgB83BDlpKOj++2wk0\nUtpGZEibxkvaiZe2oyXtSHGHWk7r018YobAgRqE/RtCKE7SUYqDUaYEWpzLudNk+fsHqmD94QiTq\ne2N8bFhRhW/E0zt8excO5lyxWEFpa+uw0tbWYb3uZ1nRNjtxN+8PBJtagsGmSCDQFC8qavEXFrYH\nfb7IUMuKjRTJu4ES/f6/ICKVwJPAauAk4AXgN8B3sCcpuhR4E7gPmAK0Ater6joRGQn8HqgAngck\n4bx/BiYCAeDHqvqLLtctAf4ATMCueX9PVR8e+EtNDZOQk3fIH6E/quFgmObiDtpK2mktadf2IW1E\nStoJD2kjVtqmMSd5anG7SnEYXyCMr+iDj+8Bf4wiSykWpVQgyAA+vucTRWJvHPnJMQDiGzVhf3jX\ni2fK7OMfsFZujoumrMdFPO4PtrWVTWprK5tEL6sBisTCRUWtu4oCzfuDwabmYKCpIxBsihcVtfgK\nC9sCfn9kqGVFRwAjRXKix1PLAPc/ErgQuAY7IV8CnAKcD3wD2AasVdULRGQR8AAwF7gZeEZVvysi\ni4FrE855jaruFZEg8IKI/FFV9yQ8fzawU1UXA4hIRn3aMQk5Sbf/MtpUsYcdCa3PQswQ0rTYXLn4\nObUKFgCIFAR3tDa0jygqD/RUukg3VV9he/uQivb2IRWN+3tbuCQeLSxsez8QbN4XDDQ1BYNNHYFg\nU6yoqMUqLGwN+P3hUp8vNhx0tEhG/5/t11wjCTar6noAEdkIPKWqKiLrgUrsdSo/CaCqy0VkpIgM\nBU4FPuFsXyYiiW+LN4rIx53HE4FpQGJCXg/8SER+ADyuqknNu+22TP7lZoVJu2nC/uhkpFHMKmhr\nmHzWIfNQb2vdXFA9/AQmxEdWV8RHrBxs6SL9LH84XFIeDpeUH2gc28t+Gi8oaN8VCDTvCwSbDjg3\nKGNFgWarqLCt0F/QUWpZ0WEiOkaEwrSF/4HGAe7fkfA4nvB9HDs3RQZyMhE5DfgwcKKqtorICuzS\nxUGq+rqIzAPOBW4RkadU9bsDjDtlTEJOUlV9XXvd9KoOTPe3tHp92qfXINYhCbc5FrHiGttqiW/S\nmZHZxz9grXw7LjrFqxjdJ1YkEhwdiQRHNzX1PgW339++LxBo2R0I2Ik7EGyKBgLNVlFRa4Hf31Hi\n9CwZLeLqvYSBJuS+PI1dS/6ek2x3q+oBEVmFXd64RUTO4YObyWXAPicZTwdO6HpCERkP7FXVB0Vk\nP/BZl2NOiknI7tgDjPc6iHwR8Qcb3yk/8bAJ6C3/GPaHd20dUVQ+yY8vcFZkbuvfCtZ6XrrwQjQa\nGN7cHBje3Nz79BI+X7gxEGjZEwg0NQaCTa3B4IFoINBMUVFrQUFBR7HPF+nsEtifAVDvuxP9QSHg\nPhFZh31T70pn+3eA3ztljueArc72J4ElIlIHvIZ9w7CrauA2EYljt8A/73LMSTEJ2R3bMQk5bTZV\nXbkWu8V0CMs3LritpT4yoqgcgIr4iFkT4iNWbPftPWxfwxaLFZa1tBSWtbT03mPRsiItRUWtuwPB\npv1BO3GHA4FmtRN3e9Dni5SB9DgLX1eq2gDMSvj+qh6eu6CbY/cAH+nh1Of0cL1K5+Hfna+MZBKy\nO7YB870OIh+0Fw17d8+IWcd395z4x47Z2vxs4ZwRpx3c9uHI7BNyr3SRfvF4QUlbW1lJW1vZ5F46\nluz6yJnpiykX5d1HuRTpd8vASM76mde9gd2l6TBiDa9ojTWNimlsS+c2P77A2ZG5bSix9EWZt7b2\nvYvRG5OQ3WH+ENOguXjc5qYhk0/s6XkRyw++7Y3h9w/5fYyPj5g5IT4yo7o35Sjz/yBJJiG7w7SQ\n02D9rOvfQ6T3MpsU797WUn/YZEJnRmaf6FPrrZQFZ4BJyEkzCdkdW/rexUjGvrIjN7UFR3dbO05k\n+Ya3bm2uO2yUng+r6KzInHZTukiZfaFQaKADQ4wuTEJ2xyZAvQ4il22Y+dl2RKSv/cQ31mqNNY1L\nrCN3Gh8fMXOiKV2kypteB5ALTEJ2QVV9XQvwttdx5Kr3Rs97KVI4ZF5/9rX844YAdK0jd/qwKV2k\nygteB5ALTEJ2zzqvA8hFClo//bJ+jyaz/GPKAbqrI4Ndujg7PLfDlC5ct8brAHKBScjuMQk5BbZN\nOP35mK+oqr/7izW0HGjZ0lzXY7/jcTp8hilduM4kZBeYhOwek5BdFhcr+taUCwYxcZN/e1usqTym\n0Yae9nBKF6bu6Y5GoN7rIHKBScjuedXrAHLN20ec/5xa/skDPU6s0r0A+8O7euyO6JQuwijRZGI0\nAHgxFAqZm9ouMAnZJVX1dW8B73odR66IWYWtWyeecfRgjhXfqDD0XEfuNE6Hz5gUH/XMYK5hHMKU\nK1xiErK7VnkdQK6oP+riNYjV2+TAPbJ85X6Arb3UkTudEak+yafWG4O5jnHQv7wOIFeYhOyulV4H\nkAsi/pJ9742df8xgjxd/+TCAvurIAD6swnPCx0RN6SIpJiG7xCRkd63wOoBcsGHG1etIYq0zyzf6\n4I3A3urIncp1WNVkU7oYrO2hUMiU6lxiErKLqurrNgG7vI4jm7UFRuzcN3x6n0OkeyNWcBjOOmp9\n1ZE7LTKli8EyrWMXmYTsPlO2SML6mde/hUig7z37IEU7AbY2b5ran91N6WLQnvA6gFxiErL7lnkd\nQLZqKql4q7l0wklunEusoY0AbbHmsTGNbu7PMXbpYrQpXfRfFPiz10HkEpOQ3fc/YIblDsb6WZ/b\njUi/Sgz4AbZAAAAZr0lEQVR9Ed/og7+D/R3vb+/vcYsis0zpov+Wh0KhvV4HkUtMQnZZVX3dHkz3\ntwHbO3z6+vbgyKRqx4ksX/nBVcD7W0cGU7oYoEe9DiDXmIScGo95HUC22TDjmrib57P85QeXW97a\nUtevOnKnch1WVWlKF32JYv7OXWcScmo8hpkfud/eGTv/xWhByRw3zym+URNxfgcDqSN3WhSZdbJP\nrdfdjCnHrAyFQru9DiLXmIScAlX1dduBF72OIxso6GtHXTzE7fOK+AMgOzu/H0gdGcDCKjg3fEzc\nlC569IjXAeQik5BT57deB5ANtk788HNxX9Gg5qzokwTf63w4kDpyp7E6bLopXXQrhilXpIRJyKnz\nINDudRCZLC6+8NtHnD8pVecXa9jBNd62tNQdOZhzLIrMOtmv1mvuRZUTVoVCofe9DiIXmYScIlX1\ndfswrYhevTn148+r5ZuYqvNb/jEHH7fHmscMtI4MduninPA8UCKuBpfdTO+KFDEJObV+5XUAmSrq\nK2reXrFwRiqvYfnGHbL000DryJ3GatnRR8THPOtOVFmvBXjI6yBylUnIqfVPzOKn3ao/+tKXEGt0\nKq8h/rFjEr/f2lLnH+y5To/MNKUL26/NYJDUMQk5harq6xTTSj5MuGDI7vdHz+vXKtLJEGtYBRDu\n/H5rS/2A+iMnsntd5H3pIgbc4XUQucwk5NS7F/tjnuHYMOOajYi43tWtKxHLB76DZYrB1pE7jdGy\no6fkd+nikVAo1OB1ELnMJOQUq6qv24tpJR/UGhy1ff+waSem7YJW8SGDFwZbR+50WmTmKX618nVB\nz9u8DiDXmYScHv8FZoABwPqZn9uCSGG6rmdZI9oSv0+mjgxgYfnPDc+z8rB08VQoFHrZ6yBynUnI\naVBVX7cV+L3XcXjtwJBJb7SUjEtf6xgQ/9hD/saTqSN3GqNlR02Jj8230oVpHaeBScjp80PyfH6L\n9TOv349IWv/mLN+4oYnfO3XkpHu+nBaZcYpffXXJnidLrAuFQn/3Ooh8YBJymlTV120A/uJ1HF7Z\nM2LGuo7A8A+l+7qWf8y4rtv2dby3I+nzYvkXh+f50Q96ceSw270OIF+YhJxe3yBPJ6/fWHW1eHFd\nsYaMAZoSt21rqU+qjtxptA6dNjU+9jk3zpXBtmHKbWljEnIaVdXX1QH3eR1Huu0sP3FNtKC42rsI\nCg5pEW8d5LwW3VmY+6WLb4dCIXNDOk1MQk6/m8mjfsmKxF+f9ukRXsYgVukhI8vaYy2j3agjQ86X\nLtYAD3gdRD4xCTnNqurr3sHuBpcXGiaf/VzcV+hai3QwxDfqsC5qbtSRO+Vw6eLLoVAor29Ep5tJ\nyN74IfBen3tlubj4Oxomn3OE13FY/vKCrtvcqiN3ysHSxX+HQqHnvQ4i35iE7IGq+rpm4Otex5Fq\nbxz5ydVq+Sq8jsPylQ/rus3NOjLkXOmihTz4+8xEJiF7pKq+rhZY4XUcqRL1BQ7sGL/Awxt5HxDf\n6MPeFNpjLaNj8ehbbl5ntA6ddmS8PBdKFzeHQqGkhpink4g85/xbKSKXJGw/TkTu8i6ygTMJ2Vuf\nBzq8DiIVNk2//GVEPL2Z10msQBnIYQty7gu7V0fudGpkxoIC9W1y+7xp9Apwp1snE5EBL501UKp6\nkvOwErgkYfuLqnpjqq/vJpOQPVRVX1cP3Op1HG7rKCx7f/eoOWkfBNIrKdzZddPWlrrDasvJshDf\n4vC8QjQr32jjwPWhUKhffeWdFmm9iPxOROpE5FERKRaRBhH5gYi8DFwoIlNF5EkReUlEnhaR6c7x\n94vIPSLyooi8LiIfdbYHROQ3IrJeRNaKyOnO9pkiskZEXhGRdSIyzdneuVRXDbDAef7fROQ0EXnc\n2WeEiPzZOW61iMx2todE5D4RWSEib4vIjc72EhFZJiKvisgGEbnIvR9zz0xC9l4NsM7rINy0Yea1\n9YiUeB1HIrHKDnTdtq2lPiW9P0bp0COPjJdn4w2xn4VCoRcGeMzRwM9UtQo4ANzgbN+jqvNU9SHg\nF8CXVPVY4KvAzxKOrwTmA4uBe0QkAHwBUFWtBj4D1DrblwA/VtW5wHFA17LKUuBpVZ2rql3nbf4O\nsFZVZ2MP0ErszjcdOMuJ42YRKQDOBnaq6hxVnQU8OcCfy6DkVEJOx8cjt1XV10WAqyAnbgbREhy7\npXHolLROINQflm/0Ya2+VNSRO2Vh6eI14D8Gcdw2Ve2caOlB4BTn8cMAIlIKnAQ8IiKvYM8Pnjic\n/Q+qGlfVN7BX15nunONBAFWtB7YARwHPA98Qka8Dk1X1kJn8+nAKzkrwqrocGCkinfOcLFPVDlXd\nDbwPjAXWA2c6Lf0Fqto4gGsNWtYk5AF8PJrrfCRZJyKPichw5/gVzn5rnI9HC5ztPX08ukpEfppw\n/cdF5DTncbOI3Op8nFktImOd7Rc6H29eFZFV/X1tVfV1a7Hf3bPe+lnX78BuYWQU8ZcXdbc9FXVk\nsEsXHw0fW5QlpYs24FOhUKi5zz0P17Wfcuf3nYOfLGC/02rt/Krqx/GHX0j1v4HznXifEJFFg4i3\nO4m/oxjgV9XXgXnYifkWEflPl67Vq6xJyI7+fDx6APi689FkPfbIuE5+VZ0PfDlhe08fj3pTAqxW\n1TnAKuA6Z/t/Amc5288f4Gu7E3higMdklP1Dj6hvLR6bca1jAMtXPqq77amoI3caqUOmTouNy4bS\nxQ2hUGjDII+dJCKdv/NLgGcSn1TVA8BmEbkQQGxzEna5UEQsEZkKTMFuqT8NXOrsfxQwCXhNRKYA\nb6vqXdgTdc3uEksT0NNKNInnPA3Y7cTWLREZD7Sq6oPYU4+mfMkxyL6E3NfHozJgmKqudLbXAqcm\nHP8n59+XsGtX0PPHo96Egce7OdezwP0ich0woPKJs/7eVcA7Azkuk2yYeV0zIp5MItQX8Y2cgH3T\n6hCpqiN3WhCtWlCgvo2pvEaS7guFQvcncfxrwBdEpA4YDvy8m30uBa4VkVeBjcDHEp7bij1E+2/A\nElVtx64xWyKyHvv/9lWq2gF8GtjglD5mcfiw7nVAzPmE+m9dngsBx4rIOuz7Nlf28bqqgTXOtW4G\nbuljf1e4OlopDfr6eNSXzo8mMfp+7VEOfcNKbDVHVLXz2gfPpapLROR47BsUL4nIsaq6p5+xUVVf\nt6tuetXlwP+SZW+Wu0bNXhsuKjvO6zh6IuIPgLUD4of0Se6sI/ssf9IT13fHKV0EHitc04HQbdnE\nQ+uALyZ5jqiqXtZlW2XiN6q6GfsmWXf+oapLuuzfDlzddUdVrcFOpl23lzr/RoCuZYwVznN7gQu6\nOTbU5ftZzsMGIO1zQGfVf3r6/njUCOzrrA8DlwMr6V23H4+wfyFznY9TE7HvwPZKRKaq6r9U9T+B\nXcDEfr2qBFX1dU+RpndjN22afkXalmUaNAl0O1x9b/jdlNSRO43UIVOPyrzSxQHsuvFAbowZKZZt\nCbk/H4+uBG5zPprMBb7bxzl7+nj0LLAZ2ATcBfRnPbHbnJuDG4DngFf7cUx3QsCjgzw27baPX7A6\n5g/O9DqOvohvWLc3rba11KX8zeSUzCtdXBsKhd5I5gSq2pDQohzM8Vepatb8naeDfPDJO7OJSCXw\neDJ/ANmkbnpVMXbrPS03EwZLkdiKU+/YolbBFK9j6Uuk9Z+rYh1rT+26vcgq3n3B5C91e9PPTXuk\n6e3HCteMR+jrpnGq3RUKhW7yOAajG9nWQs4bVfV1rdg3P971OpbebK5c/Fw2JGMAy1/e7WCVjnjr\nqFT1R040UodMOTo2fnWqr9OH57EHZxgZKGsScrIfj7JRVX3dduyk3O51LN2JWQVtDZPP8nSu44EQ\n39gxPT2X6jpyp1Oi008tUN9gu5glaz3w0VAodNj80EZmyJqEnK+q6uvWYN90zLi1+F6f9ul/IdZh\ni4hmKrGGV9DDiMh01JEBBLHOCx9Xgqb9TfZ14MxQKLS3zz0Nz5iEnAWq6uv+BFxDL6OY0i3iDza+\nU37iXK/jGAgRscC3rbvntjbXH6lpuqEyQkuPODo2/l/puJajATgjFArl/KII2c4k5CxRVV/3AMn3\nGXXNpqor1yJy2MTvGc8q6bZfeEe8dVRcU19H7nRKdPqCQvWvT8OldgIfzqb5jfOZSchZpKq+7mdk\nwJwX7UXD3t0zYtbxXscxGJY1osd+t3vD7x42RWeqCGJ9NHxsaYpLF7uxk3Ha3miM5JiEnGWq6ut+\ngMdzKK+fed0biAS9jGGwxD+2xyHtW5vr0zq4ZYSWHjE9daWL/cBHQqFQLq3zl/NMQs5CVfV138Ke\n0zXtmovHbW4aMjkjJxDqD8s3bmhPz21rSV8dudPJqSldNAPnhkKhtS6f10gxk5CzVFV93fexJ+w+\nbMKcVFpf/bl3Ecm2OVAOsvxjeuwVku46MhxSunBrCHMjcJ5ZMTo7mYScxarq6+7FntMjLf1K95Ud\nuaktODprW8cAYpWOxp7HoVvprCN3sksXFWtcONXbwImhUGiFC+cyPGAScparqq97GHvu5dZUX2vD\nzM9m5ACVgSvocRBIuuvInU6OHr2gUP3JLOX1DHC8qRlnN5OQc0BVfd2T2PM+p6xr03uj570UKRyS\n0fNq9JdYQ/b19Ny2lvpp6a4jQ+eAkWOHDrJ0UYvdz/iwlbWN7GISco6oqq97CXvhx+fcPreC1k+/\nrNjt83pFfKN6LPF0xFtHxjT6Zjrj6TRcSyurYhMGUrpQ4BuhUOiqUCiUE2sy5juTkHNIVX3de8Dp\nwK/dPO+2Cac/H/MVVfW9Z3aw/OW9Ltu0L/yuZ6u2nBQ9qr+li1bs+Yy/n+qYjPQxCTnHVNXXhavq\n6z4LfAl71ZOkxMWKvDXlgoq+98welq98eG/Pb21Oz7wW3XHmuihDe70nsANYEAqF/tTLPkYWMgk5\nR1XV1/0UWIB9533Q3j7i/NVq+Se7E1VmEP/oXt9gtrW85kkdudNwLZk8IzbhhR6eXgXMD4VC/Vkw\nwcgyJiHnsKr6utXYq6Z0XQyyX2JWYevWiWf0teBr1hEpGgqyq6fnvawjdzoxetSpRepPXHEmDHwd\nOD0UCqW9a56RHlnbwd/on6r6uibgyrrpVX/DXvKq3xMC1R918RrEOi1FoXlLit5B20f39PS+8Lvv\njA5MnJbOkBIJIueFjxv2aOHqVoS3gMtCoVAy3eKMLGBayHmiqr7uIWAOsLw/+0f8JfveGzv/mNRG\n5R2xhvY4OAS8rSN3GqYlE+ZFj/gu8CGTjPND1qypZ7inbnrV5cCPgB5biGtnf3HlvhFVC9MXVXpF\nWv53ZSy8ocfXV2gF914w6UvDRUTSGVeCdcC1E2oWvOjR9Q0PmBZyHqqqr/stMB27e9xh78htgRE7\n9w2ffkLaA0sj8Zf3OltdON42wqM6cjvwn8BxJhnnH5OQ81RVfd1ep3vcqcAhy9Ovn3n924gUeRNZ\neli+8pF97bO345109kdW7JuvR02oWfC9CTULzLp3ecgk5DxXVV/3DHZPjM8D7zWVVLzVXDohqycQ\n6g/xjZxIHzPlbW2pS9eb0j+AeRNqFlw5oWZBt0tMGfnBJGSDqvq6aFV93T3AkW9Mu/BuRFq8jinV\nRHyFYPW60nQa+iOvB86ZULPgzAk1C15J4XWMLGESsnFQVX1d86UPfe4O4AjgB6RhBjlPSfD93p5O\nYR15O3AtMHdCzYInU3B+I0uZXhZGj+5esnws9sKqn6OXHhnZqqPp4VUa3XFqb/ucXv6ZVWOCk3rd\nZwA2ArcDvzM1YqM7JiEbfbp7yfIA9kT4NwGzPQ7HNZHWFatiHS/3mmynDpn7r+NGnZXsgq7/BP4L\nWDahZoH5D2f0yCRkY0DuXrL8dODLwEfJ8pJXLPzay5GWZb3O8ZxEf+RW4LfATyfULNgw6CCNvGIS\nsjEody9ZPhW4CrgYONLbaAYnHtu/I3zgvj5nsvvk5K+87rcK+jOnRxx78p+HgIcn1CzYn2yMRn4x\nCdlI2t1Llh8LfAb4NDDR43D6TVXjHfvvCAOB3vbrRx15NXYS/sOEmgWezaVsZD+TkA3X3L1kuQAn\nY7eaPwH0uMJzpmjf9+M3IdZrC7+HOvIrwMPAQxNqFjSkKj4jv5iEbKTM3UuWVwGLnK/TgBGeBtSN\njsZf/UvjB3q9aVdoBfZdMOnGNhH5J/Ygjn9MqFmQsvUL+0tExgN3qeqnXDznMOASVf1Zqq5h9Mwk\nZCMtnNbzHD5I0PPJgK504abHVsajm7tOMqTAa8AL2Ks5r/p/Dz9en/bgPCAilcDjqjrL41DykknI\nhmfuXrJ8PPaw7bnADKAKOBooSVcM0fY1f4u2PVMK1AMbgLXAK//v4cebUn1tEbkC+Cr2G8A64NvA\nfcAoYBdwtapuFZH7gQPYi9iWA/+uqo8mJk8RuQo4TlW/6Jz7ceB2VV0hIs3Aj7F7xrQBH1PV90Rk\nLHAPMMUJ6fPAjcDHsN+Q/g+4O+EaAew5tY/DXh7sK6r6T+fa5wPFwFTgMVX9dxHxYU9gdZzzGu9T\n1Ttc/jHmFDNBveGZL9yzaCewE3iic5vTkq4AxmMnn3JgbJfHI7D/dn09fIWBfQlfe7s83oa9tNXb\nN92/tDnFL7NbIjIT+BZwkqruFpERQC1Qq6q1InINcBdwgXPIOOAU7Fn6/go8OoDLlQCrVfWbIvJD\n4DrgFuf8K1X1407yLAWWArNUda4TZ2XCeb4AqKpWi8h04H9FpLP3yVzgGKADeE1EfgKMASo6W9tO\nOcTohUnIRkb5wj2LFHtosec12hRbBDyiqrsBVHWviJyIfTMU7D7MP0zY/8+qGgc2OS3bgQgDjzuP\nXwLOTIjhCuf6MaBRRHpbAPYU4CfO/vUisgXoTMhPqWojgIhsAiZjj0yc4iTnZcD/DjDuvJPVHfsN\nI490JDzubpBKlEP/Pyd25YskTJIUIzUNscT4YoBfVfdh3zdYASwBfpWC6+YUk5ANwxvLgQtFZCSA\nU7J4DrvLIMClwNMDOF8DMFdELBGZiH3TtC9PYdeNERGfiJQBTcCQHvZ/2okLp1QxCbvW3C0RGQVY\nqvpH7PJMr6MiDVOyMIykiYjP+cjfb6q6UURuBVaKSAz7ZuKXgN+IyNewb+p9tj+ncv59FtgMbALq\ngJf7cexNwC9E5FrsVu3nVfV5EXlWRDYAf8O+qdfpZ8DPRWQ9dov8KlXt6GVUeYXzejobfv/Rj5jy\nmullYQDQ9S59kucKAc2qenuy5/Kac1PrSeza6zzsuugV2InvYex67A+xe2ncg93T4C3gGlXdJyIr\ngFeBhdgNoGtUdY2IlGDXY2cBBUBIVf/i/B4+gX2Dzaeqh637JyKnASHspHgS8BhwmaqqiJyBPaOc\nH7vb3uedpNmA/fvdLSLHYffAOM35XU3C7mkxCbhTVe9y4vsDMAH7Run3VPXhZH+eRu9MycJwlYjk\n4qeuo4GfqWoVdvezG5zte1R1nqo+hL380tdVdTb2xPM3Jxxf7PRauAG7WxvAN4HlqjofOB24zUmC\nYCf+T3WXjBMch51EL3f+PdnplnY/cJGqVmMn5c/34/VNB87CLnPcLCIFwNnATlWd4/SSMPM2p4FJ\nyHlCRP4sIi+JyEYRud7ZdrWIvC4ia7CHPCMiZSKypfNjpoiUiMg2ESkQkaki8qRznqedrk+IyP0i\nco+I/IsPegbMEZHnReQNEbnO2W+ciKwSkVdEZIOILEj7D2Jwtqnqs87jB7F7G4DdQsapvQ5T1ZXO\n9lrstQo7/R5AVVcBQ53uXx8BlorIK9g3vQLYLVSA/1PVvX3E9LyqTnHqs68AldhvHJtV9fUe4ujJ\nMlXtcHp8vI/dtXA9cKaI/EBEFnT2oDBSKxdbM0b3rnG6VgWBF0RkGfAd4FigEXvO3rWq2ugkiYXO\nto8Cf1fViIj8Aliiqm+IyPHYNcVFzvknYPepjTkfg2cDJ2D3gV3rXO8zzrludfq9FqfptSera12v\n8/v+LnXV3fECfFJVD7kp5vxc+3Pew3o19LF/Yi+MrpMpdddD4nURmQecC9wiIk+p6nf7EZeRBNNC\nzh83isir2DOTTcT+qLtCVXepahinted4GLjIeXwx8LCIlGLXKx9xEva9HDp50CNdbmz9RVXbnFbX\nP7E/Dr8AXO0k7GpVTfloOJdMcvoIgz1R/zOJTzqtx30JLf7LgZUJu1wEICKnAI3O/n8HvtQ5z7KI\nHONCnK8BlSLSOVlSYhwN2G++AJ/s60TOHBatqvogcBumh0RamIScB5ybQB8GTlTVOdh39Hubm+Gv\nwNlOV6xjsbtoWcB+VZ2b8FWVcEzXVt1hrULnI/upwA7gfmfocDZ4DfiCiNQBw7GHD3d1JXYdeB32\nqLXE1mS7iKzFvul3rbPte9g389aJyEbn+6SoajtwNfab5nrs+ZnvcZ7+DvBjEXkRuxXcl2pgjfPm\nezP2yD4jxUwvizwgIh8DPquq5zl131ew/+PWYLd8DmAn3VcT5kJ4BGgHmlT1Bmfbc8AdqvqI07Kb\nraqvij3XwuOq+qizXwh7yO/BkoXzuADY7pQ1vggcqapfTssPYZAkycl2nF4WX1XVF10My8hRpoac\nH54EljgtvNewyxbvYHedeh7Yj52kEz0MPII9bWanS7H7oX4LO7k+hN2lqzvrsEsVo7C7TO0UkSuB\nr4lIBGjGGbZrGIbNtJANI0OJSDX2nBaJOlQ12UVXjQxlErJhGEaGMDf1DMMwMoRJyIZhGBnCJGTD\nMIwMYRKyYRhGhjAJ2TAMI0OYhGwYhpEhTEI2DMPIEP8/QfEN1A4YTHQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14b2d278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_plotter(parts_of_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we've worked through one example, we'll build a more formalized method of exploring other texts in [Part 2](https://jss367.github.io/Light-verbs-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
