---
published: false
layout: post
title:  "特征工程（Feature Engineering）"
date:   2014-11-15 18:30
categories: machine_learning feature_engineering feature_learning
---



特征工程经常被说为机器学习中的**black art**，这里面包含了很多不可言说的方面，最主要的当然还是对要解决问题的了解。但是，它其实也有很多科学的地方，这就是这篇文章的主题。

实际的问题中，单个原始特征通常属于以下几类之一：

* 连续（continuous）变量；
* 无序类别（categorical）变量；
* 有序类别（ordinal）变量。

下面我由浅入深地逐渐说明针对这三类特征的常用处理方法。

#入门级

这节要讲的处理技术，应该刚接触机器学习不久的同学都会知道。


##连续特征
除了归一化（去中心，方差归一），不用做太多特殊处理，可以直接把连续特征扔到模型里使用。


##无序特征
可以使用**1-hot-of-K**的方法把每个无序特征转化为一个数值向量。比如一个无序特征`color`有三种取值：`red`，`green`，`blue`。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于`red`，`green`，`blue`。如：

color取值     | 向量表示
-------- | -----
red | (1, 0, 0)
green| (0, 1, 0)
blue | (0, 0, 1)

这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。

机器学习书籍里在讲这个的时候介绍的处理方法可能跟我上面说的有点差别。上面说的表达方式里有一个维度是可以省略的。既然我们知道`color`一定是取3个值中的一个，那么我们知道向量的前两个元素值，就能推断第3个值是多少。所以，其实用下面的方式就可以表达到底是哪种颜色：

color取值     | 向量表示
-------- | -----
red | (1, 0)
green| (0, 1)
blue | (0, 0)

这样表达的好处是少用了一个维度，降低了转化后特征之间的相关性。但在实际问题中特征基本都或多或少会有些缺失。使用第一种表达方式就可以用全0的向量来表示值缺失，而第二种表达方式是没法表达缺失的。


##有序特征
有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态有三种取值：`bad`, `normal`, `good`，显然`bad` < `normal` < `good`。

当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。

当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下：

status取值     | 向量表示
-------- | -----
bad | (1, 0, 0)
normal | (1, 1, 0)
good | (1, 1, 1)

上面这种表达方式很巧妙地利用递进表达了值之间的顺序关系。



#中级

最容易让人掉以轻心的，往往就是大家觉得最简单的事。在特征处理中，最容易让刚入门同学忽略的，是对连续特征的处理方式。

以线性分类器Linear Regression (LinearReg)为例，它是通过特征的线性加权来预测因变量$$y$$：

$$
y = w^T x
$$

但大部分实际情况下，$$y$$与$$x$$都不会是这么简单的线性关系，甚至连单调关系都不会有。举个只有一个特征的例子，如果$$y$$与$$x$$的实际关系如下图：

![nonlinear function][img_ori]

那么直接把$$x$$扔进LinearReg模型是怎么也得不到好结果的。很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是**Logistic Regression (LogisticReg)**。

对于上面这个问题，有没有什么办法使得LinearReg也能处理得不错？当然是有。

##方法一：离散化
就是对原始特征$$x$$做转化，把原来的非线性关系转化为线性关系。最常用的转化方式是对$$x$$做**离散化(discretization)**，也就是把原来的值分段，转化成一个取值为0或1的向量。原始值落在某个段里，向量中此段对应的元素就为1，其他元素为0。比如取离散点$$\{0.5, 1.5, 2.5\}$$，通过判断$$x$$属于$$(-\infty, 0.5)$$，$$[0.5, 1.5)$$，$$[1.5, 2.5)$$，$$[2.5, +\infty)$$中哪段来把它离散化为4维的向量。下面是离散结果：

原始值$$x$$ | 离散化后的值
-------- | -----
0.1 | (1, 0, 0, 0)
1.3 | (0, 1, 0, 0)
3.2 | (0, 0, 0, 1)
5.8 | (0, 0, 0, 1)

离散化方法的关键是怎么确定分段中的离散点。下面是常用的选取离散点的方法：

*  **等距离散**：顾名思义，就是离散点选取等距点。我们上面对$$x$$取离散点$$\{0.5, 1.5, 2.5\}$$就是一种等距离散，见下图。

![nonlinear function][img_equal_dist]

*  **等样本点离散**：选取的离散点保证落在每段里的样本点数量大致相同，见下图。

![nonlinear function][img_equal_size]

*  **画图观察趋势**：以$$x$$为横坐标，$$y$$为纵坐标，画图，看曲线的趋势和拐点。通过观察下面的图我们发现可以利用3条直线（红色直线）来逐段近似原来的曲线。

![nonlinear function][img_watch]


##方法二：函数变换
his is [eml][] reference-style link. *要反白的文字*[^esl]



[img_ori]: /images/nonlinear_function1.png "样本点"
[img_equal_dist]: /images/nonlinear_function2.png "等距离散离散法"
[img_equal_size]: /images/nonlinear_function3.png "等样本点离散离散法"
[img_watch]: /images/nonlinear_function4.png "画图观察趋势离散法"


[^esl]: Trevor Hastie et al, [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/).


#归一化：
  * 有些模型/优化
  * 方法的效果会强烈地依赖于特征是否归一化，如LogisticReg，SVM，NeuralNetwork，SGD等。
  * 有些模型则不受归一化影响，如DecisionTree。
  * 0/1取值的特征通常不需要归一化，归一化会破坏它的稀疏性


#离散化：
 4. 单独用此特征和目标值来训练一个决策树，依据决策树的各节点的划分方法来离散化此特征
  * 特征离散化时不一定要保留所有的取值段对应的特征，可以通过特征选择或者领域知识去掉其中的一部分（经常是头部或尾部）
  * facebook用gbdt抽取特征
  * feature hashing：
    * 可以添加新的原始特征而保持hashing后的特征长度不变
    * 可以保持原始特征的稀疏性，既然hashing时只考虑非0原始特征
    * 可以只hashing其中的一部分原始特征，而保留另一部分原始特征（如那些出现collision就会很影响精度的重要特征）
    * 很适合有个性化的应用，因为此时要加入用户id的话会导致原始特征数量变为 (用户数+1)*特征数
    * The result is increased speed and reduced memory usage, at the expense of inspectability;
    * Paper: Feature Hashing for Large Scale Multitask Learning


#特征抽取：
  * 尽可能人为删除特征里的非线性性，不要期待模型自己能很好地自动处理非线性性
  * 间接特征：通过其他模型抽取的特征
    * 如可以利用LDA的聚类结果作为特征
    * 用户画像
    * Predicting Positive and Negative Links in Online Social Networks
    * 如果有时间概念，一般临近时间内的用户行为会较像，可以考虑把前几天的用户行为作为特征
