---
published: false
layout: post
title:  "特征工程（Feature Engineering）"
date:   2014-11-15 18:30
categories: machine_learning feature_engineering
---

特征工程（Feature Engineering）
=======

特征工程经常被说为机器学习中的**black art**，这里面包含了很多不可言说的方面，最主要的当然还是对要解决问题的了解。但是，它其实也有很多科学的地方，这就是这篇文章的主题。

实际的问题中，单个原始特征通常属于以下几类之一：

* 连续（continuous）变量；
* 无序类别（categorical）变量；
* 有序类别（ordinal）变量。

下面我由浅入深地逐渐说明针对这三类特征的常用处理方法。

#入门级

这节要讲的处理技术，应该刚接触机器学习不久的同学都会知道。


##连续特征
除了归一化（去中心，方差归一），不用做太多特殊处理，可以直接把连续特征扔到模型里使用。


##无序特征
可以使用**1-hot-of-K**的方法把每个无序特征转化为一个数值向量。比如一个无序特征`color`有三种取值：`red`，`green`，`blue`。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于`red`，`green`，`blue`。如：

color取值     | 向量表示
-------- | -----
red | (1, 0, 0)
green| (0, 1, 0)
blue | (0, 0, 1)

这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。

机器学习书籍里在讲这个的时候介绍的处理方法可能跟我上面说的有点差别。上面说的表达方式里有一个维度是可以省略的。既然我们知道`color`一定是取3个值中的一个，那么我们知道向量的前两个元素值，就能推断第3个值是多少。所以，其实用下面的方式就可以表达到底是哪种颜色：

color取值     | 向量表示
-------- | -----
red | (1, 0)
green| (0, 1)
blue | (0, 0)

这样表达的好处是少用了一个维度，降低了转化后特征之间的相关性。但在实际问题中特征基本都有可能会有缺失。使用第一种表达方式就可以用全0的向量来表示值缺失，而第二种表达方式是没法表达缺失的。


##有序特征
有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态有三种取值：`bad`, `normal`, `good`，显然`bad` < `normal` < `good`。

当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。

当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下：

status取值     | 向量表示
-------- | -----
bad | (1, 0, 0)
normal | (1, 1, 0)
good | (1, 1, 1)

上面这种表达方式很巧妙地表达了值之间的顺序关系。


#中级

最容易让人忽略的，其实是连续特征的处理方式。


#归一化：
  * 有些模型/优化方法的效果会强烈地依赖于特征是否归一化，如LogisticReg，SVM，NeuralNetwork，SGD等。
  * 有些模型则不受归一化影响，如DecisionTree。
  * 0/1取值的特征通常不需要归一化，归一化会破坏它的稀疏性


#离散化：
  1. 等距离散
  2. 等样本点离散
  3. 以此特征为横坐标，目标值为纵坐标，画图，看趋势和拐点
  4. 单独用此特征和目标值来训练一个决策树，依据决策树的各节点的划分方法来离散化此特征
  * 特征离散化时不一定要保留所有的取值段对应的特征，可以通过特征选择或者领域知识去掉其中的一部分（经常是头部或尾部）
  * facebook用gbdt抽取特征
  * feature hashing：
    * 可以添加新的原始特征而保持hashing后的特征长度不变
    * 可以保持原始特征的稀疏性，既然hashing时只考虑非0原始特征
    * 可以只hashing其中的一部分原始特征，而保留另一部分原始特征（如那些出现collision就会很影响精度的重要特征）
    * 很适合有个性化的应用，因为此时要加入用户id的话会导致原始特征数量变为 (用户数+1)*特征数
    * The result is increased speed and reduced memory usage, at the expense of inspectability;
    * Paper: Feature Hashing for Large Scale Multitask Learning


#特征抽取：
  * 尽可能人为删除特征里的非线性性，不要期待模型自己能很好地自动处理非线性性
  * 间接特征：通过其他模型抽取的特征
    * 如可以利用LDA的聚类结果作为特征
    * 用户画像
    * Predicting Positive and Negative Links in Online Social Networks
    * 如果有时间概念，一般临近时间内的用户行为会较像，可以考虑把前几天的用户行为作为特征
