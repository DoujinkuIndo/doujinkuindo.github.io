---
layout: post
title: News Similarity
---

News Similarity between NewYork Times and Washington Post News

We want to find the similar news between NewYork Times and Washington Post News. The challenge is how to compare texts in the short context.

###Data Collection

I collecte the news from the first pages on the New York Times and the Washington Post News. I used the _requests_ and _Beautifulsoup_ for web scraping.

{% highlight python%}
def create_news_bot(url):
	if url=='http://www.nytimes.com/':
		r = requests.get(url)
		nyt_soup = BeautifulSoup(r.content, "lxml")
		all_news = nyt_soup.findAll('a',{'href': re.compile('com/[0-9]{4}/[0-9]{2}/[0-9]{2}/')})
		nyt_news_bot = []
		for news in all_news :
			if len(news.attrs)==1 and news.text.strip()!='' :  #make sure each title has a corresponding link
				 nyt_news_bot.append({'title':news.text,'link':news.attrs['href']}) 
		return(nyt_news_bot)
	elif url=='https://www.washingtonpost.com/':
		r = requests.get(url)
		wap_soup = BeautifulSoup(r.content)
		all_news = wap_soup.find_all('a',{'data-pb-field':'web_headline'})
		wap_news_bot = []
		for news in all_news :
			if news.text.strip()!='' :  #make sure each title has a corresponding link
				 wap_news_bot.append({'title':news.text,'link':news.attrs['href']}) 
		return(wap_news_bot)
{% endhighlight %}

###Data Cleaning & Munging

Since we aim to represent titles as vectors to improve our accuracy for clustering or text comparision, we will need to do the following tasks with the regular expression and the NLTK toolkit:

* lower case
* solve the encoding issue
* filter the usefuless stopwords
* apply the lemmatizer

Note that we apply lemmatizer which is more accuracy than stemmer, since lemmatizer is based on WordNetâ€™s built-in morphy function in the NLTK.

{% highlight python%}
def text_cleaner(news_bot):
	wnl = nltk.WordNetLemmatizer()
	for news in news_bot:
		news['title'] = re.sub(u'\u2019|\u2019s|\u2018|\u2018s|\u2014','',news['title'])  #solve the encoding issue
		news['words_vec'] = [w.lower() for w in nltk.tokenize.word_tokenize(news['title'])] #lower case
		news['words_vec'] = [word for word in news['words_vec'] if word not in stop_words] #filter stop words
		news['words_vec'] = [wnl.lemmatize(word) for word in news['words_vec']] 
	return(news_bot)
{% endhighlight %}

###Data Analysis
We calculate TF-IDF values to create numeric vectors from the words vectors. Then based on the TF-IDF vector for each news, we can calculate the cosin similarity for any two news.

The TF-IDF approach is known for accuracy of information retrieval by taking the frequecy of terms per document from the entire corpus into account, which in some ways reduces the negative effects of the binary representation from the bag-of-words model.

For illustration, we compare the popular news about powerball in two news bots we created. We hope to see that each news about powerball in new york times is also similar to the news about powerball in the washington post news bot.

{% highlight python%}
query_terms = ['powerball','jackpot']
nyt_corpus = [news['words_vec'] for news in nyt_news_bot]
nyt_tc = nltk.TextCollection(nyt_corpus)
relavant_nyt_news = []
for idx in range(len(nyt_news_bot)):
    score = 0
    for term in [t.lower() for t in query_terms]:
        score += nyt_tc.tf_idf(term,nyt_corpus[idx])
    if score > 0:
        relavant_nyt_news.append({'score':score,'title':nyt_news_bot[idx]['title'].strip(),'link':nyt_news_bot[idx]['link']})
relavant_nyt_news = sorted(relavant_nyt_news, key = lambda n: n['score'], reverse=True)
relavant_nyt_news
{% endhighlight %}

![_config.yml]({{ site.baseurl }}/images/powerballnews.png)

The following function helps search similar news provided the query terms.
{% highlight python%}
def search_similar_news_byKeywords(nyt_news_bot, wap_news_bot, query_terms):
        similar_texts = []
        #We can use TF-IDF to narrow down the candidate lists for our news bots with the keys words since TF-IDF represents the relavance of a term in the corpus
        nyt_corpus = [news['words_vec'] for news in nyt_news_bot]
        nyt_tc = nltk.TextCollection(nyt_corpus)
        relavant_nyt_news = []
        for idx in range(len(nyt_news_bot)):
            score = 0
            for term in [t.lower() for t in query_terms]:
                score += nyt_tc.tf_idf(term,nyt_corpus[idx])
            if score > 0:
                relavant_nyt_news.append({'score':score,'title':nyt_news_bot[idx]['title'].strip(),'link':nyt_news_bot[idx]['link'],'words_vec':nyt_news_bot[idx]['words_vec']})    
        relavant_nyt_news = sorted(relavant_nyt_news, key = lambda n: n['score'], reverse=True)

        wap_corpus = [news['words_vec'] for news in wap_news_bot]
        wap_tc = nltk.TextCollection(wap_corpus)
        relavant_wap_news = []
        for idx in range(len(wap_news_bot)):
            score = 0
            for term in [t.lower() for t in query_terms]:
                score += wap_tc.tf_idf(term,wap_corpus[idx])
            if score > 0:
                relavant_wap_news.append({'score':score,'title':wap_news_bot[idx]['title'].strip(),'link':wap_news_bot[idx]['link'],'words_vec':wap_news_bot[idx]['words_vec']})    
        relavant_wap_news = sorted(relavant_wap_news, key = lambda n: n['score'], reverse=True)
        
        for i in range(len(relavant_nyt_news)):
            news_nyt = relavant_nyt_news[i]
            dummy_ls = []
            dummy_ls.append(news_nyt['words_vec'])
            combined_relavant_news_corpus = dummy_ls + [news['words_vec'] for news in relavant_wap_news]  #more efficient and save space instead of using the entir corpus
            combined_relavant_news_tc = nltk.TextCollection(combined_relavant_news_corpus)
            dummy_ls2 = []
            dummy_ls2.append(news_nyt)
            combined_relavant_news = dummy_ls2 + relavant_wap_news
            #print(combined_relavant_news_corpus)
            fdist = nltk.FreqDist([w for ws in combined_relavant_news_corpus for w in ws])
            
            td_matrix = {}
            # note that combined_relavant_news_corpus[0] refers to the news_nyt            
            for idx in range(len(combined_relavant_news)):
                doc_title = combined_relavant_news[idx]['title']
                url = combined_relavant_news[idx]['link']
                td_matrix[(doc_title,url)]={}
                for term in fdist.iterkeys():#create the tf-idf score for each term in each docs
                    td_matrix[(doc_title,url)][term]= combined_relavant_news_tc.tf_idf(term,combined_relavant_news_corpus[idx])
            
            distances = {}
            (title1,url1) = (news_nyt['title'],news_nyt['link'])
            distances[(title1,url1)]={}
            min_dist = 1.0
            similar_texts.append({'nyt':(title1.strip(),url1), 'most_similar':('','')})       

            for (title2, url2) in td_matrix.keys():
                # We should not mutate the original data structures
                # since we're in a loop and need the originals multiple times

                terms1 = td_matrix[(title1, url1)].copy()
                terms2 = td_matrix[(title2, url2)].copy()
                # Create vectors from term maps
                v1 = [score for (term, score) in sorted(terms1.items())]
                v2 = [score for (term, score) in sorted(terms2.items())]

                # Compute similarity amongst documents; I use cosine distance
                distances[(title1, url1)][(title2, url2)] = nltk.cluster.util.cosine_distance(v1, v2)

                if url1 == url2:
                    #print distances[(title1, url1)][(title2, url2)]
                    continue

                if distances[(title1, url1)][(title2, url2)] < min_dist:
                    min_dist = distances[(title1, url1)][(title2,url2)]
                    similar_texts[i]['most_similar'] = (title2, url2)
                    similar_texts[i]['cosine_dist'] = distances[(title1, url1)][(title2, url2)]
        return([sim_news for sim_news in similar_texts if sim_news['most_similar']!=('','') and sim_news['cosine_dist']>0.9])
{% endhighlight %}

###Visualization


