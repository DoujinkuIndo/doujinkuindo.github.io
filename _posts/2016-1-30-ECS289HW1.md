---
layout: post
title: News Similarity
---

News Similarity between NewYork Times and Washington Post News

We want to find the similar news between NewYork Times and Washington Post News. The challenge is how to compare texts in the short contest.

###Data Collection

I collecte the news from the first pages on the New York Times and the Washington Post News. I used the _requests_ and _Beautifulsoup_ for web scraping.

{% highlight python%}
def create_news_bot(url):
	if url=='http://www.nytimes.com/':
		r = requests.get(url)
		nyt_soup = BeautifulSoup(r.content, "lxml")
		all_news = nyt_soup.findAll('a',{'href': re.compile('com/[0-9]{4}/[0-9]{2}/[0-9]{2}/')})
		nyt_news_bot = []
		for news in all_news :
			if len(news.attrs)==1 and news.text.strip()!='' :  #make sure each title has a corresponding link
				 nyt_news_bot.append({'title':news.text,'link':news.attrs['href']}) 
		return(nyt_news_bot)
	elif url=='https://www.washingtonpost.com/':
		r = requests.get(url)
		wap_soup = BeautifulSoup(r.content)
		all_news = wap_soup.find_all('a',{'data-pb-field':'web_headline'})
		wap_news_bot = []
		for news in all_news :
			if news.text.strip()!='' :  #make sure each title has a corresponding link
				 wap_news_bot.append({'title':news.text,'link':news.attrs['href']}) 
		return(wap_news_bot)
{% endhighlight %}

###Data Cleaning & Munging

Since we aim to represent titles as vectors to improve our accuracy for clustering or text comparision, we will need to do the following tasks with the regular expression and the NLTK toolkit:

* lower case
* solve the encoding issue
* filter the usefuless stopwords
* apply the lemmatizer

Note that we apply lemmatizer which is more accuracy than stemmer, since lemmatizer is based on WordNetâ€™s built-in morphy function in the NLTK.

{% highlight python%}
def text_cleaner(news_bot):
	wnl = nltk.WordNetLemmatizer()
	for news in news_bot:
		news['title'] = re.sub(u'\u2019|\u2019s|\u2018|\u2018s|\u2014','',news['title'])  #solve the encoding issue
		news['words_vec'] = [w.lower() for w in nltk.tokenize.word_tokenize(news['title'])] #lower case
		news['words_vec'] = [word for word in news['words_vec'] if word not in stop_words] #filter stop words
		news['words_vec'] = [wnl.lemmatize(word) for word in news['words_vec']] 
	return(news_bot)
{% endhighlight %}

###Data Analysis


###Visualization


