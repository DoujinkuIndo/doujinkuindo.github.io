---
title: 03-4. Linear Regression
date: 2022-04-21
category: MIN
layout: post
---

> 선형회귀 (Simple linear regression)

- 데이터를 놓고 그걸 가장 잘 설명할수 있도록 찾는 분석방법을 선형회귀라 부르며,  
  선형회귀분석은 독립변수 x에 대응하는 종속변수 y와 가장 비슷한 값 y^를 출력하는 함수 f(x)를 찾는 과정이다.
- 공식 : y = ax + b


> 다중회귀 (Multiple linear regression)

- 변수(feature)가 2개 이상인 데이터의 결과를 예측하는 것  
- 가설 함수는 다음과 같은 형태이며, n은 (feature의 개수+1)이다. +1은 bias인 첫 번째 파라미터를 포함  
- 다중 회귀 분석시 독립변수간 상관관계가 높아 발생하는 다중공선성(multicollinearity) 문제 처리가 필요  
- 다중공선성 확인은 분산팽창지수 (Variation Inflation Factor, VIF)로 확인 가능  
- 공식 :  
![mr1]({{ site.baseurl }}/images/mr1.png)


> 다항회귀 (Polynamial regression)

- 다항회귀분석은 독립변수의 차수를 높이는 형태
- 단순 선형 모델의 한계를 어느정도 극복가능함.
![pr1]({{ site.baseurl }}/images/pr1.png)



- __Linear regression__ : [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn.linear_model.LinearRegression)



클래스 구성도
-------------
```python
class sklearn.linear_model.LinearRegression(*, fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False)[source]
```

Parameters (API 그대로)
-------------
```
fit_intercept : bool, default=True
이 모델에 대한 절편을 계산할지 여부. False로 설정하면 계산에 절편이 사용되지 않음.
(즉, 데이터가 중앙에 위치할 것으로 예상됨).

normalize : bool, default=False (1.2에서 제거)
fit_interceptFalse로 설정된 경우 무시됩니다. True인 경우 회귀자 X는 평균을 빼고 l2-norm으로 나누어 회귀 전에 정규화

copy_X : bool, default=True
True이면 X가 복사

n_jobs : int, default=None
매개변수를 1 이상으로 지정하여 시스템의 CPU 코어를 최대한 활용할 수 있음.
기본값은 한 개의 코어만을 사용하며 -1로 지정하면 가용한 모든 코어를 사용함.

positive : bool, default=False
True로 설정하면 계수가 양수로 강제 설정. 이 옵션은 밀집 배열에만 지원함.
```

Attributes (API 그대로)
-------------
```
coef_ : array of shape (n_features, ) or (n_targets, n_features)
선형 회귀 문제에 대한 추정된 계수.(기울기)
맞추는 동안 여러 대상이 전달되면(y 2D) 모양의 2D 배열(n_targets, n_features)이고,
하나의 대상만 전달되면 길이가 n_features인 1D 배열입.

rank_ : int
행렬 X의 순위입니다. X가 조밀한 경우에만 사용가능.

singular_ : array of shape (min(X, y),)
X의 특이값. X가 조밀한 경우에만 사용간ㅇ.

intercept_ : float or array of shape (n_targets,)
선형 모델의 독립 항입니다. (절편)
fit_intercept = False인 경우 0.0으로 설정.

n_features_ : in_int (New in version 0.24.))
맞춤 중에 표시되는 기능의 수.


feature_names_in_ : ndarray of shape (n_features_in_,)
맞춤 중에 표시되는 기능의 이름. X에 모두 문자열인 기능 이름이 있는 경우에만 정의
```

선형회귀 예제 (참조 - 혼공머신)
-------------
```python
# KNeighborsRegressor 의 문제점을 풀고자 이번에는 선형회귀로 값을 예측해보도록 한다 
# KNeighborsRegressor 에서는 근처에 찾을수 있는 이웃이 없게 되자. 값을 잘못 예측하는 문제점이 발견되었다.

import numpy as np
import pandas as pd

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt 

perch_length = np.array(
    [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 
     21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 
     22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 
     27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 
     36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 
     40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
     )
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 
     1000.0, 1000.0]
     )

train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state = 42)

train_input = train_input.reshape(-1,1)
train_target = train_target.reshape(-1,1)
test_input = test_input.reshape(-1,1)
test_target = test_target.reshape(-1,1)

lr = LinearRegression()
lr.fit(train_input, train_target)

# 선형회귀 h = wx + b
# w : 기울기(Gradient) b : 절편(intercept) bias라고 불리기도함
# w는 최소제곱법으로 구할수 있다

# 기울기 확인
print(lr.coef_)
# 절편
print(lr.intercept_)

arr = np.empty((0,3), int)
arr = np.append(arr, np.array([[1, 2, 3]]), axis=0)
arr = np.append(arr, np.array([[4, 5, 0]]), axis=1)
print(arr)

#임의의 x 값 예측
x = lr.predict([[60,]])

plt.scatter(train_input, train_target)
plt.plot(train_input, lr.predict(train_input))
# x 의 예측치
plt.plot(60, x, color='red', marker='^')
# x 까지 그어보기
a = np.max(train_input)
b = np.max(lr.predict(train_input))
c = np.array([60])
print(type(c))
print(a.shape, b.shape, c.shape)
plt.plot([a,c],[b,x])
plt.show()
```  


다중회귀 예제
-------------
```python

#구글 드라이브에 install 해두기
# import os, sys
# from google.colab import drive
# drive.mount('/content/drive')

# my_path = '/content/drive/MyDrive/MIN_AI/additional_package'

# !pip install --target=$my_path plotly

# 패키지 이용시
import sys
sys.path.append('/content/drive/MyDrive/MIN_AI/additional_package')

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression 

# 맨해튼 집값 데이터 불러오기

url = "https://raw.githubusercontent.com/Codecademy/datasets/master/streeteasy/manhattan.csv"

df = pd.read_csv(url)
df.head()

#rent 레이블 기준으로 다른 특성과의 상관관계 확인해보기
corr_df = df.corr()['rent'].sort_values(ascending=False)
print(corr_df)

#새로운 그래프 초기화 
plt.figure(figsize=(12,8))
plt.bar(corr_df.index, corr_df[corr_df.index])
#x 축 라벨 기울기
plt.xticks(rotation=45)
plt.show()

from pandas.plotting import scatter_matrix

# 산점도 행렬
attributes = ['bedrooms','bathrooms','size_sqft','no_fee','floor','building_age_yrs','rent']
# scatter_matrix(df[attributes], c = df['rent'], figsize=(12, 8))

data = df[['bedrooms','bathrooms','size_sqft','no_fee','floor','building_age_yrs']]
target = df[['rent']]

train_input, test_input, train_target, test_target = train_test_split(data, target, random_state=42)

mlr = LinearRegression()
mlr.fit(train_input, train_target)
mlr.score(test_input,test_target)

#예측값과 실제값 비교해 보기
plt.figure(figsize=(12,6))
plt.plot(mlr.predict(test_input[:100]))
plt.plot(test_target[:100].values)
plt.show()

# mlr.predict(test_input[:100])

# 기울기
mlr.coef_
# bias
mlr.intercept_


type(mlr.coef_)
mlr.coef_
```