---
layout: post
title: Practices for real world data science
comments: True
share: True
---

[Last updated: December 16, 2021]

As I write this introduction, I have now been working as a data scientist for [electricityMap](https://www.electricitymap.org/) for about 8 months. Overall, I am responsible, together with the other data scientist in the team, of delivering high data quality at the end of the entire data processing pipeline. Briefly said, electricityMap's data is generated by first aggregating varied data points from numerous public data sources about electricity; and then validating and standardising them before eventually running them through our flow-tracing algorithm for the generation of world-wide real-time hourly electricity consumption figures.

<div id="html" markdown="0" style="display: flex; flex-direction: column; align-items: center;">
    <img src="../../resources/posts/2021-12-16/emap_infra.png" style="width: 100%; overflow: hidden; margin: 16px 0;">
    <span style="color: #666; font-size: 13px; font-style: italic;">A simplified overview of the electricityMap infrastructure. Thanks Felix!</span>
</div>



This data is at the core of our mission; to organise the world's electricity data to drive the transition towards a truly decarbonised electricity system. This global ambition implies firstly that must be able to overcome at any moment data sources becoming erroneous or unavailable and secondly that we must come up with clever ways to generate truthful data for regions of the world where the aforementioned public electrical data sources are unavailable. This reveal my other current responsibilities; develop and maintain a wide range of models to capture the dynamics of electrical production per factor and exchanges in vastly different areas of the world, and have enough domain expertise to ensure that they behave according to what is physically possible.

<div id="html" markdown="0" style="display: flex; flex-direction: column; align-items: center;">
    <img src="../../resources/posts/2021-12-16/emap_team.JPG" style="width: 75%; overflow: hidden; margin: 16px 0;">
    <span style="color: #666; font-size: 13px; font-style: italic;">The electricityMap team. (PS: yes, thank you Nick!)</span>
</div>

These responsabilities are far reaching and evolve rapidly as electricityMap scales up. Impossible to be highly specialised when only 11 (10 bright + me) people are fighting for something that is way too big for them only. The good news is that as electricityMap grows up, I am constantly able to redefine my role as a data scientist, and what practices I should adopt to be succesful.

Recently, we opened up a [position](https://electricitymap.org/jobs/index.html) to find someone brilliant that can become the most knowledgeable about our data quality, and I started delivering on tasks whose scope overflowed in the realm of data engineering. The former event, because it will most likely reduce the scope of my responsabilities, pushed me to redefine what I, as a data scientist, should focus my efforts on, and the later revealed to me the necessity of defining and implementing good practices for successfully delivering on that newly defined scope.

Hence this blog post, which aims at capturing my current thoughts and practices as a data scientist for electricityMap. For that reason, it might get updated occasionally.

Sections:

1. [The data scientist role](#the-data-scientist-role)
2. [Practices](#practices)
  - Define your scope
  - Invest in tooling (Create your toolbox with domain level classes, notebooks suck, environment for doing modelisation with representative data)
  - Test first, then debug (snapshots, integration tests)


## The data scientist role

Data scientist are often presented as _part mathematician, part computer scientist and part trend-spotter_$$^1$$ by corporates that don't really understand that finding someone that can be a stellar mathematician, computer scientist and analyst at once is a harder than finding a unicorn. I would tend to think that a company that looks for all three competencies in a single individual has an immature data infrastructure, and by lack of experience, hopes that a single individual can set it up. This thought pattern has probably been harmful to many recent graduates who have been developing and researching state of the art machine learning and optimisation models during their studies to end up having to spend most of their time researching data sources, and frying their brains over complex software architecturing problems.

To me, the previous description actually corresponds to three different jobs; the expertise in computer science should be delegated to a data engineer, the expertise in analysis and trend-spotting should be delegated to a data analyst, and mathematics and modelling should be the realm of data scientists. This means that in most cases, recruiting a data scientist should be preceded by hiring a data engineer, who will be much better suited for building up the infrastructure that will support the forthcoming modelling efforts. With libraries like [sklearn](https://scikit-learn.org/stable/) or [PyTorch](https://pytorch.org/), any talented data engineer would already be able to build up a decent pipeline for predictions or estimations without a deep understanding of the models themselves.

Once the infrastructure already built up, a data scientist can generate tremendous value, by capturing marginal gains from guiding the creation of automated Q&A systems, ensuring consistency between data distributions, and researching new models.
## Practices

Thanks for reading!

<div style="margin-top: 16px;">
  <span style="color: #666; font-size: 13px; font-style: italic;">1: source, <a href="https://www.sas.com/en_us/insights/analytics/what-is-a-data-scientist.html">SAS</a></span>
</div>