---
title: 03-2. K-NN
author: Min
date: 2022-04-20
category: MIN
layout: post
---

K-최근접 이웃(K-NN, K-Nearest Neighbor) 알고리즘은 가장 간단한 머신러닝 알고리즘으로, 분류(Classification) 에 속한다. 
- 주변의 가장 가까운 K개의 데이터를 보고 데이터가 속할 그룹을 판단하는 알고리즘이 K-NN 알고리즘이다

- __KNeighborsRegressor__ : [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)



클래스 구성도
-------------
```python
class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
```

Parameters
-------------
```
- n_neighbors : int
이웃의 수인 K를 결정한다. default는 5다. 

- weights : {'uniform', 'distance'} or callable
예측에 사용되는 가중 방법을 결정한다. default는 uniform이다. 
'uniform' : 각각의 이웃이 모두 동일한 가중치를 갖는다. 
'distance' : 거리가 가까울수록 더 높은 가중치를 가져 더 큰 영향을 미치게 된다.
callable : 사용자가 직접 정의한 함수를 사용할 수도 있다. 거리가 저장된 배열을 입력으로 받고 가중치가 저장된 배열을 반환하는 함수가 되어야 한다. 

- algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'} 
가장 가까운 이웃들을 계산하는 데 사용하는 알고리즘을 결정한다. default는 auto이다. 
'auto' : 입력된 훈련 데이터에 기반하여 가장 적절한 알고리즘을 사용한다. 
'ball_tree' : Ball-Tree 구조를 사용한다. (Ball-Tree 설명 : https://nobilitycat.tistory.com/entry/ball-tree)
'kd_tree' : KD-Tree 구조를 사용한다.
'brute' : Brute-Force 탐색을 사용한다. 

- leaf_size : int
Ball-Tree나 KD-Tree의 leaf size를 결정한다. default값은 30이다.
이는 트리를 저장하기 위한 메모리뿐만 아니라, 트리의 구성과 쿼리 처리의 속도에도 영향을 미친다. 

- p : int
민코프스키 미터법(Minkowski)의 차수를 결정한다. 예를 들어 p = 1이면 맨해튼 거리(Manhatten distance), p = 2이면 유클리드 거리(Euclidean distance)이다. 
```


예제
-------------
```python
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# np.column_stack 을 이용한 2차원 배열로 만들기
fish_data = np.column_stack((fish_length, fish_weight))
# target 임의로 만들기
fish_target = np.concatenate((np.ones(35),np.zeros(14)))

# matplotlib 시각화로 분포확인
plt.scatter(fish_data[:,0], fish_data[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('fish_length')
plt.ylabel('fish_weight')
plt.show()
```

![knn1]({{ site.baseurl }}/images/knn1.png)

```python
# test_size 기본 0.25
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42)

# 위의 이미지대로 하게 되면 이웃을 잘못 잡게 된다. 
kn = KNeighborsClassifier()
kn.fit(train_input, train_input)
kn.score(test_input, test_target)

#현재 상태에서 25, 150 좌표 근처 값을 본다.
#kneighbors 함수 사용시 좌표에서 길이와 index 를 돌려준다
neigh_dist, neigh_ind = kn.kneighbors([[25, 150]])
print(distiances, indexes)
# [[ 92.00086956 130.48375378 130.73859415 138.32150953 138.39320793]] [[21 33 19 30  1]]


# matplotlib 시각화로 분포확인
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
#kneighbors에서 얻은 25, 150의 가장 가까운값을 찾아 표기해본다
plt.scatter(train_input[neigh_ind,0], train_input[neigh_ind,1], marker='D')
plt.xlabel('fish_length')
plt.ylabel('fish_weight')
plt.show()
```

![knn2]({{ site.baseurl }}/images/knn2.png)

위의 그림과 같이 값이 좋지 않게 나온다. 
이유는 x의 값과 y의 값이 단위가 다르므로 값으로만 계산시에는 이런 현상이 발생한다.
이리하여 이럴때는 표준점수를 구하여 처리해본다.

```python
# x의 값을 y와 동일하게 해서 보면 이해를 도울수 있다. plt.xlim(0,1000)
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25,150,marker='^')
plt.scatter(train_input[neigh_ind,0], train_input[neigh_ind,1], marker='D')
plt.xlim(0,1000)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

![knn3]({{ site.baseurl }}/images/knn3.png)

표준점수 구하기 (standard score) :   
분산 : 데이터에서 평균을 뺀 값을 모두 제곱한 다음 평균을 내어 구함.   
표준편차 : 분산의 제곱근으로 데이터가 분산된 정도를 말함.  
표준점수 : 각 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지를 나타냄  

```python
#정규화 표준점수 구하기
mean = np.mean(train_input, axis=0) #평균
std = np.std(train_input, axis=0)   #표준편차
train_scaled = (train_input-mean)/std

# 또다른 방법으로 sklearn 의 정규화(StandardScaler)를 사용할수도 있다.
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
# test_input 또한 score 측정시 정규화를 진행한다.
test_scaled = ss.transform(test_input)

## 다시 좌표를 중심으로 값을 찍어보면 값들이 모여있는 것을 확인할수 있다.
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(25,100, marker='^')

# 좌표값을 새로 정규화 진행
new = ([25,150]/mean)/std

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1],marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

정규화된 값으로 분포를 확인한다. 
x,y 값들이 비슷해진것을 볼수 있다.

![knn4]({{ site.baseurl }}/images/knn4.png)


```python
#정규화된 값으로 학습 진행
kn = kn.fit(train_scaled, train_target)
kn.score(test_scaled, test_target)

#정규환된 25,150 은 어디에 속하는가? (예측)
kn.predict([new])


# 정규화된 25,150 값의 근처값을 재검색 해보자.
neigh_dist, neigh_ind = kn.kneighbors([new])

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[neigh_ind,0], train_scaled[neigh_ind,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![knn5]({{ site.baseurl }}/images/knn5.png)