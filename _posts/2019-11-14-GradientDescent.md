---
layout: post
title: Gradient Descent and Global Minimum
---

The next concept is important when we do back propagation calculations in neural networks.  

![_config.yml]({{ site.baseurl }}/images/gd4-01-entfn.png)

Our aim is to minimize this error function to its least possible value , to adjust the coefficients ( β_0,β_1,β_2...)  or in other words the weights (w_0,w_1,w_2... ) of our logisitic unit because Minimizing error → Maximizing efficiency 

![_config.yml]({{ site.baseurl }}/images/config.png)

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.
