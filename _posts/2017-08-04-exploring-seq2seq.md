---
published: false 
layout: post
title:  "Chatbot 8: 探索对话中的seq2seq模型"
date:   2017-08-04 15:29
tags: seq2seq Bot Chatbot 聊天机器人 爱因互动
---

**Seq2seq+Attention**的模型，不仅是机器翻译的baseline模型，现在也成了开域聊天生成式的baseline模型。网络上有很多文章说的都是把某些对话数据灌到这个模型，然后结果就是深度学习生成式聊天模型了。本文就是来泼冷水的，谈谈我们的经验，看看是不是所谓的深度学习生成式模型真的有说的那么神奇，已经完胜传统的检索模型……

我们使用的基本模型如下图所示：

{:.center}
![Seq2seq+Attention模型][seq2seq]



## 各种坑

#### **生成结果容易产生重复的字/词/词组**
Seq2seq+Attention模型很容易生成连续重复的字、词或词组，比如字重复“*哈哈哈哈哈*”，词组重复“*我的妈呀，我的妈呀*”。（当然会有这种现象一个原因是语料中确实大量出现这种单字重复情况。）比如：
{% highlight text %}
Q: 我不想学习肿么办
A1: 坚持加王
A2: 好好学习
A3: 加油复习
A4: 加油干哈哈哈哈哈
A8: 加油干哈哈哈哈哈哈哈
{% endhighlight %}

{% highlight text %}
Q: 感冒了,难受死了
A1: 我养你
A2: 快去医院 聊天哦
A3: 快去医院阿
A4: 快去怀里上去去
A5: 快去医院 聊天啊
A6: 快去医院 聊天吧
A7: 快去医院零零零零零零零零零零
{% endhighlight %}

其中`Q`表示问题，A表示使用beam search产生的topN答案，`A`后面的数字表示序号，比如`A2`表示排名第二的结果。

这个问题比较好解决，使用下面的启发式方法通常就能搞定：

1. 清洗训练数据，去掉重复多次的字/词/词组。
2. 在生成答案时，降低已出现unigram/bigram/trigram再次出现的概率，超过设定次数后强制不再出现（把概率设为0）。

#### **生成结果语句不够通顺**
直接beam search生成的结果，偶尔会出现结构很诡异的，比如下例中第五个答案：
{% highlight text %}
Q: 手机没电了咋办
A1: 没电馨的
A2: 好好吃
A3: 没假的陪你玩
A4: 没假的可以的
A5: 没移间
A6: 没事，有我陪你
{% endhighlight %}

解决方案：

1. 保证产生的候选集必须来自于指定候选集。这个候选集可以是训练集里的答案库，也可以是优化后满足某些条件的候选库。这时生成模型其实就变成检索模型了。
2. 上面方法约束整句必须在候选集里，在某些应用场景可能太强。一种更松点的约束方法是只约束其ngram。只要给定的字与前面的前缀字所构成的ngram在候选ngram集合里，这个字在当前步就是合法的候选字。这样原来句子级别的约束就降为ngram级别的，从而又变回了“生成式”模型。通常使用trigram即可，除非找不到对应的trigram（因为有些频次太少的trigram直接被过滤调了），这时候再backoff到bigram。


#### **改进结果的多样性**
Beam search给出的topN结果，通常都长得很像，只差一两个字，缺乏多样性，比如下面的例子：

{% highlight text %}
Q: 谢谢你爱我
A1: 我也爱你！！！！
A2: 我也爱你，主人~
A3: 我也爱你！！！
A4: 我也爱你
A5: 我爱你！！
A6: 我爱你！！！
A7: 我也爱你！！
{% endhighlight %}

解决方法：

1. Jiwei Li的算法
2. 直接把重合度较高的答案往后移
3. bs里使用抽样+Gaussian noise：不是取最大的beam_size个字，而是使用抽样的方法。抽样对应的概率值，除了通过模型计算得到的值，还可以加一个所谓的高斯噪音。
4. 限定bs里最大值的选取时来自单分支的数量：直接限制从每个beam分支所获得的topn数量。



#### 正例
{% highlight text %}
Q: 我不想学习肿么办
A1: 加油加油
A2: 好好学习
A3: 考什么
A4: 没事，以后我养你
{% endhighlight %}

{% highlight text %}
Q: 感冒了,难受死了
A1: 快去医院
A2: 怀里
A3: 我吃你
A4: 睡觉吧
A5: 多喝点热水
A6: 没事，有你的
A7: 快去医院  不要怕
A8: 快去医院  不要紧，赶紧去道歉
A12: 没事，有我陪你的
{% endhighlight %}


{% highlight text %}
Q: 谢谢你爱我
A1: 我也爱你
A2: 讨厌
A3: 大宝贝
A4: 爱你的！！
A5: 主人我爱你
A6: 亲爱的主人
{% endhighlight %}

{% highlight text %}
Q: 是先有鸡还是先有蛋
A1: 好难的问题
A2: 先有卵
A3: 可以吃你
A4: 无聊的人
A5: 可能是女的
A6: 去问问上帝就知道了
A7: 根据相关法律法规，呵呵
{% endhighlight %}

{% highlight text %}
Q: 手机没电了咋办
A1: 好好吃
A2: 怎么行
A3: 那就快去睡吧
A4: 没事，有我陪你
A5: 好好复习吧
A6: 好好吃不起
A7: 那你想怎么办
{% endhighlight %}

#### 检索
{% highlight text %}
Q: 我不想学习肿么办
A1: 坚持啊，学习要紧！
A2: 那就再睡会吧，乖乖
A3: 加油看书哦，么么哒
A4: 好，那我以后不说了
{% endhighlight %}

{% highlight text %}
Q: 感冒了,难受死了
A1: 吃饱点，给我打个包
A2: 多喝点热水，盖好被子
A3: 没事，休息一会就好了
A4: 好心疼，快去医院看看吧
A5: 没有啊，我只喜欢你
A6: 亲爱的主人，我是猪
A7: 亲爱的，你失忆了吗？
A9: 睡觉吧，睡醒就好了呢……
{% endhighlight %}

{% highlight text %}
Q: 谢谢你爱我
A1: 我也爱你
A2: 我也爱你主人
A3: 讨厌！
A4: 么么哒
A5: 主人你好
A6: 我爱你就够了
A7: 亲爱的，晚安
A8: 宝贝儿我永远爱你
{% endhighlight %}

{% highlight text %}
Q: 是先有鸡还是先有蛋
A1: 去问问上帝就知道了
A2: 根据相关法律法规，呵呵
A3: 我是主人的聊天机器人
A4: 我是你男人，纯爷们
{% endhighlight %}

{% highlight text %}
Q: 手机没电了咋办
A1: 没事，休息一会就好了
A2: 那就不要睡了，陪我聊天吧
A3: 没有我不知道的事情！
A4: 怎么会呢，她爱你爱你
A7: 那就躺着好好照顾被子吧！
{% endhighlight %}

#### 结果对比

| Seq2seq+Attention | ngram | retrieval |
|:-----|:------ |:------|
|-
| Q: 我不想学习肿么办 |
| A1: 坚持加王 <br/> A2: 好好学习 <br/> A3: 加油复习 <br/> A4: 加油干哈哈哈哈哈 <br/> A8: 加油干哈哈哈哈哈哈哈 | A1: 加油加油  <br/> A2: 好好学习  <br/> A3: 考什么  <br/> A4: 没事，以后我养你 | A1: 坚持啊，学习要紧！ <br/> A2: 那就再睡会吧，乖乖  <br/> A3: 加油看书哦，么么哒  <br/> A4: 好，那我以后不说了 |
| Q: 感冒了,难受死了 |
| A1: 我养你  <br/> A2: 快去医院 聊天哦  <br/> A3: 快去医院阿  <br/> A4: 快去怀里上去去  <br/> A5: 快去医院 聊天啊  <br/> A6: 快去医院 聊天吧  <br/> A7: 快去医院零零零零零零零零零零 | A1: 快去医院 <br/> A2: 怀里  <br/> A3: 我吃你  <br/> A4: 睡觉吧  <br/> A5: 多喝点热水  <br/> A6: 没事，有你的  <br/> A7: 快去医院  不要怕  <br/> A8: 快去医院  不要紧，赶紧去道歉  <br/> A12: 没事，有我陪你的 | A1: 吃饱点，给我打个包  <br/> A2: 多喝点热水，盖好被子  <br/> A3: 没事，休息一会就好了  <br/> A4: 好心疼，快去医院看看吧  <br/> A5: 没有啊，我只喜欢你  <br/> A6: 亲爱的主人，我是猪  <br/> A7: 亲爱的，你失忆了吗？ <br/> A9: 睡觉吧，睡醒就好了呢……
{: .table .table-striped .table-hover}


#### **使用dual encoder重排序**


#### **不同的优化算法**

1. sgd：除了收敛稍慢，其他都还好，而且通常最终效果会比收敛更快的算法好。
2. rmsprop：在xms数据上能加速，但最后的perplexity比sgd高；在微博数据上perplexity很高（百量级）时就降不下去了。
3. adam：和rmsprop的表现非常类似

## TODO: 检索模型 vs. 生成式模型
给出上面例子在检索模式下的结果。

* 检索模型的优势：
	* 可以保证用户加入的qa对
	* 语句通顺

## 未来计划

* Copy机制处理稀少关键词，比如姚明
* 


---------

最后做个广告，爱因互动在招[NLP和机器学习算法工程师](https://www.einplus.cn/join-us)，欢迎感兴趣的同学自荐~。


[seq2seq]: /images/seq2seq.png
