_This is my third attempt at writing this blog post. My first two attempts were thwarted by my internet browser crashing on my laptop. I usually write these posts on the Macs in the computational physics lab, but attempted to use my laptop yesterday and this morning because I had to write the posts from home due to some car repairs I've had to make recently. I'm not sure why my laptop crashed twice while writing this post, but I'm not taking any more chances and have moved back into the computational lab. I took pictures while my laptop was crashing so that I didn't lose what I wrote last night, and have rewritten it (with edits) below:_

Today was not as productive as I had hoped it would be. It ended up just being hours of tweaking small parameters until my graphs looked the way I wanted them to. Honestly, this waste of time is my own fault; I should be better at realizing when I am getting diminishing returns. It seemed every time I looked back at the clock, hours had gone by just deciding what multiple of standard deviation or FWHM size to use, trying and failing to make the x-axis of a Flux vs. Source ID graph make sense, or trying to add text to that graph that tells which sources do not correspond to a real star. It felt to me like the `plt.text()` command was not functioning correctly. I didn't get an error message when running the code, but no text ever showed up on the graph, either. Is it possible for a command to be corrupt and not function properly? 

I did get a few results, even though I felt like today was mostly spent wasting time on distractions. I downloaded the HST images that contain our source of interest, X1, as _flc.fits files, a total of seventeen files. Unlike drizzled data, these files measure total counts and are not composites of smaller images. I can use the same [aperture_photometry.py](https://github.com/GosnellResearchGroupSummer2018/NGC6819/blob/master/photometry%20codes/aperture_photometry.py) code I've been using, except this time I don't have to worry about an effective gain. Here's the source detection I got today:

**[phot_table](https://github.com/GosnellResearchGroupSummer2018/NGC6819/blob/master/first_attempt_at_HST_source_detection/day_six/phot_table.txt)**    
![sources]({{ thom-ory.github.io }}/images/day-fourteen/sources.png "sources")
![bright_sources]({{ thom-ory.github.io }}/images/day-fourteen/bright_sources.png "bright_sources")   
_top: source detection aimed at detecting the most sources  
bottom: source detection aimed at only detecting the brightest sources_

---
I've forgotten what parameters I used for these images at this point; look at tomorrow's post for results with accurate RA and Dec information, as well as information about what FWHM and multiple of standard deviation I used. I do remember that something unique happened when I ran the code with _flc.fits files instead of _drz.fits; instead of having to use somewhere in the range of 100-150 times the standard deviation as the threshold for being a source, I had to use a much more reasonable 3.4-3.8 times the standard deviation to get the best source detection. This is because drizzling greatly changes the characteristics of an image. 

# Conclusion
I am finally successfully writing this post when it is already late the next day, so I know that tomorrow, from the perspective of this post, won't see me able to get into the computational lab until about 5:45pm (after a full day tending to my car) where I'll spend the first few minutes writing the post you just read. I then plan on reading Rory's posts from the past two days because I just saw him in the lab and we are working on very similar things right now. I'll use his posts to help me direct my learning of DOLPHOT, our newest photometry tool. Tomorrow's post will talk about what I learn. 
