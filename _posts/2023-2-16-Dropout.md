---
layout: post
title: Still Nothing
published: true
---


Dropout has proven to be one of the most [successful ] regularization techniques to reduce overfitting, the focus of this blog won't be on Implementing dropout, as it's a straightfarward implementation using common frameworks like Tensorflow, but we will instead focus on getting deeper intuition of what dropout is actually doing to the network and why it works at all with such a random nature. By the end of this blog [].

## Overview
The blog is divided into the following sections:
* Introduction 
* How it works?
* Intuitions
    * How come such a random environment helps generalizing the model and reduce overfitting? 
    * How does the model perform very well after eliminating randomness at prediction time 
    * Why keep_prob value is typically set to 50% or higher 
* Intuitions behind the best practices
* Conclusion

</br>
remove ahmed.txt file

## Introduction
Large networks are prone to overfitting, espicially when having small datasets. In such cases, regularization is often essential, one of the most used regularization techniques to reduce overfitting is dropout, in dropout random units from the network are dropped during training, this helps to prevent strong co-adaptation that may occur between units in training, and thus reduce overfitting. 
| ![Dropout_general_image](../images/images/Dropout/dropout_general.png) |
|:--:| 
| *Figure-1 from*  *[paper](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)* |

## How it works

One of the known regularization methods as well is ensemble learning, in such method you get to train different networks on different datasets, and eventually average their predicitons, obviosly this technique requires [high computation resourecs as as many networks are presented] [ and introduce slow training process and maybe requires large datasets]. Dropout addresses these two problems, it's similar to ensemble learning in terms of having many networks, but the process is totally different.

In dropout, random units are dropped in each training step, results with a different smaller network (thinned network) than the previous iteration, the forward and backward [ propagatoins ] are [ done ] only on that specific thinned network during that specific iteration, as a result, when accumelating the training steps of all thinned networks [something odd is here=>] it's as if we are getting the average of predictions of different networks but with less computations and time. (when you start to talk about predicitons you should have mentioned how we got to the prediction stage when we were talking about averaging the [training steps ])


think of it this way, each thinned network updates the weights [sligtly ] and pass them to the next thinned network [etc ], this means a thinned network starts where the previous thinned network stopped, and weights continues updating in that manner [on that base?].

| ![Dropout_general_image](../images/images/Dropout/athlete.jpg) |
|:--:| 
| *[Figure-2](https://sportsmatik.com/uploads/matik-sports-corner/matik-know-how/relay_1564644996.jpg)* |

So thinned networks work in series rather than in parallel!, they all share the weights and participate in updating them sequencially. In dropout each layer is presented with a retention probability p, for instance, if a layer has a p value of 0.7, then roughly 30% (0.3) of units in that layer will be dropped randomly along with their incoming and outgoing connections. At test time no units are dropped and the whole network is utilized to make predictions. therefore When dropout is eliminated, units receives more connections from previous layer than what they used to during training, and thus become overexcited. To overcome the effect of overexcitation all weights in a layer are multiplied by the retention probability associated with that layer. We eventually end up with a unthinned network that has smaller weights and approximately averages the predictions of all these thinned networks.

| ![Dropout_general_image](../images/images/Dropout/units.jpg) |
|:--:| 
| *Figure-3 from* *[paper](https://sportsmatik.com/uploads/matik-sports-corner/matik-know-how/relay_1564644996.jpg)* |


## Intuitions

//THIS PART REMAINS
How dropout performs so well in regularizing networks with such a volatile and random nature?, and how could it achieves well in predicitons when randomness is suddenly eliminated at inference time?


### Then why retention probability is recommended to be set to value closer to 0.5 or higher not lower?

Imagine setting the value of p to be very small (p=0.01) and for the sake of this example assume the network to have only one hidden layer of 100 units, as a result, the produced thinned networks of this original network will almost be of 1 unit, which is 100 times smaller than the original network. This will destory the network and completely wipe out the co-adaptation that could exist between units as each unit is trained individually, in other words, causes complete randomness. The goal then is not to completely wipe out co-adaptation, but to (reduce) it, by causing some randomness to the network that lets generalization takes place and hopefully reduce overfitting. While in the same network above, when using p value of 0.5 or higher, the thinned networks will be at most 2 times smaller if not less, which allows units to work together and train in environment that is similar to that used at inference time. 

## Conclusion


