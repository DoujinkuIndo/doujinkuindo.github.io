---
layout: post
title: Still Nothing
published: true
---


Dropout has proven to be one of the most successful regularization techniques to reduce overfitting, in this blog, the focus won't be on Implementing dropout, as it's a straightforward implementation using common frameworks like Tensorflow, but we will instead focus on getting deeper intuition of what dropout is actually doing to the network and why it works at all with such a random nature.

## Overview
The blog is divided into the following sections:
* Introduction 
* How it works?
* Intuitions

</br>
remove ahmed.txt file

## Introduction
Large networks are prone to overfitting, especially when dealing with small datasets. In such models, regularization is often essential, and one of the most used regularization techniques to reduce overfitting is dropout. In dropout random units from the network are dropped during training, this helps to prevent strong co-adaptation that may occur between units, and thus reduce overfitting.
| ![Dropout_general_image](../images/images/Dropout/dropout_general.png) |
|:--:| 
| *Figure-1 from*  *[paper](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)* |

## How it works

One of the known regularization methods as well is ensemble learning, in such method you get to train different networks on different datasets, and eventually average their predicitons. Obviosly this technique requires a lot of computation resourecs in order to train many networks and [introduce slow training process and maybe requires large datasets]. Dropout addresses these two problems, it's similar to ensemble learning in terms of having many networks, but the process is totally different.

In dropout, random units are dropped in each training step, results with a different smaller network (thinned network) than the previous iteration, the forward and backward [ propagatoins ] are [ done ] only on that specific thinned network during that specific iteration, as a result, when accumelating the training steps of all thinned networks [something odd is here=>] it's as if we are getting the average of predictions of different networks but with less computations and time. (when you start to talk about predicitons you should have mentioned how we got to the prediction stage when we were talking about averaging the [training steps ])


think of it this way, each thinned network updates the weights [sligtly ] and pass them to the next thinned network [etc ], this means a thinned network starts where the previous thinned network stopped, and weights continues updating in that manner [on that base?].

| ![Dropout_general_image](../images/images/Dropout/athlete.jpg) |
|:--:| 
| *[Figure-2](https://sportsmatik.com/uploads/matik-sports-corner/matik-know-how/relay_1564644996.jpg)* |

So thinned networks work in series rather than in parallel!, they share the weights and participate in updating them sequencially. In dropout each layer is presented with a retention probability p, for instance, if a layer has a p value of 0.7, then roughly 30% (0.3) of units in that layer will be dropped randomly along with their incoming and outgoing connections. At test time no units are dropped and the whole network is utilized to make predictions. therefore When dropout is eliminated, units receives more connections from previous layer than what they used to during training, and thus become overexcited. To overcome the effect of overexcitation all weights in a layer are multiplied by the retention probability associated with that layer. We eventually end up with a unthinned network that has smaller weights and approximately averages the predictions of all these thinned networks.

| ![Dropout_general_image](../images/images/Dropout/units.jpg) |
|:--:| 
| *Figure-3 from* *[paper](https://sportsmatik.com/uploads/matik-sports-corner/matik-know-how/relay_1564644996.jpg)* |


## Intuitions

//THIS PART REMAINS
In this section we should get better intuition of 
How dropout performs so well in regularizing networks with such a volatile and random nature?, and how could it achieves well in predicitons when randomness is suddenly eliminated at inference time?

what is the impact that retention probabilty has on  network and how it reduces co-adaptation?, 


### The effect of retention probability and why it's  recommended to be set closer to 0.5 or higher not lower?.

Imagine setting the value of p to be very small (p=0.01) for all hidden layers, and for the sake of this example assume the network to have only one hidden layer of 100 units, as a result, the produced thinned networks will almost be of 1 unit, which are 100 times smaller than the original network. This will destory the network and completely wipe out the co-adaptation that could exist between units as each unit is trained individually, in other words, causes complete randomness and the network won't propagate any any useful information at test time. The goal then is not to completely wipe out co-adaptation, but to (reduce) it, by causing some randomness to the network that lets generalization takes place and hopefully reduce overfitting. In the same network above, when using p value of 0.5 or higher, the thinned networks will be at most 2 times smaller if not less, which allows units to work together (adapt to some extent and capture useful patterns) and train in environment that is similar to that used at inference time.  

This as well explains why the network works so well at inference time when randomness is suddenly eliminated, as when using p values larger than 0.5 we are not causing high degree of randomness, that's in addition to multiplying weights by p after training to prevent overexcitation. All these play role in making the network works well at test time. 

### how this [reflects to reduce overfitting]? 
As disscussed above, in dropout each unit is trained in many different networks, which in return reduces co-adaptation between units, this forces units to be as useful as possible on their own.
As in the follwoing image:

| ![Dropout_general_image](../images/images/Dropout/overfitting.jpg) |
|:--:| 
| *Figure-4* |


the next-layer unit can't fully rely on just one of the inputs, it has to pay attention to all of them, which means to spread the weights (this has similar effect of shrinking weights as in L2 regularization method). Units then become less sensitive to slight changes in the inputs and at the end this makes the network generalizes much better.




