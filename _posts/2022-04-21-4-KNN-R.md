---
title: 03-3. K-NN Regression
author: Min
date: 2022-04-20
category: MIN
layout: post
---

KNeighborsRegressor 알고리즘은 가장 간단한 머신러닝 알고리즘으로, Regression(회귀)에 속한다.
- 주변의 가장 가까운 K개의 데이터를 보고 데이터가 속할 그룹을 판단하는 알고리즘이 K-NN 알고리즘이다

- __KNeighborsRegressor__ : [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)



클래스 구성도
-------------
```python
class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
```

Parameters
-------------
```
- n_neighbors : int
이웃의 수인 K를 결정한다. default는 5다. 

- weights : {'uniform', 'distance'} or callable
예측에 사용되는 가중 방법을 결정한다. default는 uniform이다. 
'uniform' : 각각의 이웃이 모두 동일한 가중치를 갖는다. 
'distance' : 거리가 가까울수록 더 높은 가중치를 가져 더 큰 영향을 미치게 된다.
callable : 사용자가 직접 정의한 함수를 사용할 수도 있다. 거리가 저장된 배열을 입력으로 받고 가중치가 저장된 배열을 반환하는 함수가 되어야 한다. 

- algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'} 
가장 가까운 이웃들을 계산하는 데 사용하는 알고리즘을 결정한다. default는 auto이다. 
'auto' : 입력된 훈련 데이터에 기반하여 가장 적절한 알고리즘을 사용한다. 
'ball_tree' : Ball-Tree 구조를 사용한다. (Ball-Tree 설명 : https://nobilitycat.tistory.com/entry/ball-tree)
'kd_tree' : KD-Tree 구조를 사용한다.
'brute' : Brute-Force 탐색을 사용한다. 

- leaf_size : int
Ball-Tree나 KD-Tree의 leaf size를 결정한다. default값은 30이다.
이는 트리를 저장하기 위한 메모리뿐만 아니라, 트리의 구성과 쿼리 처리의 속도에도 영향을 미친다. 

- p : int
민코프스키 미터법(Minkowski)의 차수를 결정한다. 예를 들어 p = 1이면 맨해튼 거리(Manhatten distance), p = 2이면 유클리드 거리(Euclidean distance)이다. 
```


예제 (참조 - 혼공머신)
-------------
```python

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor

perch_length = np.array(
    [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 
     21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 
     22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 
     27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 
     36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 
     40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
     )
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 
     1000.0, 1000.0]
     )

plt.scatter(perch_length, perch_weight)
plt.show()

#테스트 세트 분할
train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42) 

#2차원 배열로 변경후 학습 시키기
train_input = train_input.reshape(-1,1)
train_target = train_target.reshape(-1,1)

test_input = test_input.reshape(-1,1)
test_target = test_target.reshape(-1,1)


#n_neighbors 기본 5상태 학습
kr = KNeighborsRegressor()
kr.fit(train_input, train_target)

#현재 상태에서 score 확인
print(kr.score(train_input, train_target))
print(kr.score(test_input, test_target))

#test_input값 예측
test_prediction = kr.predict(test_input)

from sklearn.metrics import mean_absolute_error

# MSE : 평균제곱오차  mean squared error
# MAE : 평균절대오차  mean absolute error
# MPE : 평균백분율오차 mean absolute percentatge error
# MAPE : 평균절대 백분율오차 mean percentage error

#예측값을 mae에 대입함
mae = mean_absolute_error(test_target, test_prediction)
# 19.157142857142862

#테스트를 위한 데이터 준비 - 원본 데이터랑 동일한 데이터 준비
predict_data = np.array(
    [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 
     21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 
     22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 
     27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 
     36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 
     40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
     )

#2차원 배열변경
predict_data = predict_data.reshape(-1,1)

#n_neighbors 값을 조정하면서 결과값을 관찰
for n in [1, 3, 5]:
  kr.n_neighbors = n
  kr.fit(train_input,train_target)
  predictino = kr.predict(predict_data)
  plt.scatter(train_input, train_target)
  plt.plot(predict_data, predictino, 'ro')
  plt.title('n_neighbors : '+ str(n))
  plt.show()
```

![knnr1]({{ site.baseurl }}/images/knn6.png)

knn의 문제점을 볼수 있음.
knn은 주위 예측값이 없을시에 정확한 값을 측정할수가 없음.
아래 보기는 무개를 100으로 주었을때 나오는 값인데.
x 가 45일때나 100으로 예측할 때에나 y값은 별 다르지 않음을 알수 있음.

```python
x = kr.predict([[100,]])

plt.scatter(train_input, train_target)
plt.scatter(100, x, marker = '^')
plt.show()
```

![knnr2]({{ site.baseurl }}/images/knn7.png)
