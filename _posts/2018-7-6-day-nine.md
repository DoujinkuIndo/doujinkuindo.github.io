---
comments: true
---
This morning had a pretty hellish start. Yesterday, I was having some trouble pushing commits to Github from the terminal, but I figured it was were no real problem, but it derailed my work for the entire morning. I keep a folder called "githubstuff" on the desktop that I use as the linked directory for our [NGC6819 github repo](https://github.com/GosnellResearchGroupSummer2018/NGC6819). The other day, I added a .fits file to that folder without realizing that it was over 300MB. Github doesn't work for files over 100MB because it would be really resource intensive to use an internet-based service for files that big. I deleted the file from the directory, but I still couldn't push commits even after the deletion. That's because the way Github handles file deletion is to push the file to the repo in "delete mode," and large files still prevent this. Since the terminal wouldn't push the file or the information that it had been deleted, I was trapped in a loop and couldn't push anything to Github at all. I tried a plethora of things to get around this, like adding the .fits file to the .gitiginore, adding something that ignores all files over 80MB to the .gitignore, adding the .fits file to another text file called .git/info/exclude that should have also ignored it, downloading a large file service for Github, and a thousand different terminal command combinations and attempts to get that file out of the way. None of them worked. I used Google heavily but never quite found the right words to describe what I was going through; I knew it had something to do with the fact that there was a record of the .fits file's existence in the git directory, but never googled the right phrase. Eventually googling "github can't push large files" (something that I'd googled using different words a hundred times before this) gave me something on stackexchange that was useful. I found a way to delete all instances of the .fits file in Github's history of changes and could finally push all the things I had been working on to the [repo](https://github.com/GosnellResearchGroupSummer2018/NGC6819), which were some results I'd gotten from doing photometry, a version of source_detection.py that was like 30 edits ahead of the one that was on the repo before, an early version of aperture_photometry.py, and some restructuring of the folders I've been over-seeing. Now that this problem is taken care of, I can finally start real work today. 

## Coding
To get my mind right again after being so focused on Github, I started by catching up on reading my colleague's blogs. They're making impressive progress. I'll have to be sure to talk to Rory on Monday and compare our codes -- they're very similar. I started my own coding by trying to edit the first half of [source_detection.py](https://github.com/GosnellResearchGroupSummer2018/NGC6819/blob/master/photometry%20codes/source_detection.py) to use source masking to subtract the background instead of sigma clipping because it seems more accurate. The results I get from this look a little better, but unortunately I haven't saved them yet because I was trying to make the code save the flux outputs to a text file. I think it would be useful this time around because this new background subtraction method seems more accurate. This was successful, but I was only ever able to get the data as a one dimensional array with 149 columns and in each column a string of 11 float values. I want the data as 11 columns and 149 rows to make other functions, namely `circular_aperture` and other aperture photometry commands, work properly. I have some ideas about how to fix and tried a few, but ran out of time before I had to go to a dinner. I can either write the rest of the code around this issue or transform the array (which I've already tried once or twice), but I'll look at both options either over the weekend or next week.

[source_detection.py](https://github.com/GosnellResearchGroupSummer2018/NGC6819/blob/master/photometry%20codes/source_detection.py) still operates under the assumption that the background is a scalar and subtracts a constant background from the image. However, it does look at a 2D map made of large pixels (~10x10 actual pixels) to determine the constant background level that it subtracts, and so is more precise since it's less prone to influence from  bright stars. Once I get this working, I'll not touch it again unless I want to find sources with this method in different fields. Instead, I'll focus on writing a source detection code that takes both x and y into consideration when subtracting the background. 

# Conclusion
I just need to keep plugging away at these codes. I'm going to write the 2D source detection code and an aperture photometry code next week, then I want to move on to making some HR-diagrams. I also think I'm going to download just one of the proper data fields soon since a lot of photometry codes are tailored to the data (i.e. certain photometry methods work better with certain fields based on the background, size, etc. of the field). Right now, I have drizzled (reduced composite) images from Hubble to work with and I need a different type of image to do good photometry. I don't want to waste time downloading the data, but I want to make sure that my code works best with the data that's most relevant to us, so I'll only grab one of the images for now.  
