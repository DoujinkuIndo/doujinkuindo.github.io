---
layout: post
title: The Hidden Power of Testing
author: Eddie Antonio Santos
---

Not along ago, when programming was just a hobby, I thought there was
nothing more to testing code than:

1. writing some code
2. checking that it compiles
3. running it
3. checking that it did what you expected it to do.

And if steps 2–4 failed, go back to 1 and try again.

With a little more experience in programming and having had hands on
experience working on various projects with other actual human beings,
I have a greater appreciation of testing and the role it plays in
development. For the Waterbear team, I was responsible for setting up
our unit tests and our continuous development environment. As well,
I designed acceptance tests for developing the debugging feature.

This post is for the beginner or the intermediate programmer.
Testing---whether it's unit tests or acceptance tests—is not only a way
of making sure your code works---_it's also a way to discover how to
design software._

And because I cannot imagine any topic quite as dry as "Eddie extols the
virtues of testing" I'll be filling this post up with tangentially
related animated GIFs.

![Meowton's Cradle](http://i.giphy.com/hO58ejrIKFIkw.gif)

So let's get started!

# But Eddie! What _is_ testing?

The word "testing" itself is pretty intuitive. You have a thing. You
check if it works. Boom. Testing! Let's go for a celebratory pint!

![Testing of the Dead](http://i.giphy.com/6MdXBLPUrL8mk.gif)

But what you're really doing is _ensuring the quality of the program_.
Testing is not only checking that the code works, but that it works
_well_, that the software behaves as expected, and---and this one takes
a while to learn---that the code is _resilient to future modifications_.
Resilient, like the extremophiles of the phylum _[tardigrada][]_, the
hearty waterbears!

[tardigrada]: http://en.wikipedia.org/wiki/Tardigrade

![Waterbear in love... or space, I guess.](http://i.giphy.com/ltxTcWfe7CzWE.gif)

What I've come to find is that this process of defining and having
a clear vision of what it means for your code to be _right_, gives you
a clear vision of how to write your code right. It might just be that
when you take on a project, you have no where to start. The task of
programming may seem daunting, but it is more so when there's no clear
place to begin. In this case... may I suggest testing?

Despite the abundance of methods, methodology and testing fanboys (yes,
even more than myself), there is no one way to do testing The Right Way™
(unless you're Edsger Dijkstra). The right way is the way that benefits
you, the programmer. That said, testing _does_ comes in many forms, and
there a lot of buzzwords and funny language that people speak in when
talking about testing. I'm going to try to clear up a few major
concepts: unit testing and acceptance testing.

# Unit testing!

This is probably what most people refer to when they're talking about
"the tests". It might be that you think unit testing is the only kind of
testing that is considered to be Real Testing (spoilers: nope). 

Unit tests are all about:

 1. breaking down your program into little tiny, self-contained pieces
    (the so called _units_).
 2. and making sure those itty-bitty pieces work well on their own.

The astute reader may notice something: this assumes that the problem
can be nicely broken down into little tiny, _self-contained_ pieces. For
a number of reasons, this is not always an easy thing to accomplish, and
the benefits of breaking down your program into testable units may not
fully justify the effort. For example, if your program needs to deal
with external systems, you have to go through the effort of emulating
the external system and its possible events, [which is not unheard
of][mocks], but certainly requires considerable effort. Nevertheless,
unit testing is must have for your testing arsenal.

[mocks]: http://en.wikipedia.org/wiki/Mock_object

## How do you do it?

Unit testing is so prevalent, most language have standard or _de facto_
ways of unit testing. For example, Java has [jUnit][], Python has the
[`unittest` module][pyunit], and for client-side JavaScript, there are
many options, including [qUnit][]. Basically, if you want a unit testing
framework look for a thing for your language that ends in
"[unit][xunit]". Of course, these aren't the only options, but without
putting much effort, you've probably got something that will easily
integrate with your system.

[jUnit]: http://junit.org/
[pyunit]: https://docs.python.org/2/library/unittest.html
[unittest]: https://docs.python.org/2/library/unittest.html
[qUnit]: https://qunitjs.com/
[xunit]: http://en.wikipedia.org/wiki/XUnit

But that misses the point. The most important part of unit testing is
_identifying the units_. You gotta find the smallest pieces of whole
cohesive units of code and only then can you assert that these units
work. As long as you know the small parts work, then you'll have the
confidence to assemble them into a larger system. But, er... how do you
identify the units?

![Looking for units](http://i.giphy.com/7SUZtE9bM5xbq.gif)

Maybe the better option is figure out what what's easy and obvious to
test _before_ you write the code for it. Then, it becomes clear what you
_need_ to code, and what is a good chunk of code to call a "unit".

Take this actual example of actual Python programming that [I actually
wrote][ghdwn]. For this script, I wanted to do a syntax check on Python
files that it downloaded. So I started defining a function along with
its _[docstring][]_. Docstrings, for those uninformed, are a super nifty
language feature in Python. Wanna document a function, class, or module?
Then the first line of said function, class, or module should be
a string that is automatically associated with the `__doc__` special
property of that object. This is super rad, you guys. It gets even
radder.

{% highlight python %}
def syntax_ok(contents):
    """
    Given a source file, returns True if the file compiles.
    """
{% endhighlight %}

Okay, so we've got the purpose down. That's one step of the process of
understanding what task you have to accomplish as a programmer. But how
will you know that this works? Enter the _[doctest][]_: beginning Python
programmers are familiar with [the interactive console][pyrepl] where
they can try lines of code and see their result. The prompt for the
interactive console is three right-facing arrows: `>>> `. Say I'm
testing the completed `syntax_ok()` function. A sample session in the
interactive console may look like this:

~~~ python
>>> syntax_ok('print("Hello, World!")')
True
>>> syntax_ok('import java.util.*;')
False
>>> syntax_ok('\x89PNG\x0D\x0A\x1A\x0A\x00\x00\x00\x0D')
False
>>> syntax_ok(r"AWESOME_CHAR_ESCAPE = '\x0G'")
False
~~~

So, why not put what you _expect_ to happen on the console into your
documentation and call it test? That's exactly what a doctest is. Now we
know what we want to both in human terms, and what we want to accomplish
in computer terms---in this case, the return value of this function.
Additionally, since, we embedded the test in the documentation, we have
precise documentation for how to _use_ the function in the future. And
if it passes the tests, then we know our documentation is correct.
I told you it got even raddererer! Putting it together, it looks like
this:

[pyrepl]: https://docs.python.org/2/tutorial/interpreter.html#interactive-mode
[doctest]: https://docs.python.org/2/library/doctest.html

{% highlight python %}
def syntax_ok(contents):
    r"""
    Given a source file, returns True if the file compiles.

    >>> syntax_ok('print("Hello, World!")')
    True
    >>> syntax_ok('import java.util.*;')
    False
    >>> syntax_ok('\x89PNG\x0D\x0A\x1A\x0A\x00\x00\x00\x0D')
    False
    >>> syntax_ok(r"AWESOME_CHAR_ESCAPE = '\x0G'")
    False
    """
{% endhighlight %}

At this stage, I knew what I wanted, but didn't know how to accomplish
it, so I just looked up appropriate documentation. But the important
part was that I knew where to start. I implemented it thusly:

{% highlight python %}
    try:
        compile(contents, '<unknown>', 'exec')
        # why does compile throw so many generic exceptions...? >.<
    except (SyntaxError, TypeError, ValueError):
        return False
    return True
{% endhighlight %}

There we go! As soon as I implemented it, I had working tests that
I could run like this[^1]:

~~~ sh
python -m doctest ghdwn.py
~~~

[^1]:

    **Bonus PROTIP**: I like to automatically run my doctests when
    I save my work. I use [`pytest`][pytest] with the [`xdist`][xdist]
    plugin. Install like so:


    ~~~ sh
    pip install pytest pytest-xdist
    ~~~

    Then to start running tests continuously, I open a new terminal and type
    the following in the same directory as the file I'm working on:

    ~~~ sh
    py.test -f --doctest-mod .
    ~~~


As with standard counterintuitive Unix fashion, no output means that my
all of my tests passed!

But lo! As `syntax_ok()` was being called in a long-running script that
checked the syntax of many, many, _many_ Python files, an enormous flaw
soon became apparent. After a while, my script would crash with a
`MemoryError`, indicating that my program had somehow run out of memory.

![Pictured: me encountering a memory leak](http://i.giphy.com/11TXm8y1koY2n6.gif)

Evidentially, calling `compile()` cached the results of compiling
code---code that I never used, since I merely called compile for its
side-effect of reporting whether a file contained syntax errors. As
a result, I had to fix this dreaded memory leak to stop randomly
crashing my long running script.

This is where having unit tests really pays off. The inevitable occurred:
_I had to modify my code_ and had to make sure that _it did the same
thing_. Luckily, I could assert that my code behaved the same, as I had
basic tests in place. Now all I had to do was figure out how to patch that
memory leak. It occurred to me that I can't have a memory leak in
a process that exits immediately, so using some Unix voodoo, I `fork`'d
my process into a parent--the one expecting an answer--and the
child--the process that would compile the code, cache the result and
immediately exit, destroying with it the cached results of compilation.
My finished product looked like this. Note the doctest within the
documentation:

{% highlight python %}
def syntax_ok(contents):
    r"""
    Given a source file, returns True if the file compiles.
    >>> syntax_ok('print("Hello, World!")')
    True
    >>> syntax_ok('import java.util.*;')
    False
    >>> syntax_ok('\x89PNG\x0D\x0A\x1A\x0A\x00\x00\x00\x0D')
    False
    >>> syntax_ok(r"AWESOME_CHAR_ESCAPE = '\x0G'")
    False
    """

    pid = os.fork()
    if pid == 0:
        # Child process. Let it crash!!!
        try:
            compile(contents, '<unknown>', 'exec')
        except:
            # Use _exit so it doesn't raise a SystemExit exception.
            os._exit(-1)
        else:
            os._exit(0)
    else:
        # Parent process.
        child_pid, status = os.waitpid(pid, 0)
        return status == 0
{% endhighlight %}

In this way, my simple unit test helped me:

 1. Figure out what task I need to accomplish. This became my "unit".
 2. Determine what would be the correct output of said unit.
 3. Document _how to use_ my function.
 4. When it came time to change my function, ensure that its behaviour
    would stay the same.

![Antonym of Tubular](http://i.giphy.com/ylKevgMaQ9MeA.gif)

Some people call this  _[test-driven development][TDD]_ or if you really
want to be pedantic _test-first development_. Either way, your code
lives to serve the test. Under this framework, _your code's only purpose
is to ensure the tests pass_. Some people are really adamant about this
process, and assert that the only way to know your code will work
properly is if you write the tests first. I... remain skeptical. It's
certainly a nifty technique, and one I use often, but it's not the
_only_ way to do things. Another method is acceptance testing.

[ghwdwn]: https://github.com/eddieantonio/ghdwn/blob/24f0a57052d67fd153ca808d5521732931a66a2e/ghdwn.py#L228
[docstring]: http://en.wikipedia.org/wiki/Docstring#Python
[TDD]: http://en.wikipedia.org/wiki/Test-driven_development

# Huzzah! Acceptance Testing!

Acceptance testing is simply defining the behaviour we expect, and under
what circumstances should we say that a thing fulfils its duty. Wait...
this sounds familiar... didn't we _just_ talk about this? Well, kind of.
While unit testing focuses on the smaller parts, acceptance testing is
much more high level. 

But testing doesn't have to be automatic or focused on specific, small
units of code. Definitely, it's nice when we can test individual units
of code---plus, it's generally indicative of a cleaner, more modular
design-- this doesn't necessarily say anything about the ultimate
awesomeness of the software. Besides, sometimes it's just straight up
difficult to break the code up perfectly in this way. It happens.

![Golly](http://i.giphy.com/104vPBH8buV9vy.gif)

Don't beat yourself up for it. There are a lot of design decisions to
make and you’re never gonna make everything perfect; 
a 1-1 correspondence between code unit and function is not the ultimate
goal of programming: it's making a system that _works_! ...er. Whatever
that means.

The point of _acceptance testing_ is to define _scenarios_ that define
the _requirements_ of your code, i.e., what your code must ultimately
achieve to be considered "good". You define any conditions that must be
setup prior to the test, the tangible steps that a person has to
undertake to make the scenario happen, and the criteria for saying
"yep, this sure did work". It's like a checklist, that you check off all
of the steps, and at the end, you know the code works.

This is a relatively new concept for me, so I don't apply any serious
formality to it. I did find myself using this in Waterbear recently to
determine whether I was writing the right thing for the Waterbear
debugger. Before I started writing any substantial amount of code for
the debugger, I had no idea where to start. I legitimately struggled for
a while with a file in my editor that just read `// debugger`. It was...
Embarrassing.

But a chance encounter with my UCOSP supervisor, Eleni Stroulia,
reminded me about acceptance testing---something I had only ever
practiced once. So I got to it! I looked at [the informal list of
requirements][debugger-reqs] that we collected on our issue tracker.
Then I edited these initial feature requests into [a feature
list][debugger-feat] that was a bit more fleshed out. After this, I got
started writing the tests!

[debugger-reqs]: https://github.com/waterbearlang/waterbear/issues/989
[debugger-feat]: https://github.com/eddieantonio/waterbear-debugger-acceptance-tests#feature-list

The template I followed was as follows:

<dl>
<dt> Setup </dt>
<dd> How to get the system into a state necessary to begin the test. For
the Waterbear debugger, most of these came along with an example script
that would demonstrate the desired phenomenon. </dd>
<dt> Preconditions </dt>
<dd> Any special state that the system must be in prior to the test. </dd>
<dt> Test </dt>
<dd> The steps necessary that a user would do to accomplish the given
task. </dd>
<dt> Acceptance criteria </dt>
<dd> This is the checklist. This is the list of things that
<em>should</em> happen. </dd>
</dl>

Of course, as this type of testing is usually performed by a human and
not automatically by a computer, care should be taken in sucking out any
subjectivity, vagueness, and ambiguity in the script.

Once everything has been defined, go do it! In my case, writing even
half of the tests helped me think of the tasks that I had to accomplish
and gave me a good idea of the architecture I had to build in order to
reach that goal. After writing enough code to fulfil even _one_ of these
task, I'd go test it. And out of the process, I got this illuminating
state diagram that displays all of the ways that a user can plausibly go
through execution states---it turned out to be way more than I expected.

![State diagrams get me excited, okay?]({{ site.baseurl }}/images/testing/states.svg)

(Apologies for the criminal-unfunniness of the last still image.)

The end result should be a list that should be clear to follow in order
check that the debugger is working properly. And hey! We now have
a clear (or more clear) definition of what it means that "the debugger
is working properly."

The process of writing the acceptance tests gave me a fresh look at the
problem I had, and allowed me to think, and visualize it in different
ways; it allowed me to _organize the complexity of the task_. And for
that reason alone, I'd recommend perhaps writing an acceptance test when
you have a large task and no idea how to tackle it.

[pytest]: http://pytest.org/latest/
[xdist]: https://pypi.python.org/pypi/pytest-xdist

# Conclusion

In the end, testing may seem like a mindless process---something that
hoighty toighty software engineering types (of which I'm one) always
goad others into doing. But the fact is, despite the obvious motivation
of "checking that it works right", testing also yields a method for
discovering how to solve a problem. And I think that's pretty neat.

![What a beaut!](http://cdn.makeagif.com/media/4-04-2015/ARXaPF.gif)

[Scary]: {% post_url 2015-02-20-Scary %}

—Eddie

---

<!--

  WHAT FOLLOWS IS MY STRANGE RAMBLING THAT LEAD TO THE WRITING OF THE
         ARTICLE YOU SEE ABOVE. Also, so more cute animal gifs.


> What _is_ testing?
> Testing means to check that a system works well, expects, and subsequent modifications.
> Ensure the quality of the output.
> It is also a method of code discovery -- figuring out how to design a system.
> Testing comes in many forms, so you should never take it with a one-size-fits-all approach.
> Talk about two types of testing: unit tests and acceptance tests.
> I'm not an expert -- I'm learning the ropes but sharing my (limited) experience.

![Kitty](http://i.giphy.com/ky7z7mTkdFmog.gif)
![Puppy](http://i.giphy.com/11HRUadigHjcbe.gif)
![Puppy II](http://i.giphy.com/mb4D2qoFWDohW.gif)

> Structure:
>  The problem.
>  The solution
>  The tools
>  How the Waterbear uses them.
>  When not to use unit tests (e.g., acceptance testing!)

> Discuss and briefly compare buzzwords: TDD, BDD, TFD.
> I use their tools though: TDD.
> "Testing _drives_ development. Your code exists to serve the tests.
> Often conflated with test-first development.

> Testing is a part of iteration. It is a part of development.

> Doctests! Discuss the discovery process with a REAL example! (Dunno where)
> `py.test -f --doctest-mod`

> What do I want to accomplish -- then I can start thinking of how it's
going to happen.
> It encourages me breaking down a task into smaller, testable parts,
with well-defined behaviour, a function name (that's important), and
just enough comments.
> Sometimes the solution is more interesting than the implmenetaion.
> It's a way of helping me think about the problem.

> Briefly describe BDD.

> Tests are supposed to be simple and not have this crazy fanciful design. Duplication is fine. 
> Brifely discuss coverage. But link to that paper! And what it means to use coverage tools.

> Briefly discuss continuous integration.
> Dem badeges, like gym leaders! It's a trend.
> Not to fix the problem, not the blame.
> Alert the team that there is an issue and it needs fixing.

> What are we using for waterbear? How do we accomplish.

# A few other things

I'll briefly discuss some other topics related to testing.


But in the world of programming, things are hardly as simple as they
seem. [But don't let that scare you!][Scary] Applying a little more thought
into the matter 

besides networking
and working with other systesms aPis typically gets in the way of having
nice little units to test. so unit test break here. So let’s talk about
more higher level tests. there are system tests that I’m not going to
talk too much about, but then there are accepentance tests."

> Acceptance tests

> Defining the behaviour we expect!
> How's it differnet than a unit test?

> This is GOOD to just use to transition into acceptance tests.
> "But testing doesn’t have to be automatic or focused on specific small
units of code. DEfinitely, it’s nice when we can test individula units
of code, and it’s generally indiciative of better design, since there
are well defined components that we can test individually and compose
together but this doesn’t neccessarily saying anything abotu the
ultimate aseweosme ness of the software. Besides, sometimes it’s just
hard to componentize the code this way. It hap-pends. don’t beat
yourself up for this. There are a lot of design descisions to make and
you’re never gonna make everything perfect and always have a 1-1
coorespondance between code unit and function, besides networking and
working with other systesms aPis typically gets in the way of having
nice little units to test. so unit test break here. So let’s talk about
more higher level tests. there are system tests that I’m not going to
talk too much about, but then there are accepentance tests."

> E.g., in waterbear, supress or ellide internal javascript errors.
> Why is this in a unit test?

> When not to unit test?

> Determining what your code _should_, but on a higher level than specs
tend to go to.

> I use them for debugger stuff, to test out all the scenarios.

> "But I am finding it helpful. I took the original list of
requiremnents from the gituhb issues, then took the list and fleshed out
the requirements, breakign them down into small piepces. Then for each
requirment, I write an accepetance test. How do you know that my
debugger does what I told you it wpould do? It does pausing, so you
should be able to pause the script. so I write an acceptance script that
says you need to have a file that runs for long enough to be able to
pause it (given), and then. . . you pause it. We verify it becacuse the
app clearly states it’s paused, and tells you what block it’s paused at
– and what it means to be paused at a block (it means that this block
will be executed next, if execution were unpuased or steppped). This
also lead to me making a handy state diagram to see how you go between
execution states and the inputs that cause these changes. it helped me
think and oraganize the complexity of the task and the valid inputs at
any particular state."


> Conclusion
>
> - why this is not called Testing: A reflection and not "Testing the right way"
> - throughout has the benefit of heloing you you (me) think out the
problem in different ways and seeing the other side of the iceburg, so
to speak.

-->
