---
layout: post
title: Artistic Algorithms
author: rbrooks6
---

Much of the literature I have read so far for digital humanities has emphasized the idea that digital humanities destroys the barrier between natural sciences and social sciences. This seems like an exciting and simple enough statement, but one thing I have found interesting is that the reality is, the juxtaposition illuminates some distinct differences and issues. It also brings to light the limitations and possibilities of the software involved in these science studies.

In *A Companion to Digital Literacy Studies: Algorithmic Criticism* Susan Schreibman and Ray Siemens explore the history and realities of text analysis with computer algorithms. One reality is that although exploring and researching social sciences often follows a similar method of other forms of scientific research with both methods involving  hypothesis, research, and conclusion. As well, both involve tools of data measurement and evaluation. Though the methods are similar, the way it is conducted and the conclusions can vary. One important distinction is the end goal. The text emphasizes that natural science researchers conduct research looking for one answer or fact to solve a concrete problem. With social sciences, the research and even the idea of “fact” can be re-interpreted and is not always definitive. As well, the tools for research can be questioned and argued. Schreibman and Siemens exemplified this with the explanation of an algorithm that evaluates words used repeatedly in text. The formula used: tf - idf orders the most commonly used words, but emphasizes importance on words used most commonly by one source/ character in a piece of literature. For social scientists, this can seem like the perfect way to prove their literary analysts by using how often a word is used to support their idea for literary themes and biases. The problem is, the results aren’t necessarily concrete. The formula can be manipulated by an analyst, and “proof” is based on how much it supports the hypothesis of the researcher. This sort of issue is not often found in natural sciences, but is incredibly present in social sciences.

The fact that text analysis algorithms can bring to light some important divides between two types of sciences is incredibly interesting. It is an idea that can be useful to study and analyze further. I expect the way humanities and social sciences function will continue to change, but how? This is also interesting because it reminds me of the studies and research being done on algorithms/artificial intelligence and art. In the article *Movie Written by Algorithm Turns Out to be Hilarious and Intense* author Annalee Newitz writes about *Sunspring*, a science fiction movie written by an algorithm. The movie turns out to be really funny and kind of awkward. Equally as important, it brings into light the realities about computer’s abilities to create art. Currently, all that can be done is imitation and conglomeration of what has already been done, but isn’t that what art has always done? How long will there be a line of divide between technology/natural sciences and humanity? As sciences advance and social science transforms in this digital age, who knows what the future hold




**Sources**

Newitz, Annalee. "Movie Written by Algorithm Turns out to Be Hilarious and Intense." Ars Technica. N.p., 09 June 2016. Web. 12 Sept. 2016.

Siemens, Raymond George, and Susan Schreibman. A Companion to Digital Literary Studies. Malden, MA: Blackwell Pub., 2007. Print.

