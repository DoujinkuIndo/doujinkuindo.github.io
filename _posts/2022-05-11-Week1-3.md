
# **Week1-3 : NLG**


## **1. NLG : 언어를 생성**

- 주어진정보(text, video, …)를 기반으로 정보 **축약, 보강, 재구성**
- 축약
    - 데이터 중에서 핵심
- 보강
    - 부족한 정보를 추가
- 재구성
    - 기존 정보의 형태를 변형 / 기존 정보로 추론 해답

## 2. Text Abbreviation: 문장에서 핵심만

- Summarization : 요약
    - Abstractive : 없던 단어를 가지고 요약, 좀 더 고난이도
    - Extractive :
    - Application: 뉴스요약
- Question generation
    - Application: online 학습도구
- Distractor generation
    - 오지선다에서오답을생성
    - Application: 데이터증강

## 3. Text Expansion : 부족한 정보를 추가

- Short text expansion
    - 정보를 추가해 데이터 길이를 늘리기
    - Application: 짧은 제목으로 내용 생성
- Topic to essay generation
    - Application: “가족”, “장남감”, “크리스마스” → “케빈은크리스마스에 혼자집에서 …”

## 4. Text Rewriting : 문서 형태를 변형하거나 추론을 통해 정답을 생성

- 문법➡ biLSTM, BERT(RoBERTa)
- 감성분석➡ BERT(RoBERTa)
- 문장유사성➡ Bert(RoBERTa)
- 추론➡ T5, Bert(ALBERT, DeBERTa)
- 언급대상추론 (WNLI) ➡ Bert(SpanBERT, RoBERTa)
- Style Transfer
    - 긍정↔부정
    - Application: 데이터증강, 말투변화
- Dialogue Generation
    - 페르소나를 갖은 참여자의 대화를 생성
    - Application: 챗봇

## 5. Models :  NLG SOTA 모델을 구성하는 주요 아키텍처

- Encoder decoder
    - RNN Seq2seq
    - Transformer
- Copy and pointing
- Gan (Generative Adversarial Network)
- Memory network
- GNN (Graph Neural Network)
- External Knowledge

---

## week1-3 과제

### 1. Paperswithcode([https://paperswithcode.com/area/natural-language-processing](https://paperswithcode.com/area/natural-language-processing))에서 NLG extractive summarization task중 하나를 선택하여 본인 블로그에 정리해보세요. 아래 3가지 항목에 대해서 정리하세요. (각 항목 고려 사항 참고)

- ****Extractive Text Summarization : 주어진 문서에서 단어와 문장을 선택하여 가장 잘 요약 표현 한것,****
- ****CNN/Daily Mail : CNN 과 Daily mail 웹사이트의 뉴스 question와 story으로 형성됨, 약 30만개의 데이터 쌍****
- **[HAHSum](https://paperswithcode.com/paper/neural-extractive-summarization-with) : Hierarchical Attentive Heterogeneous Graph Network를 사용 문장간 중복 종속성을 강조함, 문장 중복성 인색 그래프를 통해 성능 개선**
- **[MatchSum](https://paperswithcode.com/paper/extractive-summarization-as-text-matching) : 추출 요약 작업을 Text Matching 문제로 공식화, 원본 문서에 semantic text matching을 통해 문서와 문서 추출을 통해 얻는 요약문을 semantic 공간에 matching**

### 2. 팀원들의 게시글을 읽고 피드백 댓글을 달아보세요.
