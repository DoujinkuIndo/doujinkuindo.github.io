## Live-tweeting during presidential debates: Do our expressions of moral outrage change anyone’s mind, or simply add noise to an already-loud echo-chamber?

I’m the type of Twitter user who, once every few months (or years, in some cases) remembers they have an account and sends out an obligatory tweet or two.  Really, though, I’m never on Twitter except during presidential debates.  During the last election cycle as Barack Obama and Mitt Romney vied with each other for the presidency of the US, I found myself without cable or reliable internet connection.  Living in a red state at the time (the kind of state in which [Todd Akin, a dangerously anti-science buffoon]( https://blogs.scientificamerican.com/context-and-variation/here-is-some-legitimate-science-on-pregnancy-and-rape/), had held one public office or another for ~3 decades), I was outraged enough to be keeping track of the election pretty closely.  I really wanted to watch the all the debates, both local and national, but my limited data plan and poor cell-coverage made catching the live-streams a near-impossibility.  Not wanting to miss out on the action, I hopped on the unencrypted WiFi from [the bar](http://www.peppersbarstl.com/) across the street from where I was staying (which has an amazing burger, if you're ever in the area), logged into Twitter, and keyed in some pertinent hashtags like #debate and (later) [#bindersfullofwomen](http://www.thedailybeast.com/articles/2012/10/17/mitt-romney-s-binders-full-of-women-comment-sets-internet-ablaze.html). It wasn't nearly as informative as watching the actual debates, but it was surely interesting and I've continued to log on to Twitter during subsequent political debates.  In fact, most of the accounts I follow on Twitter are there for no other reason than because they're likely to post witty remarks or prudent critiques during these debates.

During this past election cycle, I had my follower-list stocked to the point that I didn't even need to enter #debate or a related hashtag - I got the blow-by-blow right on my home page.  After the second debate, though, out of curiosity, I clicked on one of the few #maga hashtags that made its way across my screen.  The interpretations of the debate on the #maga feed were antithetical to those that were expressed by people in my own Twitter network.  Trump was being congratulated for speaking incoherently, people were lauding his racist and nationalistic policy ideals, and there were numerous requests that Secretary Clinton be subjected to fates far worse than imprisonment.  I had tweeted during the debate hoping that people like this might see my tweets and, at least momentarily, entertain an alternate perspective.  Many of the tweets I read on my home screen seemed geared toward these people, trying to reason with them or pleading that they recognize "others" as human beings.  I began to wonder, being that I had seen none of these people's tweets during the debate, if they had seen anything I or other far-left Twitter users had sent off into the twitterverse over the past couple of hours.  It dawned on me that this type of debate-night [slacktivism](https://en.wikipedia.org/wiki/Slacktivism) might be completely in vain.  And, if this slacktivism really isn't having an impact on people with differing views, then it would appear that a great many people are spending ludicrous amounts of time behind their monitors when, perhaps, they'd be more effective going door-to-door (or even just landline to landline).  I decided to investigate.  Now, I'm studying the [distance](https://en.wikipedia.org/wiki/Distance_(graph_theory)) between liberal and conservative Twitter users to determine whether members of the far-left and far-right are seeing each other's tweets in order to ascertain the effectiveness of Twitter as a platform for reaching and influencing ideological opponents.

### Some background theory

There are a couple of existing bodies of research to consider when taking on this problem.  Firstly, it's worth considering previous research into slacktivism.  [One study](https://www.washingtonpost.com/news/monkey-cage/wp/2014/03/12/does-slacktivism-work/?utm_term=.b4525fa73c58) has found that public expression (such as online posts) of political support are less effective than private expressions.  It's not all for nothing, though.  Sometimes, slacktivism movements (i.e. hashtags) gain enough momentum to break out of the echo-chambers that are Facebook Land and the Twitterverse, making their way into public view where they can have a big impact (see [here](http://mediashift.org/2015/02/why-slacktivisim-matters/)).  Given the relative newness of and conflicting views on slacktivism, additional research into what types of efforts are and are not effective seem called for.

Next, getting to the heart of the theory behind my research, it's important to consider what's known about social networks.  Here, social networks refer not to social-media platforms like Twitter and Facebook, but to affiliations between people (which exist both online and in the physical world) through which information can flow.  This conception of a social network is consistent with that of [*Understanding Social Networks* by Charles Kadushin](https://books.google.com/books?hl=en&lr=&id=ALOhpMgkW_cC&oi=fnd&pg=PP2&dq=Kadushin,+Charles.+2012.+Understanding+Social+Networks:+Theories,+Concepts,+and+Findings.&ots=7P346Eq_mL&sig=SCFsAEB67Ghz7913vWsKBuOIo8Q#v=onepage&q=Kadushin%2C%20Charles.%202012.%20Understanding%20Social%20Networks%3A%20Theories%2C%20Concepts%2C%20and%20Findings.&f=false).  In a social network, the further the distance between any two people, the less likely they are to receive information from one another.  Additionally, novel information is more likely to be received via weak ties (acquaintances, friends of friends [perhaps even ideological opponents you just happen to be friends with on Facebook or Twitter]) [(see Granovetter’s paper on weak ties)](https://sociology.stanford.edu/sites/default/files/publications/the_strength_of_weak_ties_and_exch_w-gans.pdf).

### Methods

**Data Collection**

Analyzing distance between Twitter users necessitates a large sample of data taken directly from Twitter.  Because I was interested in debate tweets, specifically, set out to collect tweets from the third presidential debate.  Using [Twitter Archiver](https://chrome.google.com/webstore/detail/twitter-archiver/pkanpfekacaojdncfgbjadedbggbbphi?hl=en), a free twitter-collection plugin for Google Chrome, I set up some rules to collect tweets containing hashtags signaling engagement with the debate (#debate and/or #debate2016).  The collector ran every 15 minutes during the debate, grabbing a total of 37,286 tweets ([available here](https://www.dropbox.com/s/40adfa1rs7xgoxd/anon.csv?dl=0)).  Given the random and non-continuous sampling methods, such a large sample of tweets is idea.  Twitter Archiver does not place hashtags in their own column, nor does it provide User IDs (which are important for replication purposes because, while screen names may change, User IDs remain constant).

To get lists of hashtags and the User IDs for each tweet, I used the [Twitter API python module](https://pypi.python.org/pypi/twitter/1.17.1).  For each tweet in the dataset, I queried the API to return the full JSON data for the tweet, which includes the User ID and a list of hashtags.  Unfortunately, each time I ran this script, it threw an error about 13 hours in, meaning that I didn't get hashtags and User IDs for all my data.  Because this happened reliably at around the 13 hour point and because my data were in a dictionary (Python does not iterate over in any particular order), I'm led to believe that 13 hours is when my [free-tier EC2 instance](https://aws.amazon.com/ec2/?sc_channel=PS&sc_campaign=acquisition_US&sc_publisher=google&sc_medium=ec2_b&sc_content=ec2_p&sc_detail=ec2&sc_category=ec2&sc_segment=154833507028&sc_matchtype=p&sc_country=US&s_kwcid=AL!4422!3!154833507028!p!!g!!ec2&ef_id=WEJLvAAAAbwLD9Ql:20161207063001:s) ran out of memory.  This drastically reduced my dataset to ~16,000 tweets.

Being that the end-goal of this project is to calculate the distance between users using different hashtags, it was necessary to gather the associations (i.e., friends, followers) of each user in my data.  This was done with another set of API queries.  Again, this script crashed reliably, this time at the 16-hour mark.  I ended up with network data for several thousand users, anyway.  In retrospect, it wasn't necessary to collect both followers and users (both of which require their own GET requests), which means I could have, in theory, collected twice as much data before running out of memory.  Having both followers and users isn't necessary because only one is required to build a directed network.  If a is friends with c and c is friends with a, I now have the same information that I would have had by knowing that a is friends with c and a is following c at a lower cost in terms of computation and memory.

**Analysis**

From the data I collected, I calculated the most-used hashtags.  Ubiquitous hashtags referencing the debate itself (i.e. #debatenight, #foxnewsdebate) were removed.  Variations of the same tag were combined (i.e., different spellings and/or capitalizations were combined, #hillary and #hillaryclinton were combined, etc).  In descending order, these are draintheswamp, bigleaguetruth, trump, hilary, maga (make America great again), imwithher, badhombres, scotus, vaginaeducation, shutup, 2a, nafta, crookedhillary, minimumwage, smallbusiness, wikileaks, november8th, mexico, nevertrump, and plannedparenthood.

To get a better idea of what was going on within the data, I parsed all user/friend pairs to an edge-list that was compatible with [Gephi](https://gephi.org/).  Gephi is a free, open-source, multi-platform graph visualization program.  I also parsed an edge list of consisting of which users used which hashtags where the direction is user --> hashtag (because the hashtag is being shared by the user and is an intermediary to the other users who'll then see the hashtag).  Together, these edge lists allowed me to visualize which users are using which hashtags.

**Figure 1** ([view full-sized](https://github.com/libbyh/methods-f16/blob/master/final-projects/josh_dh_final/mixed.png?raw=true))
![figure1](mixed.png)

In figure 1, blue nodes represent users and pink nodes (labeled with pink labels) represent hashtags.  Blue edges connect users to users and pinkish edges connect users to hashtags.  Larger nodes have higher degree centrality.  Larger labels also correspond to higher degree centrality, such that the most popular hashtags have the largest nodes/labels (as well as the most connected user nodes).  Nodes with a degree of less than 1 have been filtered out, as have nodes that didn't connect to the central mass shown in the picture.  Knowledge of how to accomplish all of this came courtesy of the [Gephi](https://github.com/gephi/gephi/wiki/Import-CSV-Data) [documentation](https://github.com/gephi/gephi/wiki/Import-CSV-Data) and from [Dr. Jennifer Golbeck's](http://www.cs.umd.edu/~golbeck/) super helpful [YouTube tutorials](https://www.youtube.com/user/jengolbeck/videos).  As I interpret it, this image shows the relatedness of the different hashtags.  #shutup and #vaginaeducation appear to have an incredibly high relationship as do #smallbusiness and #minimum wage.  Many of the most popular hashtags (which appear to come from right-wing twitter) are very related, as evidenced by the big cluster toward the bottom of the image.  People tweeting about the second amendment appear have less distance to have less distance to right-wing hashtags than to left-wing hashtags.  None of these initial findings are unexpected, which leads me to believe that my Gephi manipulations were successful.

**Figure 2** ([view full-sized](https://raw.githubusercontent.com/libbyh/methods-f16/master/final-projects/josh_dh_final/modularity.png))
![figure1](modularity.png)

Figure 2 shows the same graph with the same layout, but with different colors.  Here, I've asked Gephi to calculate the graph's modularity.  [This allows Gephi to detect communities](https://www.youtube.com/watch?v=7LMnpM0p4cM).  Now, we have an even clearer picture of which hashtags and users are related to one another.

(A quick note about exporting images from Gephi.  When exporting, you can't set the render's resolution.  PNG resolutions are relatively small for large graphs.  I found that exporting to PDF was an ideal solution.  The PDF exports can be blown up in Photoshop without losing any quality [I imported them at 400 DPI, cropped, edit > transform > scale, and exported as PNG.])

Looking at these two images, we can get somewhat of an idea of the average distance of users tweeting one hashtag to other users tweeting another hashtag.  Numbers to represent these distances would be helpful, though.  Unfortunately, Gephi's ability to detect [shortest paths](https://en.wikipedia.org/wiki/Shortest_path_problem) is limited.  By clicking on two nodes, Gephi will show the shortest path between the selected nodes.  But to get the shortest paths for *all* nodes, one would have to manually select every pair and write down the shortest paths somewhere else.  Given that calculating all shortest paths for a large graph would take a long time, I suppose it makes sense not to include this functionality in Gephi.  Still, it would be nice if the functionality existed in the form of a plugin (so, I might create such a plugin).  Though Gephi can't handle this part of my investigation, Python can.  Using this [Gist](https://gist.github.com/mdsrosa/c71339cb23bc51e711d8), I was able to set up a script that

1. calculates the shortest paths between each given user and each of the hashtags (or, really, the shortest path to a *user* who uses a given hashtag)

2. for users who use hashtag x, calculates the mean and median shortest distances to hashtag y (for all hashtag pairs).

The script has been running for about 12 hours now, and may conceivably run for at least another 12 considering the number of paths it has to calculate.  So, unfortunately, I'm not able to share the results, presently.
![calpaths](paths.png)
(shortest path values represent number of edges)

### Discussion

Going into this this project, I had hoped to articulate an argument in which I described why live-tweeting during debates to sway opinions of ideological opponents may be a futile endeavor because there is a great distance between any given far-left and far-right user within the twitterverse.  When distances between nodes in a network are great, or when nodes do not connect at all, little or no information from the far-flung nodes will reach the other node.  While I could speak to this claim using my images as evidence, I'd prefer to wait until I have actual numbers, which will provide a much clearer picture than a couple of cluttered visualizations.  However, as touched on above, it does appear in the visualizations that there are groupings of users and hashtags, and that users tweeting out some hashtags are probably more likely to be further away form users tweeting other specific hashtags.

There are a few other things worth considering.  First, there are numerous limitations to my implementation of this study, most of which stem from problems with data-collection.  These include the discontinuous (every 15 minutes) sampling process, as well as the atrophy of data points that resulted from memory problems.  These factors may limit the validity and generalizability of anything I might find when my shortest-paths script is done running.  Secondly, I do not do anything to account or control for [Twitter bots](https://en.wikipedia.org/wiki/Twitterbot).  Twitter bots are thought to have [played a big role](http://www.theatlantic.com/technology/archive/2016/11/election-bots/506072/) in this election cycle and may have accounted for [as many as 25%](https://www.washingtonpost.com/news/the-intersect/wp/2016/10/19/one-in-four-debate-tweets-comes-from-a-bot-heres-how-to-spot-them/?utm_term=.73015ba63b24) of all tweets during the 3rd debate, for which I collected data.  Future investigations should decide how to handle bots.  While the bots and their reach are definitely worth looking into, an investigation into (human) user network distances may want to consider implementing some kind of bot-detection algorithm to remove non-human data points.

There's one more thing to consider.  Even if it turns out that debate-night slacktivists are ineffective when it comes to swaying opinions of others, their efforts aren't necessarily bad or wasted.  While they might not even reach people with differing views, they may get *likes* and *retweets* from similarly minded users, which can be both gratifying and reassuring.  Similarly, there may be some psychological benefit for the like-minded people reading these tweets, who see that their views are shared.  Sure, this can create a false sense consensus when, in reality, there may be just as many people on Twitter with vastly differing views whose content you simply never see.  Sure, the false-consensus effect this may create is problematic, and I think it may have contributed to the intensity of the shock that many of us felt when the election results started coming in.  I suppose we need to find a way to retain the psychological benefits of tweeting within an echo chamber while also finding (unless it turns out that ideological opponents are already being reached) ways to reach people with differing views.  Slacktivism is still young, and will probably continue to evolve over the years to come.  It's probably not going away, regardless of how effective it may actually be, so it's worth investigating what does and doesn't work in order to make it as effective as possible.
