---
layout: post
title: Representation Learning
---


# Representation learning

What is the goal?
To find underlying structure and patterns in your data.

## Auto encoders

Learn to recreate your inputs using only a certain amount of features.
These features explain the data. The key to good explanations is?


## Kolmogorov complexity (KC)

> The (Kolmogorov) complexity of a string is the length of the shortest possible description of the string in some fixed universal description language. ([Wikipedia](https://en.wikipedia.org/wiki/Kolmogorov_complexity#Definition))

Traditionally the kolmogorov complexity cannot be calculated. However, we define our string as the auto encoders input and the universal description language as the network topology (weights and nodes, activation function...). Thus, we can define the kolmogorov complexity of a data set as the size of the smallest autoencoder that can recreate it. 

## Applications to unsupervised learning

We want to minimise the KC of the auto encoder, as this gives us the best features that explain the data (proof?!?).

$$ KC (Net_{topology}) =( \sum v_{connections})^{d_{depth} + w_{width}}$$

The KC should be some function that is proportional to the shape, size, topology, ... of the neural network. 

Could we say that KC the amount of space (MB) required to store the net??


Since, neural networks are universal this is acceptable???

#### Pseudo code

```python
# Init the network
# The init sets the network to be undersized
Net = UnsupervisedLearner()

while loss>tolerance:
    loss = Net.autoencode(dataset)
    


```
How do we know when to add width/depth/recurrence/weight-tying/???

![_config.yml]({{ site.baseurl }}/images/config.png)
