---
layout: post
title: Insert
---

What if, instead of massively oversizing our network, we undersized our network. And then randomly inserted nodes?

As I talked about before, dropouts is similar to the idea of boosting. Where the weak learners are the dropped networks ...

So does this make ‘insert’ similar to random forests? Where we take the average of more complicated and specific algorithms

In one sense this is a lot nicer (?) occams razor etc..

But on the other hand, as we are increasing the complexity we dont know if the data still remains higher complexity or we missed its inherent complexity and ...
As with dropout, since we are decreaseing ...??