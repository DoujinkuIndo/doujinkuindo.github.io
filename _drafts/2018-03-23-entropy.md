---
layout: post
title: A brief introduction about Entropy
---

Information Theory is a cornerstone of Machine Learning, yet I've discovered over the years that a large amount of Data Science practitioners don't know the very basics of it. I find this a bit puzzling because, in my opinion, if you were to implement most algorithms on your own, you would come across with some of those concepts.

Let's start with a basic question, what is Information Theory?

> Information theory studies the quantification, storage, and communication of information.

This definition from [Wikipedia](https://en.wikipedia.org/wiki/Information_theory) is very good, and I hope will make it immediate why I'm claiming a good grasp of Information Theory is importnant for every Machine Learning practitioner.

One of the main concept in this branch of Mathematics is "entropy". Entropy is a measure of randomness or, in other words, unpredictability. The higher the entropy, the more unpredictable an outcome is. Entropy is at the foundations of many Machine Learning algorithms, to name a few: Decision Tree, Random Forest, Gradient Boosting.

To give an example, let's say you're extracting marble from an urn. Marbles can be only of two colors, either red or blue. You don't know a priori the proportion of red and blue marbles. Assuming the extracted all 10 blue marbles in the first 10 picks, what would you expect the outcome of the next pick to be? Well, most likely it would be another blue marble. After that many picks of the same color, the randomness of the next pick is very little.

We can do better, and measure this uncertainty using Entropy. Entropy is defined as:

$$ H(t) = I_E(p_1, ..., p_N) = - \sum_{i=1}^N p_i \log p_i $$

In our urn example, this would be:

$$ H(blue) = I_E(1, 1, 1, 1, 1, 1, 1, 1, 1, 1) = - \sum_{i=1}^{10} p_i \log p_i = 0 $$

Now, this is arguably a powerful tool. Let's think for instance if we want to represent a very complicated distribution, with a much simpler one. We do this all the time in Bayesian inference, when we try to choose a likelihood for a random variable of interest. In this case, what we do it to select the likelihood function which maximizes the entropy.

Another popular application of entropy is in tree-based algorithms. In this case entropy is uses to select the split point for each node. What we are trying to do here is to minimize the entropy, choosing the value at which the two child nodes will have the smaller entropy, in other world be as much homogeneous within each node.

I've coded up a very simple implementation of a Decesion Tree in Python, to show how entropy is used.

```python

import numpy as np

class DecisionTree:

    def __init__(self, criterion='entropy'):
        self.criterion = criterion
    def fit(self, X):
        pass
```
