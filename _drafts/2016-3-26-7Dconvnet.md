---
layout: post
title: The 7D conv net
---

Following on from my previous post on convolving over rotations (in the z axis), I want to expand on the idea of adding other dimensions to colvolve.

The motivation for this is; if we can make our features more invariant to irrelevant variables then we can learn faster, or from less data (as there are less parameters). Thus we can build deeper nets with less width. Thus gaining 

For example. A face is still a face, regardless of its size, rotation, position, colour, if half of it is occluded, ...

### Position (x,y)

It is common practice to convolve over the two spatial dimensions of an image. The motivation being that the location of a feature, in an image, (e.g. face) does not effect whether or not it is that feature. (an assumption on the designers part)

### Rotation (z)

This idea is what inspired this line of thought and it makes a lot of sense to me. A face is still a face, regardless of whether it is upside down, or sideways...

### Scale (z)

Scale invariance would be a nice feature of a conv net. However, at first glance it seems very computationally expensive. 

So to make a feature of a different scale we would need to interpolate or remove weights from our kernels. Given the current architecture of 

Algorithm. 
Take kernel, K, of size 3x3 and map it onto a 6x6 weight space. Use the values of K and interpolate between them to fill in the new, larger kernel. 

### Rotation (x,y)



### Occlusion

Partial features.

### Colour


# Trade-off

Fundamentally, there is a trade off between the computations required for each kernel and the kernels invariance to irrelevant variables.

Which is better?

I think both approaches probably end up using a roughly similar amount of computations (as to make up for the lack of invariance we need many ) 

But, having more convolutions would mean less parameters, thus less space is required? Although, having a SxOxCxRxNxdxnxn ...?