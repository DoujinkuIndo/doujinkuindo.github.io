---
layout: post
title: Dropout
---

### What is it really doing?

##### Discouraging co-adaption?

The authors of the paper cite this as the main motivation for dropout. They compare dropout to meiosos and the merging of two seperate chromosomes.


The metaphor goes ... quote?

To be honest this metaphor doesnt really make that much sense to me, as

Also, I somehow missed their justification for why co-adaption is a bad thing. Sure, in over sized neural networks .. But this does not apply to all networks.

In my opinion co-adaption is where neural networks get their strength and is a form of compositionality. Allowing ...

##### Robust features

Another justification the authors cite in their paper is that 

Falsification?

Wouldnt this just add redundancy?


##### Adding invariance

To network topology?

##### Sparsifying 

Making features (more) independent?


##### Regularisation…

##### Boosting?

Our weak learners here would be the smaller nets where some proportion of the network has been ‘dropped’. Th

### Dropout is a convolution?

Recently I have been studying convolutional networks. And as a result, I am looking at everything else through that lens.



### Questions and thoughts
* Averaging over different network topologies. How does this even make sense?


When we train a net with dropout are we convolving possible structures of our net (a large assumption)?

* That a small batch of the data approximates the true data (the same assumption made for SGD anyway?)


